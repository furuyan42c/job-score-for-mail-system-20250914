# ğŸ“š å®Ÿè£…ã‚¬ã‚¤ãƒ‰: ãƒã‚¤ãƒˆæ±‚äººãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  - å…¨è³ªå•ã¸ã®å›ç­”

**ä½œæˆæ—¥**: 2025-09-16  
**ç›®çš„**: asks.md ã®å…¨è³ªå•ã«å¯¾ã™ã‚‹å®Ÿè£…å¯èƒ½ãªå…·ä½“çš„å›ç­”  
**åˆ¶ç´„**: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã¯å¿œå‹Ÿãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã¿ï¼ˆä½æ‰€ç­‰ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æƒ…å ±ãªã—ï¼‰

## ğŸ”´ å„ªå…ˆåº¦: é«˜ï¼ˆå®Ÿè£…ãƒ–ãƒ­ãƒƒã‚«ãƒ¼ï¼‰ã®å›ç­”

### 1. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°

#### 1.1 åŸºç¤ã‚¹ã‚³ã‚¢è¨ˆç®—å¼

```python
# æ±‚äººãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
VALID_EMPLOYMENT_TYPE_CDS = [1, 3,6,8]  # 1=ã‚¢ãƒ«ãƒã‚¤ãƒˆã€ 3=ãƒ‘ãƒ¼ãƒˆã®ã¿ï¼ˆemployment_type_view.csvã‚ˆã‚Šï¼‰
MIN_FEE_THRESHOLD = 500  # 500å††ä»¥ä¸‹ã®feeã¯é™¤å¤–

def filter_jobs(jobs_df):
    """
    æ±‚äººãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    """
    # employment_type_cdã§ãƒ•ã‚£ãƒ«ã‚¿
    filtered = jobs_df[jobs_df['employment_type_cd'].isin(VALID_EMPLOYMENT_TYPE_CDS)]
    
    # fee > 500ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆfeeã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    if 'fee' in filtered.columns:
        filtered = filtered[filtered['fee'] > MIN_FEE_THRESHOLD]
    
    return filtered

def calculate_basic_score(job, area_stats, company_popularity):
    """
    åŸºç¤ã‚¹ã‚³ã‚¢ã‚’0-100ã®ç¯„å›²ã§è¨ˆç®—
    ç¦åˆ©åšç”Ÿã¨ã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šé™¤ã€feeã‚’è¿½åŠ 
    """
    # fee ãŒ500å††ä»¥ä¸‹ã®æ±‚äººã¯ã‚¹ã‚³ã‚¢0
    if hasattr(job, 'fee') and job.fee <= MIN_FEE_THRESHOLD:
        return 0
    
    # å„è¦ç´ ã‚’0-100ã«æ­£è¦åŒ–
    # min_salaryã¨max_salaryã®å¹³å‡ã‚’æ™‚çµ¦ã¨ã—ã¦ä½¿ç”¨
    avg_wage = (job.min_salary + job.max_salary) / 2 if job.min_salary and job.max_salary else 1200
    hourly_wage_score = normalize_hourly_wage(avg_wage, area_stats)
    fee_score = normalize_fee(job.fee)
    popularity_score = calculate_company_popularity_score(job.endcl_cd, company_popularity)
    
    # åŠ é‡å¹³å‡ã§è¨ˆç®—ï¼ˆ3è¦ç´ ï¼‰
    basic_score = (
        hourly_wage_score * 0.40 +    # æ™‚çµ¦
        fee_score * 0.30 +             # å¿œå‹Ÿå˜ä¾¡å ±é…¬
        popularity_score * 0.30        # ä¼æ¥­äººæ°—åº¦ï¼ˆendcl_cdãƒ™ãƒ¼ã‚¹ï¼‰
    )
    
    return min(100, max(0, basic_score))

def normalize_hourly_wage(wage, area_stats):
    """æ™‚çµ¦ã®æ­£è¦åŒ–ï¼ˆã‚¨ãƒªã‚¢å¹³å‡ã‚’åŸºæº–ã«ï¼‰"""
    area_avg = area_stats['avg_salary']  # min_salary/max_salaryã®å¹³å‡
    area_std = area_stats['std_salary']
    
    # z-scoreã‚’ä½¿ã£ã¦æ­£è¦åŒ–ã—ã€0-100ã«å¤‰æ›
    z_score = (wage - area_avg) / area_std if area_std > 0 else 0
    # z-score -2 to +2 ã‚’ 0 to 100 ã«ãƒãƒƒãƒ—
    return min(100, max(0, (z_score + 2) * 25))

def normalize_fee(fee):
    """
    å¿œå‹Ÿå˜ä¾¡å ±é…¬ï¼ˆfeeï¼‰ã®æ­£è¦åŒ–
    500å††ä»¥ä¸‹ã¯0ç‚¹ã€5000å††ä»¥ä¸Šã¯100ç‚¹
    """
    if fee <= 500:
        return 0
    elif fee >= 5000:
        return 100
    else:
        # 500-5000å††ã‚’0-100ã«ãƒªãƒ‹ã‚¢ãƒãƒƒãƒ”ãƒ³ã‚°
        return (fee - 500) / (5000 - 500) * 100

def calculate_company_popularity_score(endcl_cd, company_popularity):
    """
    ä¼æ¥­äººæ°—åº¦ã‚¹ã‚³ã‚¢ï¼ˆendcl_cdãƒ™ãƒ¼ã‚¹ã€360æ—¥ãƒ‡ãƒ¼ã‚¿ï¼‰
    """
    if endcl_cd not in company_popularity:
        return 30  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸­é–“ã‚¹ã‚³ã‚¢
    
    stats = company_popularity[endcl_cd]
    
    # 360æ—¥é–“ã®å¿œå‹Ÿç‡ã§åˆ¤å®š
    application_rate = stats['applications_360d'] / max(1, stats['views_360d'])
    
    if application_rate >= 0.15:  # 15%ä»¥ä¸ŠãŒå¿œå‹Ÿ
        return 100
    elif application_rate >= 0.10:  # 10%ä»¥ä¸Š
        return 80
    elif application_rate >= 0.05:  # 5%ä»¥ä¸Š
        return 60
    elif application_rate >= 0.02:  # 2%ä»¥ä¸Š
        return 40
    else:
        return 20

def calculate_editorial_popularity_score(job, user_location):
    """
    ç·¨é›†éƒ¨ãŠã™ã™ã‚ç”¨ã®äººæ°—åº¦ã‚¹ã‚³ã‚¢ï¼ˆfee Ã— å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•°ï¼‰
    """
    # feeã‚¹ã‚³ã‚¢
    if hasattr(job, 'fee'):
        fee_score = normalize_fee(job.fee)
    else:
        fee_score = 30
    
    # å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•°ã‚¹ã‚³ã‚¢ï¼ˆrecent_applicationsã‚’ä½¿ç”¨ï¼‰
    if hasattr(job, 'recent_applications'):
        click_score = min(100, job.recent_applications * 2)  # å¿œå‹Ÿ1ä»¶=2ç‚¹
    else:
        click_score = 30
    
    # åœ°åŸŸé‡ã¿ä»˜ã‘
    location_weight = get_location_weight(job, user_location)
    
    # ç·åˆã‚¹ã‚³ã‚¢
    return (fee_score * 0.5 + click_score * 0.5) * location_weight

def get_location_weight(job, user_location):
    """
    åœ°åŸŸã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
    å¸‚åŒºç”ºæ‘: 1.0ã€å‘¨è¾ºå¸‚åŒºç”ºæ‘: 0.7ã€åŒã˜éƒ½é“åºœçœŒ: 0.5ã€ãã‚Œä»¥å¤–: 0.3
    """
    if not user_location:
        return 1.0
    
    # å¸‚åŒºç”ºæ‘ãŒå®Œå…¨ä¸€è‡´
    if hasattr(job, 'city_cd') and job.city_cd == user_location.get('city_cd'):
        return 1.0
    
    # å‘¨è¾ºå¸‚åŒºç”ºæ‘ï¼ˆç°¡æ˜“åˆ¤å®šï¼šåŒã˜éƒ½é“åºœçœŒå†…ã®è¿‘ã„ã‚³ãƒ¼ãƒ‰ï¼‰
    if hasattr(job, 'city_cd') and user_location.get('nearby_cities'):
        if job.city_cd in user_location['nearby_cities']:
            return 0.7
    
    # åŒã˜éƒ½é“åºœçœŒ
    if hasattr(job, 'pref_cd') and job.pref_cd == user_location.get('pref_cd'):
        return 0.5
    
    # ãã‚Œä»¥å¤–
    return 0.3

def prepare_company_popularity_data(user_actions_df):
    """
    éå»360æ—¥ã®user_actionsã‹ã‚‰endcl_cdåˆ¥ã®äººæ°—åº¦ã‚’è¨ˆç®—
    """
    from datetime import datetime, timedelta
    
    # 360æ—¥å‰ã®æ—¥ä»˜
    cutoff_date = datetime.now() - timedelta(days=360)
    
    # æœŸé–“å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    recent_actions = user_actions_df[user_actions_df['action_date'] >= cutoff_date]
    
    # endcl_cdåˆ¥ã«é›†è¨ˆ
    company_stats = recent_actions.groupby('endcl_cd').agg({
        'action_id': 'count',  # ç·ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
        'user_id': lambda x: x[recent_actions.loc[x.index, 'action_type'] == 'applied'].nunique(),  # å¿œå‹Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
    }).rename(columns={
        'action_id': 'views_360d',
        'user_id': 'applications_360d'
    })
    
    return company_stats.to_dict('index')
```

#### 1.2 SEOã‚¹ã‚³ã‚¢ã®ãƒãƒƒãƒãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯

```python
def preprocess_semrush_keywords(semrush_df):
    """
    SEMRUSHã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰å‡¦ç†ã¨åŠ å·¥
    """
    processed_keywords = []
    
    for _, row in semrush_df.iterrows():
        keyword = row['keyword'].lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†è§£ã¨æ­£è¦åŒ–
        # ä¾‹: "ã‚³ãƒ³ãƒ“ãƒ‹ ãƒã‚¤ãƒˆ" â†’ ["ã‚³ãƒ³ãƒ“ãƒ‹", "ãƒã‚¤ãƒˆ", "ã‚³ãƒ³ãƒ“ãƒ‹ãƒã‚¤ãƒˆ"]
        parts = keyword.split()
        variations = [
            keyword,  # å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            ''.join(parts),  # ã‚¹ãƒšãƒ¼ã‚¹ãªã—ç‰ˆ
            ' '.join(parts),  # ã‚¹ãƒšãƒ¼ã‚¹ã‚ã‚Šç‰ˆ
        ]
        
        # å„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç™»éŒ²
        for var in variations:
            if var:  # ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
                processed_keywords.append({
                    'original': row['keyword'],
                    'processed': var,
                    'volume': row['volume'],
                    'difficulty': row.get('keyword_difficulty', 50),
                    'intent': row.get('intent', 'Informational')
                })
    
    return pd.DataFrame(processed_keywords)

# ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã”ã¨ã®SEOã‚¹ã‚³ã‚¢é‡ã¿å®šç¾©
FIELD_WEIGHT_CONFIG = {
    'application_name': 1.5,    # é«˜ã„é‡ã¿
    'company_name': 1.5,        # é«˜ã„é‡ã¿
    'catch_copy': 1.0,          # æ¨™æº–çš„ãªé‡ã¿
    'salary': 0.3,              # å°ã•ã„é‡ã¿
    'hours': 0.3,               # å°ã•ã„é‡ã¿
    'features': 0.7,            # ä¸­ç¨‹åº¦ã®é‡ã¿
    'station_name_eki': 0.5     # ä¸­ç¨‹åº¦ã®é‡ã¿
}

def calculate_seo_score(job, processed_keywords_df):
    """
    SEOã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã”ã¨ã®é‡ã¿ä»˜ã‘å¯¾å¿œï¼‰
    """
    seo_score = 0
    matched_keywords = []
    
    # å…¨è§’ãƒ»åŠè§’ã‚’çµ±ä¸€ã™ã‚‹é–¢æ•°
    import unicodedata
    def normalize_text(text):
        if text is None:
            return ''
        return unicodedata.normalize('NFKC', str(text)).lower()
    
    # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
    field_texts = {}
    for field_name in FIELD_WEIGHT_CONFIG.keys():
        if hasattr(job, field_name):
            field_texts[field_name] = normalize_text(getattr(job, field_name))
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã¨ã‚¹ã‚³ã‚¢è¨ˆç®—
    for _, keyword_row in processed_keywords_df.iterrows():
        keyword = keyword_row['processed']
        matched = False
        max_field_score = 0
        matched_field = None
        
        # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        for field_name, field_text in field_texts.items():
            if keyword in field_text:
                # æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ ã¨æ„å›³ã«å¿œã˜ãŸåŸºæœ¬ã‚¹ã‚³ã‚¢
                volume = keyword_row['volume']
                intent = keyword_row['intent']
                
                # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ™ãƒ¼ã‚¹ï¼‰
                if volume >= 10000:
                    base_score = 15
                elif volume >= 5000:
                    base_score = 10
                elif volume >= 1000:
                    base_score = 7
                elif volume >= 500:
                    base_score = 5
                else:
                    base_score = 3
                
                # æ¤œç´¢æ„å›³ã«ã‚ˆã‚‹èª¿æ•´
                intent_multiplier = {
                    'Commercial': 1.5,  # å•†ç”¨æ„å›³ã¯é«˜ä¾¡å€¤
                    'Transactional': 1.3,  # å–å¼•æ„å›³ã‚‚ä¾¡å€¤é«˜
                    'Informational': 1.0,  # æƒ…å ±æ„å›³ã¯æ¨™æº–
                    'Navigational': 0.8  # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ„å›³ã¯ä½ã‚
                }.get(intent, 1.0)
                
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰é‡ã¿ã‚’é©ç”¨
                field_weight = FIELD_WEIGHT_CONFIG.get(field_name, 1.0)
                field_score = base_score * intent_multiplier * field_weight
                
                # æœ€ã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¡ç”¨
                if field_score > max_field_score:
                    max_field_score = field_score
                    matched_field = field_name
                    matched = True
        
        if matched:
            seo_score += max_field_score
            matched_keywords.append({
                'keyword': keyword_row['original'],
                'volume': keyword_row['volume'],
                'score': max_field_score,
                'matched_field': matched_field  # ãƒãƒƒãƒã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¨˜éŒ²
            })
            
            # æœ€å¤§7å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ã§
            if len(matched_keywords) >= 7:
                break
    
    # æœ€å¤§100ç‚¹ã«æ­£è¦åŒ–
    return min(100, seo_score), matched_keywords

def save_keyword_scoring(job_id, matched_keywords, processed_keywords_df):
    """
    keyword_scoringãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆãƒãƒƒãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æƒ…å ±ä»˜ãï¼‰
    """
    scoring_records = []
    
    for match in matched_keywords:
        # å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰IDã‚’å–å¾—
        original_keyword = processed_keywords_df[
            processed_keywords_df['original'] == match['keyword']
        ].iloc[0] if not processed_keywords_df[
            processed_keywords_df['original'] == match['keyword']
        ].empty else None
        
        if original_keyword is not None:
            scoring_records.append({
                'job_id': job_id,
                'keyword_id': original_keyword.get('keyword_id'),
                'processed_keyword': match['keyword'],
                'base_score': match['score'],
                'matched_field': match.get('matched_field', ''),  # ãƒãƒƒãƒã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                'field_weight': FIELD_WEIGHT_CONFIG.get(match.get('matched_field', ''), 1.0),  # é©ç”¨ã•ã‚ŒãŸé‡ã¿
                'processed_at': datetime.now()
            })
    
    return scoring_records
```

#### 1.3 ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ã®å®Ÿæ•°è¨ˆç®—

```python
def calculate_personalized_score(job, user_profile):
    """
    user_profilesãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸå®Ÿæ•°è¨ˆç®—ã«ã‚ˆã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢
    å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ã¯ãªãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®å¿œå‹Ÿå‚¾å‘ã¨ã®é¡ä¼¼åº¦ã§è¨ˆç®—
    """
    if user_profile is None or user_profile.total_applications == 0:
        return 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢ï¼ˆå¿œå‹Ÿå±¥æ­´ãŒãªã„å ´åˆï¼‰
    
    score_components = []
    weights = []
    
    # 1. éƒ½é“åºœçœŒãƒãƒƒãƒãƒ³ã‚°ï¼ˆapplied_pref_cdsï¼‰
    if user_profile.applied_pref_cds:
        pref_matches = calculate_location_match_score(
            job.pref_cd, 
            parse_frequency_string(user_profile.applied_pref_cds)
        )
        score_components.append(pref_matches)
        weights.append(0.20)
    
    # 2. å¸‚åŒºç”ºæ‘ãƒãƒƒãƒãƒ³ã‚°ï¼ˆapplied_city_cdsï¼‰
    if user_profile.applied_city_cds:
        city_matches = calculate_location_match_score(
            job.city_cd,
            parse_frequency_string(user_profile.applied_city_cds)
        )
        score_components.append(city_matches)
        weights.append(0.15)
    
    # 3. è·ç¨®å¤§åˆ†é¡ãƒãƒƒãƒãƒ³ã‚°ï¼ˆapplied_occupation_cd1sï¼‰
    if user_profile.applied_occupation_cd1s:
        occupation_matches = calculate_category_match_score(
            job.occupation_cd1,
            parse_frequency_string(user_profile.applied_occupation_cd1s)
        )
        score_components.append(occupation_matches)
        weights.append(0.20)
    
    # 4. é›‡ç”¨å½¢æ…‹ãƒãƒƒãƒãƒ³ã‚°ï¼ˆapplied_employment_type_cdsï¼‰
    if user_profile.applied_employment_type_cds:
        employment_matches = calculate_category_match_score(
            job.employment_type_cd,
            parse_frequency_string(user_profile.applied_employment_type_cds)
        )
        score_components.append(employment_matches)
        weights.append(0.10)
    
    # 5. çµ¦ä¸ã‚¿ã‚¤ãƒ—ãƒãƒƒãƒãƒ³ã‚°ï¼ˆapplied_salary_type_cdsï¼‰
    if user_profile.applied_salary_type_cds:
        salary_type_matches = calculate_category_match_score(
            job.salary_type_cd,
            parse_frequency_string(user_profile.applied_salary_type_cds)
        )
        score_components.append(salary_type_matches)
        weights.append(0.10)
    
    # 6. çµ¦ä¸ãƒ¬ãƒ³ã‚¸ãƒãƒƒãƒãƒ³ã‚°ï¼ˆapplied_salary_statsï¼‰
    if user_profile.applied_salary_stats:
        salary_stats = json.loads(user_profile.applied_salary_stats)
        salary_matches = calculate_salary_range_match(
            job.min_salary,
            job.max_salary,
            salary_stats
        )
        score_components.append(salary_matches)
        weights.append(0.15)
    
    # 7. ã‚¨ãƒ³ãƒ‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆapplied_endcl_cdsï¼‰
    # åŒã˜ä¼æ¥­ã¸ã®å¿œå‹Ÿã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¹ã‚³ã‚¢
    if user_profile.applied_endcl_cds:
        endcl_matches = calculate_exact_match_score(
            job.endcl_cd,
            parse_frequency_string(user_profile.applied_endcl_cds)
        )
        score_components.append(endcl_matches)
        weights.append(0.15)  # é‡è¤‡æŠ‘åˆ¶ã®é‡è¦åº¦ã‚’ä¸Šã’ã‚‹
    
    # é‡ã¿ä»˜ãå¹³å‡ã‚’è¨ˆç®—
    if score_components:
        total_weight = sum(weights)
        weighted_score = sum(s * w for s, w in zip(score_components, weights))
        final_score = (weighted_score / total_weight) if total_weight > 0 else 50
    else:
        final_score = 50
    
    return min(100, max(0, final_score))

def parse_frequency_string(freq_str):
    """
    é »åº¦æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
    ä¾‹: "13:5,14:3,11:1" â†’ {13: 5, 14: 3, 11: 1}
    """
    if not freq_str:
        return {}
    
    result = {}
    for item in freq_str.split(','):
        if ':' in item:
            code, count = item.split(':')
            try:
                result[int(code)] = int(count)
            except ValueError:
                continue
    return result

def calculate_location_match_score(job_value, user_frequencies):
    """
    åœ°åŸŸãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢è¨ˆç®—
    """
    if not user_frequencies:
        return 50
    
    total_count = sum(user_frequencies.values())
    if job_value in user_frequencies:
        # ã“ã®åœ°åŸŸã¸ã®å¿œå‹Ÿå‰²åˆã‚’è¨ˆç®—
        ratio = user_frequencies[job_value] / total_count
        # 0-100ã«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆæœ€å¤§50%ã®å¿œå‹Ÿç‡ã§100ç‚¹ï¼‰
        return min(100, ratio * 200)
    else:
        # æœªå¿œå‹Ÿåœ°åŸŸã¯ä½ã‚¹ã‚³ã‚¢
        return 20

def calculate_category_match_score(job_value, user_frequencies):
    """
    ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢è¨ˆç®—
    """
    if not user_frequencies:
        return 50
    
    total_count = sum(user_frequencies.values())
    if job_value in user_frequencies:
        ratio = user_frequencies[job_value] / total_count
        # ã‚«ãƒ†ã‚´ãƒªã¯åœ°åŸŸã‚ˆã‚Šé‡è¦åº¦ã‚’é«˜ã
        return min(100, ratio * 250)
    else:
        return 30

def calculate_salary_range_match(job_min, job_max, user_stats):
    """
    çµ¦ä¸ãƒ¬ãƒ³ã‚¸ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢è¨ˆç®—
    """
    if not user_stats:
        return 50
    
    user_avg = user_stats.get('avg', 1200)
    user_min = user_stats.get('min', 1000)
    user_max = user_stats.get('max', 2000)
    
    job_avg = (job_min + job_max) / 2
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã‚€çµ¦ä¸ãƒ¬ãƒ³ã‚¸ã¨ã®è·é›¢ã‚’è¨ˆç®—
    if user_min <= job_avg <= user_max:
        # ç¯„å›²å†…ã¯é«˜ã‚¹ã‚³ã‚¢
        distance_from_avg = abs(job_avg - user_avg)
        max_distance = max(user_avg - user_min, user_max - user_avg)
        if max_distance > 0:
            score = 100 - (distance_from_avg / max_distance * 50)
        else:
            score = 100
    else:
        # ç¯„å›²å¤–ã¯è·é›¢ã«å¿œã˜ã¦æ¸›ç‚¹
        if job_avg < user_min:
            distance = user_min - job_avg
        else:
            distance = job_avg - user_max
        
        # 500å††é›¢ã‚Œã‚‹ã”ã¨ã«10ç‚¹æ¸›ç‚¹
        score = max(20, 70 - (distance / 500 * 10))
    
    return score

def calculate_exact_match_score(job_value, user_frequencies):
    """
    å®Œå…¨ä¸€è‡´ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä¼æ¥­ã‚³ãƒ¼ãƒ‰ï¼‰
    ãƒã‚¤ãƒˆã§ã¯åŒã˜ä¼æ¥­ã¸ã®é‡è¤‡å¿œå‹Ÿã‚’æŠ‘åˆ¶
    """
    if not user_frequencies:
        return 50
    
    if job_value in user_frequencies:
        # éå»ã«å¿œå‹Ÿæ¸ˆã¿ã®ä¼æ¥­ã¯ä½ã‚¹ã‚³ã‚¢ï¼ˆé‡è¤‡å¿œå‹Ÿã‚’é¿ã‘ã‚‹ï¼‰
        repeat_count = user_frequencies[job_value]
        if repeat_count >= 3:
            return 10  # 3å›ä»¥ä¸Šå¿œå‹Ÿæ¸ˆã¿ï¼šæœ€ä½ã‚¹ã‚³ã‚¢
        elif repeat_count == 2:
            return 20  # 2å›å¿œå‹Ÿæ¸ˆã¿ï¼šä½ã‚¹ã‚³ã‚¢
        else:
            return 30  # 1å›å¿œå‹Ÿæ¸ˆã¿ï¼šã‚„ã‚„ä½ã‚¹ã‚³ã‚¢
    else:
        return 70  # æœªå¿œå‹Ÿã®ä¼æ¥­ï¼šé«˜ã‚¹ã‚³ã‚¢ï¼ˆæ–°ã—ã„ä¼æ¥­ã‚’æ¨å¥¨ï¼‰

# user_profilesãƒ†ãƒ¼ãƒ–ãƒ«ã®æ›´æ–°å‡¦ç†
def update_user_profiles(user_actions_df):
    """
    user_actionsã‹ã‚‰user_profilesã‚’è¨ˆç®—ãƒ»æ›´æ–°
    """
    profiles = []
    
    for user_id in user_actions_df['user_id'].unique():
        user_actions = user_actions_df[user_actions_df['user_id'] == user_id]
        
        # å¿œå‹Ÿã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿
        applications = user_actions[user_actions['action_type'] == 'applied']
        
        if len(applications) > 0:
            profile = {
                'user_id': user_id,
                'applied_pref_cds': format_frequency_dict(applications['pref_cd'].value_counts()),
                'applied_city_cds': format_frequency_dict(applications['city_cd'].value_counts()),
                'applied_occupation_cd1s': format_frequency_dict(applications['occupation_cd1'].value_counts()),
                'applied_occupation_cd2s': format_frequency_dict(applications['occupation_cd2'].value_counts()),
                'applied_occupation_cd3s': format_frequency_dict(applications['occupation_cd3'].value_counts()),
                'applied_jobtype_details': format_frequency_dict(applications['jobtype_detail'].value_counts()),
                'applied_employment_type_cds': format_frequency_dict(applications['employment_type_cd'].value_counts()),
                'applied_salary_type_cds': format_frequency_dict(applications['salary_type_cd'].value_counts()),
                'applied_salary_stats': json.dumps({
                    'min': applications['min_salary'].min(),
                    'max': applications['max_salary'].max(),
                    'avg': applications[['min_salary', 'max_salary']].mean().mean(),
                    'median': applications[['min_salary', 'max_salary']].median().median()
                }),
                'applied_station_cds': format_frequency_dict(applications['station_cd'].value_counts()),
                'applied_feature_codes': format_frequency_dict(
                    pd.Series([f for features in applications['feature_codes'].str.split(',') 
                              for f in features if f]).value_counts()
                ),
                'applied_endcl_cds': format_frequency_dict(applications['endcl_cd'].value_counts()),
                'total_applications': len(applications),
                'avg_applied_fee': applications['fee'].mean(),
                'first_application_at': applications['action_date'].min(),
                'last_application_at': applications['action_date'].max(),
                'profile_updated_at': datetime.now()
            }
            profiles.append(profile)
    
    return profiles

def format_frequency_dict(value_counts):
    """
    value_countsã‚’é »åº¦æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    ä¾‹: {13: 5, 14: 3} â†’ "13:5,14:3"
    """
    if value_counts.empty:
        return ""
    
    items = []
    for code, count in value_counts.head(10).items():  # ä¸Šä½10å€‹ã¾ã§
        items.append(f"{code}:{count}")
    
    return ",".join(items)
```

### 2. ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãƒ«ãƒ¼ãƒ«

#### 2.1 14ãƒ‹ãƒ¼ã‚ºã‚«ãƒ†ã‚´ãƒªã®åˆ¤å®šæ¡ä»¶
```python
NEEDS_CATEGORIES = {
    'æ—¥æ‰•ã„ãƒ»é€±æ‰•ã„': {
        'keywords': ['æ—¥æ‰•ã„', 'å³æ—¥æ‰•ã„', 'é€±æ‰•ã„', 'æ—¥æ‰•OK', 'å³é‡‘'],
        'fields': ['application_name', 'salary', 'features', 'catch_copy'],
        'priority': 1,
        'logic': 'any'  # ã„ãšã‚Œã‹ã«ãƒãƒƒãƒ
    },
    'çŸ­æœŸãƒ»å˜ç™ºOK': {
        'keywords': ['çŸ­æœŸ', 'å˜ç™º', '1æ—¥ã®ã¿', 'æœŸé–“é™å®š', '1day'],
        'fields': ['application_name', 'employment_type', 'features'],
        'priority': 2,
        'logic': 'any'
    },
    'é«˜æ™‚çµ¦': {
        'keywords': ['é«˜åå…¥', 'é«˜æ™‚çµ¦', 'é«˜çµ¦'],
        'fields': ['application_name', 'catch_copy'],
        'priority': 3,
        'dynamic_check': lambda job, area_stats: ((job.min_salary + job.max_salary) / 2 if job.min_salary and job.max_salary else 1200) >= area_stats['avg_salary'] * 1.2
    },
    'ã‚·ãƒ•ãƒˆè‡ªç”±': {
        'keywords': ['ã‚·ãƒ•ãƒˆè‡ªç”±', 'ã‚·ãƒ•ãƒˆç›¸è«‡', 'é€±1', 'è‡ªç”±ã‚·ãƒ•ãƒˆ', 'å¥½ããªæ™‚é–“'],
        'fields': ['application_name', 'work_hours', 'features'],
        'priority': 4,
        'logic': 'any'
    },
    'æœªçµŒé¨“æ­“è¿': {
        'keywords': ['æœªçµŒé¨“', 'åˆå¿ƒè€…', 'çµŒé¨“ä¸å•', 'æœªçµŒé¨“OK', 'ç ”ä¿®ã‚ã‚Š'],
        'fields': ['application_name', 'requirements', 'catch_copy'],
        'priority': 5,
        'logic': 'any'
    },
    'åœ¨å®…ãƒ»ãƒªãƒ¢ãƒ¼ãƒˆ': {
        'keywords': ['åœ¨å®…', 'ãƒªãƒ¢ãƒ¼ãƒˆ', 'ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯', 'å®Œå…¨åœ¨å®…', 'remote'],
        'fields': ['application_name', 'work_location', 'features'],
        'priority': 6,
        'logic': 'any'
    },
    'å­¦ç”Ÿæ­“è¿': {
        'keywords': ['å­¦ç”Ÿæ­“è¿', 'å­¦ç”ŸOK', 'å¤§å­¦ç”Ÿ', 'å°‚é–€å­¦ç”Ÿ'],
        'fields': ['application_name', 'requirements', 'features'],
        'priority': 7,
        'feature_code': 104  # feature_masterå‚ç…§
    },
    'é«˜æ ¡ç”Ÿæ­“è¿': {
        'keywords': ['é«˜æ ¡ç”Ÿæ­“è¿', 'é«˜æ ¡ç”ŸOK', 'é«˜æ ¡ç”Ÿå¯'],
        'fields': ['application_name', 'age', 'features'],
        'priority': 8,
        'logic': 'any'
    },
    'ä¸»å©¦æ­“è¿': {
        'keywords': ['ä¸»å©¦æ­“è¿', 'ä¸»å©¦ãƒ»ä¸»å¤«', 'ä¸»å©¦OK', 'å®¶äº‹ã¨ä¸¡ç«‹'],
        'fields': ['application_name', 'requirements', 'features'],
        'priority': 9,
        'logic': 'any'
    },
    'ã‚·ãƒ‹ã‚¢æ­“è¿': {
        'keywords': ['ã‚·ãƒ‹ã‚¢æ­“è¿', 'ã‚·ãƒ‹ã‚¢OK', '60æ­³ä»¥ä¸Š', 'ä¸­é«˜å¹´'],
        'fields': ['application_name', 'age', 'features'],
        'priority': 10,
        'logic': 'any'
    },
    'åœŸæ—¥ã®ã¿OK': {
        'keywords': ['åœŸæ—¥ã®ã¿', 'åœŸæ—¥ç¥', 'é€±æœ«ã®ã¿', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥'],
        'fields': ['application_name', 'work_days', 'features'],
        'priority': 11,
        'logic': 'any'
    },
    'å‰¯æ¥­ãƒ»Wãƒ¯ãƒ¼ã‚¯OK': {
        'keywords': ['å‰¯æ¥­', 'Wãƒ¯ãƒ¼ã‚¯', 'ãƒ€ãƒ–ãƒ«ãƒ¯ãƒ¼ã‚¯', 'æ›ã‘æŒã¡'],
        'fields': ['application_name', 'requirements', 'features'],
        'priority': 12,
        'logic': 'any'
    },
    'äº¤é€šè²»æ”¯çµ¦': {
        'keywords': ['äº¤é€šè²»', 'äº¤é€šè²»æ”¯çµ¦', 'äº¤é€šè²»å…¨é¡'],
        'fields': ['application_name', 'benefits', 'features'],
        'priority': 13,
        'flag_check': 'transportation_allowance'
    },
    'å³æ—¥å‹¤å‹™OK': {
        'keywords': ['å³æ—¥å‹¤å‹™', 'å³æ—¥ã‚¹ã‚¿ãƒ¼ãƒˆ', 'ä»Šã™ã', 'å³é–‹å§‹'],
        'fields': ['application_name', 'start_date', 'features'],
        'priority': 14,
        'logic': 'any'
    }
}

def categorize_job(job, area_stats, occupation_master):

    """
    æ±‚äººã‚’14ã®ãƒ‹ãƒ¼ã‚ºã‚«ãƒ†ã‚´ãƒª+è·ç¨®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡
    è¤‡æ•°è©²å½“ã™ã‚‹å ´åˆã¯åˆ¶é™ãªã—ï¼ˆå…¨ã¦è¿”ã™ï¼‰
    """
    matched_categories = []
    
    for category_name, rules in NEEDS_CATEGORIES.items():
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        if 'keywords' in rules:
            for field in rules['fields']:
                field_value = str(getattr(job, field, '')).lower()
                if any(keyword in field_value for keyword in rules['keywords']):
                    matched_categories.append((rules['priority'], category_name))
                    break
        
        # ãƒ•ãƒ©ã‚°ãƒã‚§ãƒƒã‚¯
        if 'flag_check' in rules:
            if getattr(job, rules['flag_check'], False):
                matched_categories.append((rules['priority'], category_name))
        
        # å‹•çš„ãƒã‚§ãƒƒã‚¯
        if 'dynamic_check' in rules:
            if rules['dynamic_check'](job, area_stats):
                matched_categories.append((rules['priority'], category_name))
    
    # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆã—ã¦è¿”ã™ï¼ˆåˆ¶é™ãªã—ï¼‰
    matched_categories.sort(key=lambda x: x[0])
    needs_categories = [cat[1] for cat in matched_categories]
    
    # è·ç¨®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚‚è¿½åŠ ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰
    occupation_categories = get_occupation_categories(job, occupation_master)
    
    return {
        'needs_categories': needs_categories,  # å…¨è©²å½“ã‚«ãƒ†ã‚´ãƒª
        'occupation_categories': occupation_categories  # è·ç¨®ã‚«ãƒ†ã‚´ãƒª
    }

def get_occupation_categories(job, occupation_master):
    """
    å®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãè·ç¨®ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
    occupation_cd1ï¼ˆå¤§åˆ†é¡ï¼‰ã«åŸºã¥ã„ã¦åˆ†é¡
    """
    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è·ç¨®ã‚³ãƒ¼ãƒ‰ï¼ˆsample_job_data.csvã‚ˆã‚Š100å˜ä½ã®å€¤ã‚’ä½¿ç”¨ï¼‰
    OCCUPATION_CATEGORY_MAP = {
        100: 'ã‚³ãƒ³ãƒ“ãƒ‹ãƒ»ã‚¹ãƒ¼ãƒ‘ãƒ¼',
        200: 'é£²é£Ÿãƒ»ãƒ•ãƒ¼ãƒ‰',
        300: 'ã‚¢ãƒ‘ãƒ¬ãƒ«ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³',
        400: 'è»½ä½œæ¥­ãƒ»å€‰åº«',
        500: 'ã‚ªãƒ•ã‚£ã‚¹ãƒ¯ãƒ¼ã‚¯',
        600: 'ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
        700: 'æ•™è‚²ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼',
        800: 'åŒ»ç™‚ãƒ»ä»‹è­·ãƒ»ç¦ç¥‰',
        900: 'ãƒ‡ãƒªãƒãƒªãƒ¼ãƒ»é…é”',
        1000: 'ç¾å®¹ãƒ»ã‚¨ã‚¹ãƒ†',
        1100: 'ã‚¨ãƒ³ã‚¿ãƒ¡ãƒ»ãƒ¬ã‚¸ãƒ£ãƒ¼',
        1200: 'ãã®ä»–'
    }
    
    # jobã®occupation_cd1ã‹ã‚‰å¯¾å¿œã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
    category = OCCUPATION_CATEGORY_MAP.get(job.occupation_cd1, 'ãã®ä»–')
    
    # è©³ç´°åˆ†é¡ã‚‚å«ã‚ã‚‹å ´åˆ
    if hasattr(job, 'jobtype_detail'):
        detail = occupation_master.get_detail_name(job.occupation_cd1, job.jobtype_detail)
        return {'main': category, 'detail': detail}
    
    return {'main': category}
```

#### 2.2 é«˜æ™‚çµ¦ã®åˆ¤å®šåŸºæº–
```python
def calculate_area_stats(jobs_df, granularity='prefecture'):
    """
    ã‚¨ãƒªã‚¢åˆ¥ã®æ™‚çµ¦çµ±è¨ˆã‚’è¨ˆç®—
    granularity: 'prefecture'ï¼ˆéƒ½é“åºœçœŒï¼‰ã¾ãŸã¯ 'city'ï¼ˆå¸‚åŒºç”ºæ‘ï¼‰
    """
    if granularity == 'prefecture':
        group_col = 'prefecture_id'
    else:
        group_col = 'city_id'
    
    # min_salaryã¨max_salaryã®å¹³å‡å€¤ã§è¨ˆç®—
    jobs_df['avg_salary'] = (jobs_df['min_salary'] + jobs_df['max_salary']) / 2
    area_stats = jobs_df.groupby(group_col).agg({
        'avg_salary': ['mean', 'std', 'median'],
        'job_id': 'count'
    }).reset_index()
    
    area_stats.columns = [group_col, 'avg_salary', 'std_salary', 
                          'median_salary', 'job_count']
    
    return area_stats

def is_high_wage(job, area_stats):
    """
    é«˜æ™‚çµ¦åˆ¤å®šï¼ˆã‚¨ãƒªã‚¢å¹³å‡ã®1.2å€ä»¥ä¸Šï¼‰
    """
    # éƒ½é“åºœçœŒãƒ¬ãƒ™ãƒ«ã§åˆ¤å®š
    area_avg = area_stats.loc[
        area_stats['prefecture_id'] == job.prefecture_id, 
        'avg_salary'
    ].values[0]
    
    # min_salaryã¨max_salaryã®å¹³å‡å€¤ã‚’ä½¿ç”¨
    avg_wage = (job.min_salary + job.max_salary) / 2 if job.min_salary and job.max_salary else 1200
    
    return avg_wage >= area_avg * 1.2

def convert_to_hourly_wage(job):
    """
    çµ¦ä¸ã‚’æ™‚çµ¦ã«æ›ç®—
    min_salaryã¨max_salaryã®å¹³å‡å€¤ã‚’ä½¿ç”¨
    """
    # min_salaryã¨max_salaryã®å¹³å‡ã‚’è¿”ã™
    if job.min_salary and job.max_salary:
        return (job.min_salary + job.max_salary) / 2
    elif job.min_salary:
        return job.min_salary
    elif job.max_salary:
        return job.max_salary
    else:
        return 1000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
```


### 3. 40ä»¶é¸å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

#### 3.1 ãƒ¡ãƒ¼ãƒ«é…ä¿¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆ

ãƒ¡ãƒ¼ãƒ«é…ä¿¡ã§ã¯ä»¥ä¸‹ã®6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æ±‚äººã‚’æŒ¯ã‚Šåˆ†ã‘ã¾ã™ï¼ˆåˆè¨ˆ40ä»¶ï¼‰ï¼š
1. **ç·¨é›†éƒ¨ãŠã™ã™ã‚ã®äººæ°—ãƒã‚¤ãƒˆï¼ˆ5ä»¶ï¼‰**: fee Ã— å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•°ã®ã‚¹ã‚³ã‚¢ä¸Šä½
2. **ã‚ãªãŸã«ãŠã™ã™ã‚æ±‚äººTOP5ï¼ˆ5ä»¶ï¼‰**: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ä¸Šä½
3. **åœ°åŸŸåˆ¥æ±‚äººTOP10ï¼ˆ10ä»¶ï¼‰**: éƒ½é“åºœçœŒå†…ã§è·ç¨®ãƒãƒƒãƒãƒ³ã‚°
4. **è¿‘éš£æ±‚äººTOP8ï¼ˆ8ä»¶ï¼‰**: å¸‚åŒºç”ºæ‘å‘¨è¾ºã§è·ç¨®ãƒãƒƒãƒãƒ³ã‚°
5. **é«˜åå…¥ãƒ»æ—¥æ‰•ã„ãƒã‚¤ãƒˆTOP7ï¼ˆ7ä»¶ï¼‰**: é«˜æ™‚çµ¦ãƒ»æ—¥æ‰•ã„æ¡ä»¶ã®æ±‚äºº
6. **æ–°ç€æ±‚äººï¼ˆ5ä»¶ï¼‰**: éå»1é€±é–“ä»¥å†…ã«æŠ•ç¨¿ã•ã‚ŒãŸæ±‚äººï¼‹ã‚¹ã‚³ã‚¢ä¸Šä½

å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…±é€šæ¡ä»¶ï¼š
- 2é€±é–“ä»¥å†…ã«å¿œå‹Ÿã—ãŸä¼æ¥­ï¼ˆendcl_cdï¼‰ã¯é™¤å¤–
- åœ°åŸŸé‡ã¿ä»˜ã‘ï¼šå¸‚åŒºç”ºæ‘ > å¸‚åŒºç”ºæ‘å‘¨è¾º > éƒ½é“åºœçœŒ

#### 3.2 ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®é‡è¤‡å‡¦ç†
```python
class JobSelector:
    """
    40ä»¶ã®æ±‚äººã‚’5ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æŒ¯ã‚Šåˆ†ã‘ã‚‹ã‚¯ãƒ©ã‚¹
    é‡è¤‡ã‚’é™¤å¤–ã—ã€å„æ±‚äººã¯1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã«ç™»å ´
    """
    
    def __init__(self, user, jobs_df, scores_df, area_stats):
        self.user = user
        self.jobs_df = jobs_df
        self.scores_df = scores_df
        self.area_stats = area_stats
        self.selected_job_ids = set()  # é‡è¤‡é˜²æ­¢ç”¨
        
    def select_40_jobs(self):
        """
        40ä»¶ã‚’é¸å®šã—ã€6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æŒ¯ã‚Šåˆ†ã‘
        å„ªå…ˆé †ä½: ç·¨é›†éƒ¨ â†’ TOP5 â†’ åœ°åŸŸåˆ¥ â†’ è¿‘éš£ â†’ é«˜åå…¥ â†’ æ–°ç€
        """
        sections = {
            'editorial_picks': [],
            'top5': [],
            'regional': [],
            'nearby': [],
            'high_income': [],
            'new': []
        }
        
        # 0. ç·¨é›†éƒ¨ãŠã™ã™ã‚ã®é¸å®šï¼ˆæœ€å„ªå…ˆï¼‰
        sections['editorial_picks'] = self._select_editorial_picks(5)
        
        # 1. TOP5ã®é¸å®šï¼ˆãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºï¼‰
        sections['top5'] = self._select_top5()
        
        # 2. åœ°åŸŸåˆ¥æ±‚äººã®é¸å®šï¼ˆéƒ½é“åºœçœŒå†…ï¼‰
        sections['regional'] = self._select_regional(10)
        
        # 3. è¿‘éš£æ±‚äººã®é¸å®šï¼ˆå¸‚åŒºç”ºæ‘å‘¨è¾ºï¼‰
        sections['nearby'] = self._select_nearby(8)
        
        # 4. é«˜åå…¥ãƒ»æ—¥æ‰•ã„æ±‚äººã®é¸å®š
        sections['high_income'] = self._select_high_income(7)
        
        # 5. æ–°ç€æ±‚äººã®é¸å®š
        sections['new'] = self._select_new(5)
        
        # 40ä»¶ã«æº€ãŸãªã„å ´åˆã®è£œå……
        self._fill_shortage(sections)
        
        return sections
    
    def _select_editorial_picks(self, count):
        """ç·¨é›†éƒ¨ãŠã™ã™ã‚ï¼šfeeÃ—å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•°ã®ä¸Šä½"""
        user_location = self._get_user_location()
        
        # å€™è£œæ±‚äººã‚’å–å¾—
        candidates = self.jobs_df[
            ~self.jobs_df['job_id'].isin(self.selected_job_ids)
        ].copy()
        
        # ç·¨é›†éƒ¨ãŠã™ã™ã‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        candidates['editorial_score'] = candidates.apply(
            lambda job: calculate_editorial_popularity_score(job, user_location),
            axis=1
        )
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        candidates = candidates.nlargest(count * 3, 'editorial_score')
        
        selected = []
        for _, job in candidates.iterrows():
            if len(selected) >= count:
                break
            
            # 2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’é™¤å¤–
            if self._was_applied_within_2weeks(job.endcl_cd):
                continue
            
            selected.append(job.job_id)
            self.selected_job_ids.add(job.job_id)
        
        return selected
    
    def _select_top5(self):
        """TOP5: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ã®ä¸Šä½5ä»¶ï¼ˆ2é€±é–“å¿œå‹Ÿä¼æ¥­ã‚’é™¤å¤–ï¼‰"""
        user_location = self._get_user_location()
        
        candidates = self.scores_df[
            (self.scores_df['user_id'] == self.user.id) &
            (~self.scores_df['job_id'].isin(self.selected_job_ids))
        ].copy()
        
        # åœ°åŸŸé‡ã¿ä»˜ã‘ã‚’é©ç”¨
        candidates['weighted_score'] = candidates.apply(
            lambda row: row['total_score'] * get_location_weight(
                self.jobs_df[self.jobs_df['job_id'] == row['job_id']].iloc[0],
                user_location
            ),
            axis=1
        )
        
        candidates = candidates.nlargest(30, 'weighted_score')
        
        selected = []
        for _, row in candidates.iterrows():
            if len(selected) >= 5:
                break
            
            job = self.jobs_df[self.jobs_df['job_id'] == row['job_id']].iloc[0]
            
            # 2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’é™¤å¤–
            if self._was_applied_within_2weeks(job.endcl_cd):
                continue
            
            # éå»7æ—¥ä»¥å†…ã«æ¨è–¦ã—ã¦ã„ãªã„
            if not self._was_recently_recommended(job.job_id):
                selected.append(job.job_id)
                self.selected_job_ids.add(job.job_id)
        
        return selected
    
    def _select_nearby(self, count):
        """è¿‘éš£æ±‚äºº: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨å®šå¸‚åŒºç”ºæ‘ã¨ãã®éš£æ¥ã‚¨ãƒªã‚¢"""
        user_city = self._estimate_user_city()
        nearby_cities = self._get_nearby_cities(user_city)
        
        candidates = self.jobs_df[
            (self.jobs_df['city_id'].isin(nearby_cities)) &
            (~self.jobs_df['job_id'].isin(self.selected_job_ids))
        ].copy()
        
        # äººæ°—åº¦ï¼ˆå¿œå‹Ÿæ•°ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        candidates['popularity'] = candidates['recent_applications'] / candidates['recent_views'].clip(lower=1)
        candidates = candidates.nlargest(count * 2, 'popularity')
        
        selected = []
        for _, job in candidates.iterrows():
            if len(selected) >= count:
                break
            
            # 2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’é™¤å¤–
            if self._was_applied_within_2weeks(job.endcl_cd):
                continue
                
            selected.append(job.job_id)
            self.selected_job_ids.add(job.job_id)
        
        return selected
    
    def _select_regional(self, count):
        """åœ°åŸŸåˆ¥æ±‚äºº: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨å®šéƒ½é“åºœçœŒã‹ã‚‰ï¼ˆè·ç¨®ãƒãƒƒãƒãƒ³ã‚°é‡è¦–ï¼‰"""
        user_prefecture = self._estimate_user_prefecture()
        user_profile = self._get_user_profile()
        
        candidates = self.jobs_df[
            (self.jobs_df['prefecture_id'] == user_prefecture) &
            (~self.jobs_df['job_id'].isin(self.selected_job_ids))
        ].copy()
        
        # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ï¼ˆè·ç¨®ãƒãƒƒãƒãƒ³ã‚°å«ã‚€ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        if user_profile:
            candidates = candidates.merge(
                self.scores_df[['job_id', 'personalized_score']], 
                on='job_id'
            ).nlargest(count * 2, 'personalized_score')
        else:
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯åŸºç¤ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            candidates = candidates.merge(
                self.scores_df[['job_id', 'basic_score']], 
                on='job_id'
            ).nlargest(count * 2, 'basic_score')
        
        selected = []
        for _, job in candidates.iterrows():
            if len(selected) >= count:
                break
            
            # 2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’é™¤å¤–
            if self._was_applied_within_2weeks(job.endcl_cd):
                continue
                
            selected.append(job.job_id)
            self.selected_job_ids.add(job.job_id)
        
        return selected
    
    def _select_high_income(self, count):
        """é«˜åå…¥ãƒ»æ—¥æ‰•ã„ãƒã‚¤ãƒˆï¼šé«˜æ™‚çµ¦ã¾ãŸã¯æ—¥æ‰•ã„å¯èƒ½ãªæ±‚äºº"""
        user_location = self._get_user_location()
        
        candidates = self.jobs_df[
            ~self.jobs_df['job_id'].isin(self.selected_job_ids)
        ].copy()
        
        # é«˜åå…¥ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        area_avg_salary = self.area_stats.get('avg_salary', 1200)
        candidates['high_income_score'] = 0
        
        # é«˜æ™‚çµ¦ã‚¹ã‚³ã‚¢
        if 'min_salary' in candidates.columns and 'max_salary' in candidates.columns:
            candidates['avg_salary'] = (candidates['min_salary'] + candidates['max_salary']) / 2
            # ã‚¨ãƒªã‚¢å¹³å‡ã®1.2å€ä»¥ä¸Šã‚’é«˜æ™‚çµ¦ã¨ã™ã‚‹
            candidates.loc[candidates['avg_salary'] >= area_avg_salary * 1.2, 'high_income_score'] += 50
        
        # æ—¥æ‰•ã„å¯èƒ½ã‚¹ã‚³ã‚¢ï¼ˆfeature_codesã«212ãŒå«ã¾ã‚Œã‚‹å ´åˆï¼‰
        if 'feature_codes' in candidates.columns:
            candidates['has_daily_payment'] = candidates['feature_codes'].apply(
                lambda x: '212' in str(x).split(',') if x else False
            )
            candidates.loc[candidates['has_daily_payment'], 'high_income_score'] += 50
        
        # åœ°åŸŸé‡ã¿ä»˜ã‘ã‚’é©ç”¨
        candidates['weighted_score'] = candidates.apply(
            lambda job: job.high_income_score * get_location_weight(job, user_location),
            axis=1
        )
        
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        candidates = candidates[candidates['weighted_score'] > 0].nlargest(count * 2, 'weighted_score')
        
        selected = []
        for _, job in candidates.iterrows():
            if len(selected) >= count:
                break
            
            # 2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’é™¤å¤–
            if self._was_applied_within_2weeks(job.endcl_cd):
                continue
                
            selected.append(job.job_id)
            self.selected_job_ids.add(job.job_id)
        
        return selected
    
    def _select_new(self, count):
        """æ–°ç€æ±‚äºº: éå»1é€±é–“ä»¥å†…ã«æŠ•ç¨¿ï¼ˆã‚¹ã‚³ã‚¢ä¸Šä½å„ªå…ˆï¼‰"""
        from datetime import datetime, timedelta
        
        cutoff_date = datetime.now() - timedelta(days=7)
        user_location = self._get_user_location()
        user_profile = self._get_user_profile()
        
        candidates = self.jobs_df[
            (self.jobs_df['posting_date'] >= cutoff_date) &
            (~self.jobs_df['job_id'].isin(self.selected_job_ids))
        ].copy()
        
        # ã‚¹ã‚³ã‚¢ã¨åœ°åŸŸé‡ã¿ä»˜ã‘ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚½ãƒ¼ãƒˆ
        if user_profile:
            # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆ
            candidates = candidates.merge(
                self.scores_df[['job_id', 'personalized_score']], 
                on='job_id',
                how='left'
            )
            candidates['weighted_score'] = candidates.apply(
                lambda job: job.get('personalized_score', 50) * get_location_weight(job, user_location),
                axis=1
            )
        else:
            # åŸºç¤ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
            candidates = candidates.merge(
                self.scores_df[['job_id', 'basic_score']], 
                on='job_id',
                how='left'
            )
            candidates['weighted_score'] = candidates.apply(
                lambda job: job.get('basic_score', 50) * get_location_weight(job, user_location),
                axis=1
            )
        
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ç€ã®ä¸­ã§ã®å„ªå…ˆé †ä½ï¼‰
        candidates = candidates.nlargest(count * 3, 'weighted_score')
        
        selected = []
        for _, job in candidates.iterrows():
            if len(selected) >= count:
                break
            
            # 2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’é™¤å¤–
            if self._was_applied_within_2weeks(job.endcl_cd):
                continue
                
            selected.append(job.job_id)
            self.selected_job_ids.add(job.job_id)
        
        return selected
    
    def _fill_shortage(self, sections):
        """40ä»¶ã«æº€ãŸãªã„å ´åˆã®è£œå……"""
        total_count = sum(len(jobs) for jobs in sections.values())
        
        if total_count < 40:
            shortage = 40 - total_count
            
            # ã‚¹ã‚³ã‚¢é †ã§æœªé¸å®šã®æ±‚äººã‹ã‚‰è£œå……
            remaining = self.scores_df[
                (self.scores_df['user_id'] == self.user.id) &
                (~self.scores_df['job_id'].isin(self.selected_job_ids))
            ].nlargest(shortage, 'total_score')
            
            # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å‡ç­‰ã«é…åˆ†
            for i, (_, row) in enumerate(remaining.iterrows()):
                if i < shortage:
                    # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ã§å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
                    section_names = ['regional', 'nearby', 'high_income', 'new']
                    target_section = section_names[i % len(section_names)]
                    sections[target_section].append(row['job_id'])
                    self.selected_job_ids.add(row['job_id'])
    
    def _estimate_user_prefecture(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éƒ½é“åºœçœŒã‚’å¿œå‹Ÿå±¥æ­´ã‹ã‚‰æ¨å®š"""
        # user_actionsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æœ€é »å‡ºã®éƒ½é“åºœçœŒã‚’å–å¾—
        recent_applications = self._get_user_application_history()
        if recent_applications.empty:
            return 13  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ±äº¬éƒ½
        
        prefecture_counts = recent_applications['prefecture_id'].value_counts()
        return prefecture_counts.index[0] if not prefecture_counts.empty else 13
    
    def _estimate_user_city(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¸‚åŒºç”ºæ‘ã‚’å¿œå‹Ÿå±¥æ­´ã‹ã‚‰æ¨å®š"""
        recent_applications = self._get_user_application_history()
        if recent_applications.empty:
            return 13101  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ±äº¬éƒ½åƒä»£ç”°åŒº
        
        city_counts = recent_applications['city_id'].value_counts()
        return city_counts.index[0] if not city_counts.empty else 13101
    
    def _get_nearby_cities(self, city_id):
        """éš£æ¥å¸‚åŒºç”ºæ‘ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å®Ÿè£…ã§ã¯éš£æ¥ãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯åŒã˜éƒ½é“åºœçœŒã®å¸‚åŒºç”ºæ‘ã‚’è¿”ã™ç°¡æ˜“å®Ÿè£…
        prefecture_id = city_id // 1000
        nearby = [city_id]  # è‡ªèº«ã‚’å«ã‚€
        
        # åŒã˜éƒ½é“åºœçœŒã®ä»–ã®å¸‚åŒºç”ºæ‘ã‚’3ã¤ã¾ã§è¿½åŠ 
        all_cities = self.jobs_df[
            self.jobs_df['prefecture_id'] == prefecture_id
        ]['city_id'].unique()
        
        for city in all_cities[:4]:
            if city != city_id:
                nearby.append(city)
        
        return nearby
    
    def _was_recently_recommended(self, job_id):
        """éå»7æ—¥ä»¥å†…ã«æ¨è–¦æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        # job_processing_historyãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        # ç°¡æ˜“å®Ÿè£…ã§ã¯Falseã‚’è¿”ã™
        return False
    
    def _was_applied_within_2weeks(self, endcl_cd):
        """éå»2é€±é–“ä»¥å†…ã«å¿œå‹Ÿæ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        from datetime import datetime, timedelta
        
        # user_actionsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰2é€±é–“ä»¥å†…ã®å¿œå‹Ÿã‚’ãƒã‚§ãƒƒã‚¯
        cutoff_date = datetime.now() - timedelta(days=14)
        
        # ç°¡æ˜“å®Ÿè£…ï¼šapplied_endcl_cdsã®å†…å®¹ã¨æ—¥ä»˜ã‚’ãƒã‚§ãƒƒã‚¯
        recent_applications = self._get_recent_applications(cutoff_date)
        return endcl_cd in recent_applications
    
    def _get_recent_applications(self, cutoff_date):
        """æŒ‡å®šæ—¥ä»¥é™ã«å¿œå‹Ÿã—ãŸä¼æ¥­ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        # user_actionsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
        # ç°¡æ˜“å®Ÿè£…ã§ã¯set()ã‚’è¿”ã™
        return set()
    
    def _get_user_location(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åœ°åŸŸæƒ…å ±ã‚’å–å¾—"""
        user_prefecture = self._estimate_user_prefecture()
        user_city = self._estimate_user_city()
        nearby_cities = self._get_nearby_cities(user_city)
        
        return {
            'pref_cd': user_prefecture,
            'city_cd': user_city,
            'nearby_cities': nearby_cities
        }
    
    def _get_user_application_history(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿œå‹Ÿå±¥æ­´ã‚’å–å¾—"""
        # user_actionsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
        # ç°¡æ˜“å®Ÿè£…ã§ã¯ç©ºã®DataFrameã‚’è¿”ã™
        import pandas as pd
        return pd.DataFrame()
    
    def _get_applied_companies(self):
        """éå»ã«å¿œå‹Ÿã—ãŸä¼æ¥­ã®endcl_cdãƒªã‚¹ãƒˆã‚’å–å¾—"""
        # user_profilesãƒ†ãƒ¼ãƒ–ãƒ«ã®applied_endcl_cdsã‹ã‚‰å–å¾—
        # ã¾ãŸã¯user_actionsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç›´æ¥å–å¾—
        user_profile = self._get_user_profile()
        if user_profile and hasattr(user_profile, 'applied_endcl_cds'):
            # "EX12345678:3,EX87654321:1" å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            endcl_freq_str = user_profile.applied_endcl_cds
            if endcl_freq_str:
                applied_companies = set()
                for item in endcl_freq_str.split(','):
                    if ':' in item:
                        endcl_cd, count = item.split(':')
                        applied_companies.add(endcl_cd)
                return applied_companies
        return set()
    
    def _get_user_profile(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        # user_profilesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
        # ç°¡æ˜“å®Ÿè£…ã§ã¯Noneã‚’è¿”ã™
        return None
```

### 4. ãƒ¡ãƒ¼ãƒ«æ–‡é¢ã®5ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ‡ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯

#### 4.1 å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¡¨ç¤ºå†…å®¹ï¼ˆERDæº–æ‹ ï¼‰
```python
EMAIL_SECTIONS_CONFIG = {
    'editorial_picks': {
        'title': 'ç·¨é›†éƒ¨ãŠã™ã™ã‚ã®äººæ°—ãƒã‚¤ãƒˆ',
        'count': 5,
        'display_fields': {
            'title': True,          # æ±‚äººã‚¿ã‚¤ãƒˆãƒ«
            'company': True,        # ä¼šç¤¾å
            'salary': True,         # çµ¦ä¸
            'location': True,       # å‹¤å‹™åœ°
            'features': True,       # ç‰¹å¾´ã‚¿ã‚°ï¼ˆæœ€å¤§3å€‹ï¼‰
            'work_hours': True,     # å‹¤å‹™æ™‚é–“
            'access': True,         # ã‚¢ã‚¯ã‚»ã‚¹
            'popularity': True      # äººæ°—åº¦æŒ‡æ¨™
        }
    },
    'top5': {
        'title': 'ã‚ãªãŸã«ãŠã™ã™ã‚æ±‚äººTOP5',
        'count': 5,
        'display_fields': {
            'title': True,
            'company': True,
            'salary': True,
            'location': True,
            'features': True,       # ç‰¹å¾´ã‚¿ã‚°ï¼ˆæœ€å¤§3å€‹ï¼‰
            'work_hours': True,
            'access': True
        }
    },
    'regional': {
        'title': '{prefecture}ã®ãŠã™ã™ã‚æ±‚äººTOP10',
        'count': 10,
        'display_fields': {
            'title': True,
            'company': True,
            'salary': True,
            'location': False,      # éƒ½é“åºœçœŒã¯è‡ªæ˜
            'features': True,       # ç‰¹å¾´ã‚¿ã‚°ï¼ˆæœ€å¤§2å€‹ï¼‰
            'work_hours': False,
            'access': True
        }
    },
    'nearby': {
        'title': '{city}å‘¨è¾ºã®ãŠã™ã™ã‚ãƒã‚¤ãƒˆTOP10',
        'count': 8,  # 40ä»¶ã«åã‚ã‚‹ãŸã‚èª¿æ•´
        'display_fields': {
            'title': True,
            'company': True,
            'salary': True,
            'location': False,      # å¸‚åŒºç”ºæ‘ã¯è‡ªæ˜
            'features': True,       # ç‰¹å¾´ã‚¿ã‚°ï¼ˆæœ€å¤§2å€‹ï¼‰
            'work_hours': True,
            'access': True
        }
    },
    'high_income': {
        'title': 'é«˜åå…¥ãƒ»æ—¥æ‰•ã„ãƒã‚¤ãƒˆTOP10',
        'count': 7,  # 40ä»¶ã«åã‚ã‚‹ãŸã‚èª¿æ•´
        'display_fields': {
            'title': True,
            'company': True,
            'salary': True,         # çµ¦ä¸ã‚’å¼·èª¿
            'location': True,
            'features': True,       # ç‰¹å¾´ã‚¿ã‚°ï¼ˆæœ€å¤§3å€‹ï¼‰
            'work_hours': False,
            'access': False,
            'payment_type': True    # æ—¥æ‰•ã„å¯å¦
        }
    },
    'new': {
        'title': 'æ–°ç€æ±‚äºº',
        'count': 5,
        'display_fields': {
            'title': True,
            'company': True,
            'salary': True,
            'location': True,
            'features': True,       # ç‰¹å¾´ã‚¿ã‚°ï¼ˆæœ€å¤§2å€‹ï¼‰
            'work_hours': False,
            'access': False,
            'posting_date': True    # æŠ•ç¨¿æ—¥
        }
    }
}

def format_job_for_email(job, section_type, user_context=None):
    """
    ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦æ±‚äººæƒ…å ±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    """
    config = EMAIL_SECTIONS_CONFIG[section_type]
    display = config['display_fields']
    
    formatted = {
        'job_id': job.job_id,
        'url': f"https://example.com/jobs/{job.job_id}"
    }
    
    if display.get('title'):
        formatted['title'] = truncate_text(job.application_name, 50)
    
    if display.get('company'):
        formatted['company'] = job.company_name
    
    if display.get('salary'):
        formatted['salary'] = format_salary(job)
    
    if display.get('location'):
        formatted['location'] = f"{job.city_name}"
    
    if display.get('features'):
        max_tags = 3 if section_type in ['top5', 'benefits'] else 2
        formatted['features'] = extract_feature_tags(job)[:max_tags]
    
    if display.get('work_hours'):
        formatted['work_hours'] = remove_html_tags(job.hours) if hasattr(job, 'hours') else ''
    
    
    if display.get('access'):
        formatted['access'] = job.station_name_eki if hasattr(job, 'station_name_eki') else ''
    
    if display.get('benefits'):
        formatted['benefits'] = extract_benefits_detail(job)
    
    if display.get('posting_date'):
        formatted['posting_date'] = job.posting_date.strftime('%mæœˆ%dæ—¥')
    
    return formatted

def truncate_text(text, max_length):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šæ–‡å­—æ•°ã§åˆ‡ã‚Šè©°ã‚"""
    if len(text) <= max_length:
        return text
    return text[:max_length-3] + '...'

def format_salary(job):
    """çµ¦ä¸è¡¨ç¤ºã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    # min_salaryã¨max_salaryã‚’ä½¿ç”¨
    if job.min_salary and job.max_salary:
        if job.min_salary == job.max_salary:
            return f"æ™‚çµ¦{job.min_salary:,}å††"
        else:
            return f"æ™‚çµ¦{job.min_salary:,}å††ã€œ{job.max_salary:,}å††"
    elif job.min_salary:
        return f"æ™‚çµ¦{job.min_salary:,}å††ã€œ"
    elif job.max_salary:
        return f"æ™‚çµ¦ã€œ{job.max_salary:,}å††"
    else:
        return job.salary_text

def extract_feature_tags(job):
    """æ±‚äººã®ç‰¹å¾´ã‚¿ã‚°ã‚’æŠ½å‡ºï¼ˆfeature_codesã‹ã‚‰å¤‰æ›ï¼‰"""
    if not hasattr(job, 'feature_codes') or not job.feature_codes:
        return []
    
    # feature_masterå®šç¾©
    feature_master = {
        '103': 'æœªçµŒé¨“OK',
        '200': 'äº¤é€šè²»æ”¯çµ¦',
        '212': 'æ—¥æ‰•ã„OK',
        '217': 'é€±æ‰•ã„OK',
        '300': 'ã¾ã‹ãªã„æœ‰',
        '400': 'åˆ¶æœè²¸ä¸',
        '500': 'ç¤¾å“¡å‰²å¼•',
        '600': 'é«ªå‹è‡ªç”±',
        '700': 'ã‚·ãƒ•ãƒˆè‡ªç”±',
        '800': 'çŸ­æ™‚é–“OK',
        '900': 'é§…ãƒã‚«'  # 5åˆ†ä»¥å†…
    }
    
    tags = []
    # feature_codesã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ï¼ˆä¾‹: "103,200,300"ï¼‰
    feature_codes = str(job.feature_codes).split(',') if job.feature_codes else []
    
    for code in feature_codes:
        code = code.strip()
        if code in feature_master:
            tags.append(feature_master[code])
    
    return tags

def remove_html_tags(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    import re
    if not text:
        return ''
    # HTMLã‚¿ã‚°ã‚’é™¤å»
    clean_text = re.sub(r'<[^>]+>', '', str(text))
    # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
    clean_text = re.sub(r'\s+', ' ', clean_text)
    return clean_text.strip()

def extract_benefits_detail(job):
    """ç‰¹å…¸ã®è©³ç´°èª¬æ˜"""
    benefits = []
    
    if job.feature_bonus:
        benefits.append('å…¥ç¤¾ç¥ã„é‡‘ã‚ã‚Š')
    if job.meal_provided:
        benefits.append('ã¾ã‹ãªã„ç„¡æ–™')
    if job.employee_discount:
        benefits.append('ç¤¾å“¡å‰²å¼•ã‚ã‚Š')
    if job.uniform_provided:
        benefits.append('åˆ¶æœç„¡æ–™è²¸ä¸')
    
    return 'ã€'.join(benefits)
```

## ğŸŸ¡ å„ªå…ˆåº¦: ä¸­ï¼ˆå®Ÿè£…ã«å½±éŸ¿ï¼‰ã®å›ç­”

### 5. ãƒ‡ãƒ¼ã‚¿ç®¡ç†æˆ¦ç•¥

#### 5.1 é‡è¤‡æ±‚äººã®å‡¦ç†ï¼ˆendcl_cdãƒ™ãƒ¼ã‚¹ï¼‰
```python
DUPLICATE_HANDLING = {
    'strategy': 'update',  # 'update', 'skip', 'version'
    'duplicate_key': ['endcl_cd', ],
    'update_fields': 'all',  # 'all' ã¾ãŸã¯ç‰¹å®šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒªã‚¹ãƒˆ
    'keep_history': True
}

def handle_duplicate_jobs(new_job, existing_job):
    """
    é‡è¤‡æ±‚äººã®å‡¦ç†
    job_idãŒåŒã˜å ´åˆ: æ›´æ–°
    å†…å®¹ãŒåŒã˜ã§IDãŒç•°ãªã‚‹å ´åˆ: é‡è¤‡ã‚­ãƒ¼ã§åˆ¤å®š
    """
    if new_job.job_id == existing_job.job_id:
        # åŒä¸€IDã®å ´åˆã¯æ›´æ–°
        if DUPLICATE_HANDLING['keep_history']:
            # å±¥æ­´ã‚’ä¿å­˜
            save_job_history(existing_job)
        
        # å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ›´æ–°
        update_job(new_job)
        return 'updated'
    
    # é‡è¤‡ã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
    is_duplicate = all(
        getattr(new_job, key) == getattr(existing_job, key)
        for key in DUPLICATE_HANDLING['duplicate_key']
    )
    
    if is_duplicate:
        if DUPLICATE_HANDLING['strategy'] == 'skip':
            return 'skipped'
        elif DUPLICATE_HANDLING['strategy'] == 'update':
            # ã‚ˆã‚Šæ–°ã—ã„æƒ…å ±ã§æ›´æ–°
            if new_job.posting_date > existing_job.posting_date:
                update_job(new_job)
                return 'updated'
        
    return 'new'
```

#### 5.2 ãƒ‡ãƒ¼ã‚¿æ›´æ–°æˆ¦ç•¥
```python
UPDATE_STRATEGY = {
    'method': 'soft_delete',    # 'soft_delete' or 'hard_delete'
    'update_type': 'full',      # 'full' or 'differential'
    'history_retention': 30,    # å±¥æ­´ä¿æŒæ—¥æ•°
    'archive_old_data': True
}

def update_existing_job(job_id, new_data):
    """æ—¢å­˜æ±‚äººã®æ›´æ–°"""
    # ã‚½ãƒ•ãƒˆãƒ‡ãƒªãƒ¼ãƒˆï¼ˆis_activeãƒ•ãƒ©ã‚°ï¼‰
    if UPDATE_STRATEGY['method'] == 'soft_delete':
        # is_activeã‚’falseã«è¨­å®š
        deactivate_job(job_id)
        # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æŒ¿å…¥
        create_job(new_data)
    else:
        # å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸Šæ›¸ã
        overwrite_job(job_id, new_data)
    
    # å¤‰æ›´å±¥æ­´ã‚’è¨˜éŒ²
    if UPDATE_STRATEGY['archive_old_data']:
        archive_job_change(job_id, new_data)
```

### 6. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

#### 6.1 ãƒãƒƒãƒå‡¦ç†ã®ä¸­æ–­ã¨å†é–‹
```python
BATCH_CONFIG = {
    'checkpoint_interval': 100,  # 100ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    'retry_attempts': 3,
    'retry_delay': 60,  # ç§’
    'failure_threshold': 0.1,  # 10%ä»¥ä¸Šå¤±æ•—ã§åœæ­¢
    'recovery_mode': 'checkpoint'  # 'checkpoint' or 'restart'
}

class BatchProcessor:
    def __init__(self):
        self.checkpoint_file = 'batch_checkpoint.json'
        self.failed_users = []
        
    def process_batch(self, users):
        """ãƒãƒƒãƒå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³"""
        checkpoint = self.load_checkpoint()
        start_index = checkpoint.get('last_processed_index', 0)
        
        for i, user in enumerate(users[start_index:], start=start_index):
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‡¦ç†
                self.process_user(user)
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if i % BATCH_CONFIG['checkpoint_interval'] == 0:
                    self.save_checkpoint({
                        'last_processed_index': i,
                        'failed_users': self.failed_users,
                        'timestamp': datetime.now().isoformat()
                    })
                    
            except Exception as e:
                self.failed_users.append({
                    'user_id': user.id,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                
                # å¤±æ•—ç‡ãƒã‚§ãƒƒã‚¯
                failure_rate = len(self.failed_users) / (i + 1)
                if failure_rate > BATCH_CONFIG['failure_threshold']:
                    raise BatchProcessingError(f"å¤±æ•—ç‡ãŒé–¾å€¤ã‚’è¶…ãˆã¾ã—ãŸ: {failure_rate:.2%}")
        
        # å¤±æ•—ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å†å‡¦ç†
        self.retry_failed_users()
    
    def retry_failed_users(self):
        """å¤±æ•—ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å†å‡¦ç†"""
        for attempt in range(BATCH_CONFIG['retry_attempts']):
            if not self.failed_users:
                break
                
            time.sleep(BATCH_CONFIG['retry_delay'])
            
            retry_list = self.failed_users.copy()
            self.failed_users = []
            
            for failed_user in retry_list:
                try:
                    user = get_user_by_id(failed_user['user_id'])
                    self.process_user(user)
                except Exception as e:
                    self.failed_users.append(failed_user)
```

#### 6.2 ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆå¤±æ•—æ™‚ã®å‡¦ç†
```python
EMAIL_ERROR_HANDLING = {
    'continue_on_failure': True,  # ä¸€éƒ¨å¤±æ•—ã§ã‚‚ç¶™ç¶š
    'max_retries': 3,
    'retry_delay': 30,  # ç§’
    'log_location': 'logs/email_failures.json',
    'alert_threshold': 100  # 100ä»¶ä»¥ä¸Šå¤±æ•—ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
}

def generate_emails_with_error_handling(users):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ããƒ¡ãƒ¼ãƒ«ç”Ÿæˆ"""
    results = {
        'success': [],
        'failed': [],
        'retried': []
    }
    
    for user in users:
        success = False
        last_error = None
        
        for attempt in range(EMAIL_ERROR_HANDLING['max_retries']):
            try:
                email_content = generate_email_for_user(user)
                save_to_queue(user.id, email_content)
                results['success'].append(user.id)
                success = True
                break
                
            except Exception as e:
                last_error = e
                if attempt < EMAIL_ERROR_HANDLING['max_retries'] - 1:
                    time.sleep(EMAIL_ERROR_HANDLING['retry_delay'])
                    results['retried'].append({
                        'user_id': user.id,
                        'attempt': attempt + 1,
                        'error': str(e)
                    })
        
        if not success:
            results['failed'].append({
                'user_id': user.id,
                'error': str(last_error),
                'timestamp': datetime.now().isoformat()
            })
            
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜
            log_email_failure(user.id, last_error)
    
    # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
    if len(results['failed']) >= EMAIL_ERROR_HANDLING['alert_threshold']:
        send_alert(f"ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆå¤±æ•—ãŒ{len(results['failed'])}ä»¶ç™ºç”Ÿ")
    
    return results
```

### 7. ãƒ¡ãƒ¼ãƒ«å†…å®¹ã®è©³ç´°

#### 7.0 ãƒ¡ãƒ¼ãƒ«ä»¶åç”ŸæˆAIï¼ˆGPT-5 nanoï¼‰

```python
# GPT-5 nano è¨­å®šï¼ˆå‚è€ƒæ–‡é¢ã‚’æ´»ç”¨ã—ãŸå­¦ç¿’ä»˜ãï¼‰
AI_CONFIG = {
    'model': 'gpt-5-nano',
    'api_endpoint': 'https://api.openai.com/v1/chat/completions',
    'api_key_location': 'OPENAI_API_KEY',  # ç’°å¢ƒå¤‰æ•°ã§ç®¡ç†
    'max_tokens': 60,  # ä»¶åã¯çŸ­ã„ã®ã§60ãƒˆãƒ¼ã‚¯ãƒ³ã§ååˆ†
    'temperature': 0.7,  # é©åº¦ãªå‰µé€ æ€§ï¼ˆ0.7ï¼‰
    'top_p': 0.9,
    'frequency_penalty': 0.3,  # ç¹°ã‚Šè¿”ã—ã‚’é¿ã‘ã‚‹
    'presence_penalty': 0.3,
    'timeout': 5,  # 5ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    'retry_count': 2,
    'fallback_strategy': 'template'  # å¤±æ•—æ™‚ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨
}

def generate_email_subject(user_data, job_selections):
    """
    GPT-5 nanoã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¼ãƒ«ä»¶åã‚’ç”Ÿæˆ
    """
    import openai
    import os
    
    # APIè¨­å®š
    openai.api_key = os.getenv(AI_CONFIG['api_key_location'])
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
    context = prepare_subject_context(user_data, job_selections)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    prompt = build_subject_prompt(context)
    
    try:
        response = openai.ChatCompletion.create(
            model=AI_CONFIG['model'],
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯é–‹å°ç‡ã®é«˜ã„ãƒ¡ãƒ¼ãƒ«ä»¶åã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=AI_CONFIG['max_tokens'],
            temperature=AI_CONFIG['temperature'],
            top_p=AI_CONFIG['top_p'],
            frequency_penalty=AI_CONFIG['frequency_penalty'],
            presence_penalty=AI_CONFIG['presence_penalty'],
            timeout=AI_CONFIG['timeout']
        )
        
        subject = response.choices[0].message.content.strip()
        
        # ä»¶åã®æ¤œè¨¼
        if validate_subject(subject):
            return subject
        else:
            return generate_fallback_subject(context)
            
    except Exception as e:
        logger.error(f"GPT-5 nano ã‚¨ãƒ©ãƒ¼: {e}")
        return generate_fallback_subject(context)

def prepare_subject_context(user_data, job_selections):
    """
    ä»¶åç”Ÿæˆç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
    """
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨å®šåœ°åŸŸï¼ˆå¿œå‹Ÿå±¥æ­´ã‹ã‚‰ï¼‰
    user_area = estimate_user_area(user_data)
    
    # ã‚ˆãå¿œå‹Ÿã™ã‚‹ã‚«ãƒ†ã‚´ãƒªï¼ˆä¸Šä½3ã¤ï¼‰
    frequent_categories = get_frequent_categories(user_data, limit=3)
    
    # æœ€è¿‘ã®å¿œå‹Ÿï¼ˆ5ä»¶åˆ†ã®ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ï¼‰
    recent_applications = get_recent_applications(user_data, limit=5)
    
    # TOP5æ±‚äººã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæœ€åˆã®3ã¤ï¼‰
    top5_titles = [job['title'][:20] for job in job_selections['top5'][:3]]
    
    # æœ€ã‚‚ç‰¹å¾´çš„ãªæ±‚äººï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ã®1ä»¶ï¼‰
    featured_job = job_selections['top5'][0]['title'] if job_selections['top5'] else None
    
    # æ–°ç€æ±‚äººæ•°
    new_count = len(job_selections.get('new', []))
    
    return {
        'user_area': user_area,
        'frequent_categories': frequent_categories,
        'recent_applications': recent_applications,
        'top5_jobs': top5_titles,
        'featured_job': featured_job,
        'new_count': new_count,
        'total_count': sum(len(jobs) for jobs in job_selections.values())
    }

def build_subject_prompt(context):
    """
    GPT-5 nanoç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆå‚è€ƒæ–‡é¢ä»˜ãï¼‰
    """
    # å‚è€ƒã¨ãªã‚‹å„ªè‰¯ä»¶å5ã¤
    reference_subjects = [
        'ã€ãƒã‚¤ãƒˆé€Ÿå ±ã€‘æ¸‹è°·ã‚¨ãƒªã‚¢ã®é«˜æ™‚çµ¦æ±‚äºº35ä»¶ï¼‹æ–°ç€5ä»¶',
        'ã€ä»Šé€±ã®æ³¨ç›®ã€‘æœªçµŒé¨“OKÃ—æ—¥æ‰•ã„å¯ã®äººæ°—æ±‚äºº40ä»¶',
        'ã€æ–°ç€ã‚ã‚Šã€‘ã‚ãªãŸã®è¡—ã®é€±1ã€œOKãƒã‚¤ãƒˆ38ä»¶',
        'ã€å³é¸40ä»¶ã€‘å¤ä¼‘ã¿çŸ­æœŸãƒã‚¤ãƒˆï¼†é«˜æ™‚çµ¦ç‰¹é›†',
        'ã€æœ¬æ—¥é…ä¿¡ã€‘é£²é£Ÿãƒ»è²©å£²ãƒ»ã‚ªãƒ•ã‚£ã‚¹ç³»ãŠã™ã™ã‚æ±‚äºº'
    ]
    
    prompt = f"""
ãƒ¡ãƒ¼ãƒ«ä»¶åã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å‚è€ƒã¨ãªã‚‹å„ªè‰¯ä»¶å:
{chr(10).join(f"- {subject}" for subject in reference_subjects)}

æ¡ä»¶:
- æœ€å¤§50æ–‡å­—
- çµµæ–‡å­—ã¯ä½¿ç”¨ã—ãªã„
- ã€ã€‘ã§é‡è¦éƒ¨åˆ†ã‚’å›²ã‚€
- æ•°å­—ã‚’å«ã‚ã‚‹ï¼ˆæ±‚äººæ•°ãªã©ï¼‰
- å‚è€ƒä»¶åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¤ã¤ç‹¬è‡ªæ€§ã‚‚å‡ºã™

ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±:
- åœ°åŸŸ: {context['user_area']}
- ã‚ˆãè¦‹ã‚‹ã‚«ãƒ†ã‚´ãƒª: {', '.join(context['frequent_categories'][:2])}
- æ¨è–¦æ±‚äººæ•°: {context['total_count']}ä»¶

ç‰¹å¾´çš„ãªæ±‚äºº:
- {context['featured_job']}

ä»¶å:"""
    
    return prompt

def validate_subject(subject):
    """
    ç”Ÿæˆã•ã‚ŒãŸä»¶åã®æ¤œè¨¼
    """
    # é•·ã•ãƒã‚§ãƒƒã‚¯
    if len(subject) > 50:
        return False
    
    # ç¦æ­¢æ–‡å­—ãƒã‚§ãƒƒã‚¯
    forbidden_chars = ['ğŸ˜€', 'ğŸ‰', 'ğŸ’°', '!', 'ï¼ï¼']
    if any(char in subject for char in forbidden_chars):
        return False
    
    # æœ€ä½é™ã®å†…å®¹ãƒã‚§ãƒƒã‚¯
    if len(subject) < 10:
        return False
    
    return True

def generate_fallback_subject(context):
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä»¶åç”Ÿæˆ
    """
    templates = [
        'ã€ãƒã‚¤ãƒˆé€Ÿå ±ã€‘{user_area}ã®ãŠã™ã™ã‚æ±‚äºº{total_count}ä»¶',
        'ã€{user_area}ã€‘ä»Šé€±ã®æ³¨ç›®ãƒã‚¤ãƒˆ{total_count}ä»¶ã‚’ãŠå±Šã‘',
        'ã€æ–°ç€ã‚ã‚Šã€‘{user_area}ã‚¨ãƒªã‚¢ã®äººæ°—æ±‚äºº{total_count}ä»¶',
        'ã€{frequent_category}ã€‘ä»–ã€ãŠã™ã™ã‚æ±‚äºº{total_count}ä»¶'
    ]
    
    import random
    template = random.choice(templates)
    
    # ã‚«ãƒ†ã‚´ãƒªé¸æŠ
    frequent_category = context['frequent_categories'][0] if context['frequent_categories'] else 'äººæ°—'
    
    return template.format(
        user_area=context['user_area'],
        total_count=context['total_count'],
        frequent_category=frequent_category
    )

# ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
BATCH_AI_CONFIG = {
    'method': 'individual_with_cache',  # å€‹åˆ¥ç”Ÿæˆã ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨
    'rate_limit': 100,  # 100ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
    'cache_similar_users': True,  # é¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ä»¶åå†åˆ©ç”¨
    'cache_ttl': 3600,  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    'parallel_requests': 10,  # ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
    'batch_size': 100  # 100ãƒ¦ãƒ¼ã‚¶ãƒ¼ãšã¤å‡¦ç†
}

def generate_subjects_batch(users_batch):
    """
    ãƒãƒƒãƒå‡¦ç†ã§ã®ä»¶åç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import hashlib
    
    results = {}
    cache = {}
    
    def get_cache_key(context):
        """é¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¤å®šç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_parts = [
            context['user_area'],
            ','.join(sorted(context['frequent_categories'][:2])),
            str(context['total_count'] // 10 * 10)  # 10ä»¶å˜ä½ã§ä¸¸ã‚ã‚‹
        ]
        return hashlib.md5('|'.join(key_parts).encode()).hexdigest()
    
    with ThreadPoolExecutor(max_workers=BATCH_AI_CONFIG['parallel_requests']) as executor:
        futures = {}
        
        for user in users_batch:
            context = prepare_subject_context(user.data, user.job_selections)
            cache_key = get_cache_key(context)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if BATCH_AI_CONFIG['cache_similar_users'] and cache_key in cache:
                results[user.id] = cache[cache_key]
                continue
            
            # éåŒæœŸå®Ÿè¡Œ
            future = executor.submit(generate_email_subject, user.data, user.job_selections)
            futures[future] = (user.id, cache_key)
        
        # çµæœåé›†
        for future in as_completed(futures):
            user_id, cache_key = futures[future]
            try:
                subject = future.result(timeout=10)
                results[user_id] = subject
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                if BATCH_AI_CONFIG['cache_similar_users']:
                    cache[cache_key] = subject
                    
            except Exception as e:
                logger.error(f"User {user_id} ä»¶åç”Ÿæˆå¤±æ•—: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                context = prepare_subject_context(
                    get_user_by_id(user_id).data,
                    get_user_by_id(user_id).job_selections
                )
                results[user_id] = generate_fallback_subject(context)
    
    return results

# A/Bãƒ†ã‚¹ãƒˆå¯¾å¿œ
AB_TEST_CONFIG = {
    'enabled': False,  # åˆæœŸã¯ç„¡åŠ¹
    'variation_count': 2,  # A/B ã®2ãƒ‘ã‚¿ãƒ¼ãƒ³
    'test_percentage': 10,  # 10%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
    'metrics': ['open_rate', 'click_rate']  # è¿½è·¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
}

def generate_subject_with_ab_test(user_data, job_selections):
    """
    A/Bãƒ†ã‚¹ãƒˆå¯¾å¿œã®ä»¶åç”Ÿæˆ
    """
    if not AB_TEST_CONFIG['enabled']:
        return generate_email_subject(user_data, job_selections)
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹åˆ¤å®š
    import random
    if random.random() > AB_TEST_CONFIG['test_percentage'] / 100:
        return generate_email_subject(user_data, job_selections)
    
    # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
    variations = []
    for i in range(AB_TEST_CONFIG['variation_count']):
        # temperatureã‚’å¤‰ãˆã¦ç”Ÿæˆ
        original_temp = AI_CONFIG['temperature']
        AI_CONFIG['temperature'] = 0.5 + i * 0.2  # 0.5, 0.7, 0.9...
        
        subject = generate_email_subject(user_data, job_selections)
        variations.append(subject)
        
        AI_CONFIG['temperature'] = original_temp
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠã—ã¦ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
    selected = random.choice(variations)
    track_ab_test(user_data['user_id'], selected, variations)
    
    return selected
```

#### 7.1 HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ 
```python
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{subject}</title>
    <style>
        /* ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³CSSï¼ˆGmailã‚µãƒãƒ¼ãƒˆï¼‰ */
        body {{ font-family: 'Hiragino Sans', sans-serif; }}
        .container {{ max-width: 600px; margin: 0 auto; }}
        .header {{ background: #4A90E2; color: white; padding: 20px; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #e0e0e0; }}
        .job-card {{ margin: 10px 0; padding: 10px; background: #f9f9f9; }}
        .tag {{ display: inline-block; padding: 2px 8px; background: #e3f2fd; 
                border-radius: 12px; font-size: 12px; margin: 0 4px; }}
        .footer {{ text-align: center; padding: 20px; color: #666; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ãƒã‚¤ãƒˆé€Ÿå ± - {date}</h1>
        </div>
        
        <div class="greeting">
            <p>ã“ã‚“ã«ã¡ã¯ï¼</p>
            <p>ä»Šé€±ã®ãŠã™ã™ã‚ãƒã‚¤ãƒˆæƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚</p>
        </div>
        
        {sections_html}
        
        <div class="footer">
            <p>é…ä¿¡åœæ­¢ã¯<a href="{unsubscribe_url}">ã“ã¡ã‚‰</a></p>
            <p>Â© 2025 ãƒã‚¤ãƒˆé€Ÿå ±</p>
        </div>
    </div>
</body>
</html>
"""
```

#### 7.2 æ–‡å­—æ•°åˆ¶é™
```python
EMAIL_CONSTRAINTS = {
    'subject': {
        'max_chars': 50,  # ä»¶åã¯50æ–‡å­—ä»¥å†…
        'template': 'ã€ãƒã‚¤ãƒˆé€Ÿå ±ã€‘{user_area}ã®ãŠã™ã™ã‚æ±‚äºº{count}ä»¶'
    },
    'job_description': {
        'max_chars': 100,  # å„æ±‚äººã®èª¬æ˜ã¯100æ–‡å­—
        'truncate': True
    },
    'total_size': {
        'max_kb': 102,  # Gmailç­‰ã®åˆ¶é™ï¼ˆ102KBä»¥ä¸‹æ¨å¥¨ï¼‰
        'warning_kb': 90  # 90KBè¶…ãˆã§è­¦å‘Š
    }
}

def validate_email_size(html_content):
    """ãƒ¡ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã®æ¤œè¨¼"""
    size_kb = len(html_content.encode('utf-8')) / 1024
    
    if size_kb > EMAIL_CONSTRAINTS['total_size']['max_kb']:
        # ã‚µã‚¤ã‚ºè¶…éæ™‚ã¯ç”»åƒã‚’å‰Šé™¤ã€èª¬æ˜ã‚’çŸ­ç¸®
        html_content = reduce_email_size(html_content)
    elif size_kb > EMAIL_CONSTRAINTS['total_size']['warning_kb']:
        logger.warning(f"ãƒ¡ãƒ¼ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„: {size_kb:.1f}KB")
    
    return html_content
```

## ğŸŸ¢ å„ªå…ˆåº¦: ä½ï¼ˆæœ€é©åŒ–é–¢é€£ï¼‰ã®å›ç­”

### 8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### 8.1 ãƒ¡ãƒ¢ãƒªç®¡ç†ã®å…·ä½“ç­–
```python
MEMORY_OPTIMIZATION = {
    'pandas_dtypes': {
        'job_id': 'int32',
        'user_id': 'int32',
        'min_salary': 'float32',
        'max_salary': 'float32',
        'prefecture_id': 'int16',
        'city_id': 'int32',
        'is_active': 'bool',
        'company_name': 'category',  # ã‚«ãƒ†ã‚´ãƒªå‹ã§å¤§å¹…å‰Šæ¸›
        'job_category': 'category'
    },
    'chunk_size': 1000,  # 1000è¡Œãšã¤å‡¦ç†
    'gc_frequency': 100  # 100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«GC
}

def optimize_dataframe_memory(df):
    """DataFrameã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
    for col, dtype in MEMORY_OPTIMIZATION['pandas_dtypes'].items():
        if col in df.columns:
            if dtype == 'category':
                df[col] = df[col].astype('category')
            else:
                df[col] = df[col].astype(dtype)
    
    # ä¸è¦ã‚«ãƒ©ãƒ ã®å³åº§å‰Šé™¤
    unnecessary_cols = ['temp_col', 'debug_info']
    df.drop(columns=[col for col in unnecessary_cols if col in df.columns], inplace=True)
    
    return df
```

#### 8.2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
```python
CACHE_CONFIG = {
    'backend': 'supabase',  # Supabase Edge Functionsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    'ttl': {
        'user_preferences': 3600,    # 1æ™‚é–“
        'area_stats': 86400,         # 24æ™‚é–“
        'job_scores': 1800,          # 30åˆ†
        'als_model': 21600           # 6æ™‚é–“
    },
    'key_pattern': 'job_match:{type}:{id}:{date}'
}

def get_cached_or_compute(cache_key, compute_func, ttl=3600):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¾ãŸã¯è¨ˆç®—"""
    # Supabaseã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cached = supabase_cache.get(cache_key)
    
    if cached and not is_expired(cached, ttl):
        return cached['data']
    
    # è¨ˆç®—ã—ã¦ä¿å­˜
    result = compute_func()
    supabase_cache.set(cache_key, {
        'data': result,
        'timestamp': datetime.now().isoformat()
    }, ttl=ttl)
    
    return result
```

### 9. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°UI

#### 9.1 èªè¨¼æ–¹å¼
```python
AUTH_CONFIG = {
    'method': 'supabase_auth',  # Supabase Authä½¿ç”¨
    'allowed_roles': ['admin', 'operator'],
    'session_timeout': 3600,  # 1æ™‚é–“
    'require_2fa': False
}

# Next.js middleware.ts
async function authenticate(req) {
    const token = req.headers.authorization?.split(' ')[1]
    
    if (!token) {
        return { authorized: false }
    }
    
    // Supabase Authæ¤œè¨¼
    const { data: user, error } = await supabase.auth.getUser(token)
    
    if (error || !AUTH_CONFIG['allowed_roles'].includes(user.role)) {
        return { authorized: false }
    }
    
    return { authorized: true, user }
}
```

#### 9.2 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°
```python
REALTIME_CONFIG = {
    'method': 'supabase_realtime',  # Supabaseã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿèƒ½
    'channels': ['batch_progress', 'error_alerts'],
    'polling_fallback': 5000,  # 5ç§’ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
}

# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆReactï¼‰
useEffect(() => {
    const channel = supabase
        .channel('batch_progress')
        .on('postgres_changes', {
            event: 'UPDATE',
            schema: 'public',
            table: 'batch_status'
        }, (payload) => {
            setProgress(payload.new.progress)
        })
        .subscribe()
    
    return () => {
        supabase.removeChannel(channel)
    }
}, [])
```

### 10. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿

#### 10.1 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæˆ¦ç•¥ï¼ˆä»®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰


```python
TEST_DATA_CONFIG = {
    'job_count': 100000,
    'user_count': 10000,
    'method': 'specified_users_plus_faker',  # æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ + Fakerç”Ÿæˆ
    'seed': 42,  # å†ç¾æ€§ç¢ºä¿
    'realistic_patterns': True
}

# æŒ‡å®šã•ã‚ŒãŸ9äººã®ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼
SPECIFIED_TEST_USERS = [
    {
        'email': 'koganei@gmail.com',
        'prefecture': 'æ±äº¬éƒ½',
        'pref_cd': 13,
        'city': 'å°é‡‘äº•å¸‚',
        'city_cd': 13210,
        'age': 24,
        'age_range': '20-24',
        'gender': 'ç”·æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': 'æ—¥æ‰•ã„',
        'preferred_occupation': None
    },
    {
        'email': 'shinjuku@gmail.com',
        'prefecture': 'æ±äº¬éƒ½',
        'pref_cd': 13,
        'city': 'æ–°å®¿åŒº',
        'city_cd': 13104,
        'age': 22,
        'age_range': '20-24',
        'gender': 'ç”·æ€§',
        'user_type': 'å¤§å­¦ç”Ÿ',
        'preferred_needs': 'é«˜åå…¥',
        'preferred_occupation': None
    },
    {
        'email': 'machida@gmail.com',
        'prefecture': 'æ±äº¬éƒ½',
        'pref_cd': 13,
        'city': 'ç”ºç”°å¸‚',
        'city_cd': 13209,
        'age': 25,
        'age_range': '25-29',
        'gender': 'å¥³æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': None,
        'preferred_occupation': 'ãƒ•ãƒ¼ãƒ‰'
    },
    {
        'email': 'yokohama@gmail.com',
        'prefecture': 'ç¥å¥ˆå·çœŒ',
        'pref_cd': 14,
        'city': 'æ¨ªæµœå¸‚',
        'city_cd': 14100,
        'age': 26,
        'age_range': '25-29',
        'gender': 'ç”·æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': 'é«˜åå…¥',
        'preferred_occupation': None
    },
    {
        'email': 'ube@gmail.com',
        'prefecture': 'å±±å£çœŒ',
        'pref_cd': 35,
        'city': 'å®‡éƒ¨å¸‚',
        'city_cd': 35202,
        'age': 20,
        'age_range': '20-24',
        'gender': 'ç”·æ€§',
        'user_type': 'å­¦ç”Ÿ',
        'preferred_needs': None,
        'preferred_occupation': 'è»½ä½œæ¥­'
    },
    {
        'email': 'osaka@gmail.com',
        'prefecture': 'å¤§é˜ªåºœ',
        'pref_cd': 27,
        'city': 'å¤§é˜ªå¸‚',
        'city_cd': 27100,
        'age': 23,
        'age_range': '20-24',
        'gender': 'å¥³æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': 'æ—¥æ‰•ã„',
        'preferred_occupation': None
    },
    {
        'email': 'fukuoka@gmail.com',
        'prefecture': 'ç¦å²¡çœŒ',
        'pref_cd': 40,
        'city': 'ç¦å²¡å¸‚',
        'city_cd': 40130,
        'age': 24,
        'age_range': '20-24',
        'gender': 'ç”·æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': 'é«˜åå…¥',
        'preferred_occupation': None
    },
    {
        'email': 'kaigo@gmail.com',
        'prefecture': 'æ±äº¬éƒ½',
        'pref_cd': 13,
        'city': 'å°é‡‘äº•å¸‚',
        'city_cd': 13210,
        'age': 26,
        'age_range': '25-29',
        'gender': 'å¥³æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': None,
        'preferred_occupation': 'ä»‹è­·'
    },
    {
        'email': 'juku@gmail.com',
        'prefecture': 'æ±äº¬éƒ½',
        'pref_cd': 13,
        'city': 'å°é‡‘äº•å¸‚',
        'city_cd': 13210,
        'age': 26,
        'age_range': '25-29',
        'gender': 'å¥³æ€§',
        'user_type': 'ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼',
        'preferred_needs': None,
        'preferred_occupation': 'å¡¾'
    }
]

def generate_test_data():
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    from faker import Faker
    import numpy as np
    
    fake = Faker('ja_JP')
    Faker.seed(TEST_DATA_CONFIG['seed'])
    np.random.seed(TEST_DATA_CONFIG['seed'])
    
    # ãƒªã‚¢ãƒ«ãªåˆ†å¸ƒãƒ‘ã‚¿ãƒ¼ãƒ³
    prefecture_weights = get_realistic_prefecture_distribution()
    category_weights = get_realistic_category_distribution()
    
    jobs = []
    for i in range(TEST_DATA_CONFIG['job_count']):
        job = {
            'job_id': i + 1,
            'company_name': fake.company(),
            'application_name': generate_realistic_job_title(fake, category_weights),
            'min_salary': np.random.normal(1000, 200),  # æ­£è¦åˆ†å¸ƒ
            'max_salary': np.random.normal(1400, 300),  # æ­£è¦åˆ†å¸ƒ
            'prefecture_id': np.random.choice(47, p=prefecture_weights),
            'posting_date': fake.date_between('-30d', 'today'),
            # ... ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        }
        jobs.append(job)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    users = generate_users_with_behavior_patterns(TEST_DATA_CONFIG['user_count'])
    
    return jobs, users

def generate_users_with_behavior_patterns(count):
    """ãƒªã‚¢ãƒ«ãªè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆï¼ˆæŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼å«ã‚€ï¼‰"""
    import uuid
    from datetime import datetime, timedelta
    import random
    
    patterns = {
        'active': 0.2,      # 20%ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼
        'regular': 0.5,     # 50%ãŒå®šæœŸçš„
        'occasional': 0.3   # 30%ãŒæ™‚ã€…
    }
    
    users = []
    
    # ã¾ãšæŒ‡å®šã•ã‚ŒãŸ9äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ 
    for i, spec_user in enumerate(SPECIFIED_TEST_USERS):
        user = {
            'user_id': str(uuid.uuid4()),
            'email': spec_user['email'],
            'age_range': spec_user['age_range'],
            'gender': spec_user['gender'],
            'is_active': True,
            'created_at': datetime.now() - timedelta(days=random.randint(30, 365)),
            'updated_at': datetime.now()
        }
        users.append(user)
    
    # æ®‹ã‚Šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’Fakerã§ç”Ÿæˆ
    fake = Faker('ja_JP')
    for i in range(len(SPECIFIED_TEST_USERS), count):
        pattern = np.random.choice(list(patterns.keys()), p=list(patterns.values()))
        user = {
            'user_id': str(uuid.uuid4()),
            'email': f'user{i+1}@example.com',
            'age_range': np.random.choice(['20-24', '25-29', '30-34', '35-39']),
            'gender': np.random.choice(['ç”·æ€§', 'å¥³æ€§']),
            'is_active': True,
            'created_at': datetime.now() - timedelta(days=random.randint(30, 365)),
            'updated_at': datetime.now()
        }
        users.append(user)
    
    return users

def generate_user_actions_for_test():
    """ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿œå‹Ÿå±¥æ­´ã‚’ç”Ÿæˆ"""
    import uuid
    from datetime import datetime, timedelta
    import random
    
    actions = []
    
    # æŒ‡å®šã•ã‚ŒãŸ9äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”¨ã®å¿œå‹Ÿå±¥æ­´
    for i, user in enumerate(SPECIFIED_TEST_USERS):
        # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«5-20ä»¶ã®å¿œå‹Ÿå±¥æ­´ã‚’ç”Ÿæˆ
        num_applications = random.randint(5, 20)
        
        for j in range(num_applications):
            # ãƒ‹ãƒ¼ã‚ºã«åŸºã¥ã„ãŸæ±‚äººé¸æŠ
            if user['preferred_needs'] == 'æ—¥æ‰•ã„':
                salary_type_cd = random.choice([1, 2])  # æ™‚çµ¦ã¾ãŸã¯æ—¥çµ¦
                fee = random.randint(0, 1000)
            elif user['preferred_needs'] == 'é«˜åå…¥':
                salary_type_cd = 1  # æ™‚çµ¦
                fee = random.randint(1000, 5000)
            else:
                salary_type_cd = random.choice([1, 2, 3])
                fee = random.randint(0, 3000)
            
            # è·ç¨®ã«åŸºã¥ã„ãŸé¸æŠï¼ˆ100å˜ä½ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ï¼‰
            if user['preferred_occupation'] == 'ãƒ•ãƒ¼ãƒ‰':
                occupation_cd1 = 200  # é£²é£Ÿãƒ»ãƒ•ãƒ¼ãƒ‰
            elif user['preferred_occupation'] == 'è»½ä½œæ¥­':
                occupation_cd1 = 400  # è»½ä½œæ¥­ãƒ»å€‰åº«
            elif user['preferred_occupation'] == 'ä»‹è­·':
                occupation_cd1 = 800  # åŒ»ç™‚ãƒ»ä»‹è­·ãƒ»ç¦ç¥‰
            elif user['preferred_occupation'] == 'å¡¾':
                occupation_cd1 = 700  # æ•™è‚²ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼
            else:
                occupation_cd1 = random.choice([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])
            
            action = {
                'action_id': str(uuid.uuid4()),
                'user_id': user['email'],  # emailã‚’ä¸€æ™‚çš„ã«user_idã¨ã—ã¦ä½¿ç”¨
                'job_id': random.randint(1000000, 9999999),
                'action_type': 'applied',
                'source_type': 'email',
                'pref_cd': user['pref_cd'],
                'city_cd': user['city_cd'],
                'occupation_cd1': occupation_cd1,
                'occupation_cd2': occupation_cd1,
                'occupation_cd3': occupation_cd1,
                'jobtype_detail': random.randint(1, 10),
                'employment_type_cd': random.choice([1, 3]),  # 1=ã‚¢ãƒ«ãƒã‚¤ãƒˆ, 3=ãƒ‘ãƒ¼ãƒˆã®ã¿
                'salary_type_cd': salary_type_cd,
                'min_salary': random.randint(1000, 1500),
                'max_salary': random.randint(1500, 2500),
                'station_cd': random.randint(1000000, 9999999),
                'feature_codes': ','.join([str(random.randint(100, 300)) for _ in range(3)]),
                'endcl_cd': f'EX{random.randint(1000000, 9999999):08d}',
                'fee': fee,
                'action_date': datetime.now() - timedelta(days=random.randint(1, 180)),
                'context': {'device': 'mobile', 'source': 'email'}
            }
            actions.append(action)
    
    return actions
```

## ğŸ“‹ å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆalgorithms.md ã¨ã—ã¦ä¿å­˜ï¼‰

```yaml
# ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä»•æ§˜ v1.0

ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°:
  åŸºç¤ã‚¹ã‚³ã‚¢:
    æ™‚çµ¦ã®é‡ã¿: 0.35
    ã‚¢ã‚¯ã‚»ã‚¹ã®é‡ã¿: 0.25
    ç¦åˆ©åšç”Ÿã®é‡ã¿: 0.20
    äººæ°—åº¦ã®é‡ã¿: 0.20
    æ­£è¦åŒ–æ–¹æ³•: z-score â†’ 0-100å¤‰æ›
    
  SEOã‚¹ã‚³ã‚¢:
    ãƒãƒƒãƒãƒ³ã‚°: éƒ¨åˆ†ä¸€è‡´
    æœ€å¤§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: 5
    ã‚¹ã‚³ã‚¢è¨ˆç®—: æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ™ãƒ¼ã‚¹
    
  ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚º:
    ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : ALSï¼ˆimplicitï¼‰
    æ½œåœ¨å› å­: 20
    æ­£å‰‡åŒ–: 0.1
    ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: 10

ã‚«ãƒ†ã‚´ãƒªåˆ†é¡:
  åˆ¤å®šæ–¹æ³•: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚° + å‹•çš„ãƒã‚§ãƒƒã‚¯
  å„ªå…ˆé †ä½: 1-14ï¼ˆå®šç¾©æ¸ˆã¿ï¼‰
  æœ€å¤§ã‚«ãƒ†ã‚´ãƒªæ•°: 3

40ä»¶é¸å®š:
  é‡è¤‡å‡¦ç†: é™¤å¤–ï¼ˆå„æ±‚äººã¯1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ï¼‰
  å„ªå…ˆé †ä½: TOP5 â†’ è¿‘éš£ â†’ åœ°åŸŸ â†’ ãŠå¾— â†’ æ–°ç€
  è£œå……ãƒ­ã‚¸ãƒƒã‚¯: ã‚¹ã‚³ã‚¢é †
  æœ€ä½ä¿è¨¼: 20ä»¶

ãƒ¡ãƒ¼ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³:
  TOP5: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ä¸Šä½
  åœ°åŸŸåˆ¥: æ¨å®šéƒ½é“åºœçœŒã®åŸºç¤ã‚¹ã‚³ã‚¢ä¸Šä½
  è¿‘éš£: æ¨å®šå¸‚åŒºç”ºæ‘ã®äººæ°—é †
  ãŠå¾—: ç‰¹å…¸ã‚¹ã‚³ã‚¢ä¸Šä½
  æ–°ç€: 3æ—¥ä»¥å†…ã®æŠ•ç¨¿æ—¥æ™‚é †

ã‚¨ãƒ©ãƒ¼å‡¦ç†:
  ãƒãƒƒãƒå‡¦ç†: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ–¹å¼
  ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆ: å€‹åˆ¥ãƒªãƒˆãƒ©ã‚¤ï¼ˆæœ€å¤§3å›ï¼‰
  å¤±æ•—é–¾å€¤: 10%
  
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:
  ãƒ¡ãƒ¢ãƒª: dtypeæœ€é©åŒ– + ã‚«ãƒ†ã‚´ãƒªå‹
  ã‚­ãƒ£ãƒƒã‚·ãƒ¥: Supabase Edge Functions
  ä¸¦åˆ—å‡¦ç†: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒãƒå˜ä½
```

## ğŸ“Š å®Ÿè£…æº–å‚™å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### âœ… å³åº§ã«å®Ÿè£…å¯èƒ½ãªé …ç›®
- [x] ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¨ˆç®—å¼: ç¢ºå®šæ¸ˆã¿
- [x] ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šãƒ«ãƒ¼ãƒ«: 14ã‚«ãƒ†ã‚´ãƒªå®šç¾©æ¸ˆã¿
- [x] 40ä»¶é¸å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : é‡è¤‡é™¤å¤–ãƒ»å„ªå…ˆé †ä½ç¢ºå®š
- [x] ãƒ¡ãƒ¼ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³: 5ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»•æ§˜ç¢ºå®š
- [x] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ–¹å¼æ¡ç”¨
- [x] ãƒ‡ãƒ¼ã‚¿ç®¡ç†: ã‚½ãƒ•ãƒˆãƒ‡ãƒªãƒ¼ãƒˆ + å±¥æ­´ä¿æŒ

### ğŸ“ å®Ÿè£…æ™‚ã®æ³¨æ„ç‚¹
1. **ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±åˆ¶é™**: å¿œå‹Ÿå±¥æ­´ã¨ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã¿ã‹ã‚‰æ¨å®š
2. **ãƒ¡ãƒ¢ãƒªç®¡ç†**: 10,000ãƒ¦ãƒ¼ã‚¶ãƒ¼å‡¦ç†æ™‚ã®4GBåˆ¶é™é †å®ˆ
3. **å‡¦ç†æ™‚é–“**: 30åˆ†ä»¥å†…å®Œäº†å¿…é ˆ
4. **ãƒ†ã‚¹ãƒˆå„ªå…ˆ**: TDDï¼ˆRED â†’ GREEN â†’ REFACTORï¼‰
5. **ç¶™ç¶šçš„æ¤œè¨¼**: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰UIã§éšæ™‚ç¢ºèª

ã“ã‚Œã‚‰ã®å›ç­”ã«ã‚ˆã‚Šã€tasks.mdã®å…¨ã‚¿ã‚¹ã‚¯ãŒå®Ÿè£…å¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚