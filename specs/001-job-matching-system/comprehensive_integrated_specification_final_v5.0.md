# ãƒã‚¤ãƒˆæ±‚äººãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  æœ€çµ‚çµ±åˆä»•æ§˜æ›¸ v5.0

**ä½œæˆæ—¥**: 2025-09-16  
**æ–‡æ›¸ã‚¿ã‚¤ãƒ—**: æœ€çµ‚çµ±åˆä»•æ§˜æ›¸ï¼ˆå…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»å…¨ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆç‰ˆï¼‰  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: Production Ready  
**å¯¾è±¡èª­è€…**: é–‹ç™ºãƒãƒ¼ãƒ ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼  
**çµ±åˆå…ƒ**: v1.0, v3.0, v4.0, answers.md, asks.md, data-model.md, plan.md, tasks.md, research.md, quickstart.md

---

## ç›®æ¬¡

1. [ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼](#1-ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼)
2. [ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦](#2-ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦)
3. [æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#3-æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
4. [ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ](#4-ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ)
5. [ã‚³ã‚¢æ©Ÿèƒ½è©³ç´°](#5-ã‚³ã‚¢æ©Ÿèƒ½è©³ç´°)
6. [ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°](#6-ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°)
7. [6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ¡ãƒ¼ãƒ«æ§‹æˆ](#7-6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ¡ãƒ¼ãƒ«æ§‹æˆ)
8. [ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#8-ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
9. [ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ](#9-ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ )
10. [å®Ÿè£…è¨ˆç”»](#10-å®Ÿè£…è¨ˆç”»)
11. [å“è³ªä¿è¨¼](#11-å“è³ªä¿è¨¼)
12. [é‹ç”¨è¦ä»¶](#12-é‹ç”¨è¦ä»¶)
13. [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰](#13-ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰)
14. [ä»˜éŒ²](#14-ä»˜éŒ²)

---

## 1. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### 1.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã€10ä¸‡ä»¶ã®ãƒã‚¤ãƒˆæ±‚äººãƒ‡ãƒ¼ã‚¿ã‹ã‚‰1ä¸‡äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãã‚Œãã‚Œã«æœ€é©ãª40ä»¶ã‚’æ¯æ—¥è‡ªå‹•é¸å®šã—ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«é…ä¿¡æº–å‚™ã‚’è¡Œã†å¤§è¦æ¨¡ãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

### 1.2 ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤
- **å¿œå‹Ÿç‡å‘ä¸Š**: å¾“æ¥æ¯”150%ã®å¿œå‹Ÿç‡å‘ä¸Šã‚’ç›®æ¨™
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦**: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹é–¢é€£æ€§ã®é«˜ã„æ±‚äººæä¾›
- **é‹ç”¨åŠ¹ç‡åŒ–**: å®Œå…¨è‡ªå‹•åŒ–ã«ã‚ˆã‚Šæ‰‹å‹•ä½œæ¥­ã‚’æ’é™¤

### 1.3 ä¸»è¦æˆæœç‰©
- æ¯æ—¥1ä¸‡é€šã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ¡ãƒ¼ãƒ«ç”Ÿæˆï¼ˆ6ã‚»ã‚¯ã‚·ãƒ§ãƒ³Ã—40æ±‚äººï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ SQLãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- 30åˆ†ä»¥å†…ã®å…¨å‡¦ç†å®Œäº†ä¿è¨¼

### 1.4 ä¸»è¦å¤‰æ›´ç‚¹ï¼ˆv4.0â†’v5.0ï¼‰
- **å®Ÿè£…è©³ç´°ã®å®Œå…¨çµ±åˆ**: answers.mdã®å…¨å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’çµ±åˆ
- **GPT-5 nanoçµ±åˆ**: ãƒ¡ãƒ¼ãƒ«ä»¶åç”Ÿæˆã®è©³ç´°å®Ÿè£…
- **ç¶™ç¶šçš„æ¤œè¨¼æˆ¦ç•¥**: tasks.mdã®CHECKãƒã‚¤ãƒ³ãƒˆæ‰‹æ³•çµ±åˆ
- **MCPã‚µãƒ¼ãƒãƒ¼æ´»ç”¨**: Sequential, Serena, Magicç­‰ã®åŠ¹æœçš„æ´»ç”¨æˆ¦ç•¥
- **ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰**: 30åˆ†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ã®çµ±åˆ

---

## 2. ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

### 2.1 ã‚·ã‚¹ãƒ†ãƒ ã®ç›®çš„
æ±‚è·è€…ã«ã‚ˆã‚Šè‰¯ã„ãƒã‚¤ãƒˆæ±‚äººæƒ…å ±ã‚’å±Šã‘ã‚‹ã“ã¨ã§ã€ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã€å¿œå‹Ÿç‡ã¨æ¡ç”¨æˆåŠŸç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

### 2.2 ä¸»è¦æ©Ÿèƒ½ä¸€è¦§

| æ©Ÿèƒ½ã‚«ãƒ†ã‚´ãƒª | æ©Ÿèƒ½å | èª¬æ˜ |
|------------|--------|------|
| ãƒ‡ãƒ¼ã‚¿å‡¦ç† | CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆ | 10ä¸‡ä»¶ã®æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’æ—¥æ¬¡ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ |
| ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° | 3æ®µéšã‚¹ã‚³ã‚¢è¨ˆç®— | åŸºç¤ãƒ»SEOãƒ»ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ã®ç®—å‡º |
| ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ | è‡ªå‹•åˆ†é¡ | 14ãƒ‹ãƒ¼ã‚ºÃ—12è·ç¨®ã‚«ãƒ†ã‚´ãƒªã¸ã®åˆ†é¡ |
| ãƒãƒƒãƒãƒ³ã‚° | æœ€é©åŒ–é¸å®š | å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æœ€é©ãª40ä»¶ã‚’é¸å®š |
| ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆ | 6ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆ | ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸHTMLç”Ÿæˆ |
| ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° | SQLå®Ÿè¡Œç”»é¢ | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ç¢ºèª |

### 2.3 ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨è€…

#### ã‚¨ãƒ³ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆæ±‚è·è€…ï¼‰
- **è¦æ¨¡**: 1ä¸‡äºº
- **ç‰¹å¾´**: 18-65æ­³ã€ã‚¢ãƒ«ãƒã‚¤ãƒˆãƒ»ãƒ‘ãƒ¼ãƒˆå¸Œæœ›è€…
- **åœ°åŸŸ**: å…¨å›½ï¼ˆä¸»ã«éƒ½å¸‚éƒ¨ï¼‰

#### ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…
- **è¦æ¨¡**: 5-10å
- **å½¹å‰²**: ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã€ã‚¨ãƒ©ãƒ¼å¯¾å¿œã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†

### 2.4 é…ä¿¡ãƒ¡ãƒ¼ãƒ«ä¾‹ï¼ˆ6ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆï¼‰

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“§ ã‚²ãƒƒãƒˆãƒã‚¤ãƒˆé€šä¿¡ã€€2025å¹´7æœˆ15æ—¥å·
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€»æœ¬ãƒ¡ãƒ¼ãƒ«ã¯ã€Œã‚²ãƒƒãƒˆãƒã‚¤ãƒˆã€ã«ã”ç™»éŒ²ã®
ç›´å·± æ§˜ï¼ˆæ±äº¬éƒ½å°é‡‘äº•å¸‚åœ¨ä½ï¼‰ã«ãŠå±Šã‘ã—ã¦ã„ã¾ã™ã€‚

ã“ã‚“ã«ã¡ã¯ã€ã‚²ãƒƒãƒˆãƒã‚¤ãƒˆç·¨é›†éƒ¨ã§ã™ï¼
ã€Œå¤ãƒœã€ã¾ã é–“ã«åˆã†ï¼Ÿã€â”€â”€ ãã‚“ãªå£°ã«ãŠå¿œãˆã—ã¦
æ—¥æ‰•ã„ Ã— é«˜æ™‚çµ¦ Ã— é§…è¿‘ ã‚’ä¸­å¿ƒã«å³é¸ã—ãŸ **è¶…ãŠã™ã™ã‚40æ±‚äºº** ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â—† ç·¨é›†éƒ¨ãŠã™ã™ã‚äººæ°—ãƒã‚¤ãƒˆ TOP5ã€NEWã€‘
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[feeÃ—å¿œå‹Ÿæ•°ã§é¸å®šã•ã‚ŒãŸæ³¨ç›®æ±‚äºº]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â—† ã‚ãªãŸã«ãŠã™ã™ã‚æ±‚äºº TOP5
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ä¸Šä½æ±‚äºº]

â—† æ±äº¬éƒ½ãŠã™ã™ã‚æ±‚äºº TOP10
[éƒ½é“åºœçœŒå†…ï¼‹è·ç¨®ãƒãƒƒãƒãƒ³ã‚°]

â—† å°é‡‘äº•å¸‚ã§äººæ°—ã®ãƒã‚¤ãƒˆ 8é¸
[å¸‚åŒºç”ºæ‘å‘¨è¾ºæ±‚äºº]

â—† ç·¨é›†éƒ¨ãŠã™ã™ã‚ "é«˜åå…¥ãƒ»æ—¥æ‰•ã„" TOP7
[é«˜æ™‚çµ¦ OR æ—¥æ‰•ã„å¯èƒ½æ±‚äºº]

â—† æ–°ç€æ±‚äºº TOP5
[7æ—¥ä»¥å†…æŠ•ç¨¿æ±‚äºº]

[ä»¥ä¸‹ã€6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¶šã...]
```

---

## 3. æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 3.1 æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è©³ç´°

```yaml
backend:
  language: Python 3.11
  framework: FastAPI
  batch_processor: APScheduler
  data_processing: 
    - pandas 2.x with PyArrow backend
    - scikit-learn 1.3+
    - numpy 1.24+
    - implicit 0.7+ (å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°)
  ai_integration:
    - openai 1.0+ (GPT-5 nano)
    - fallback: rule-based generation
  database_client: supabase-py v2.x
  testing: pytest, pytest-asyncio
  
frontend:
  language: TypeScript 5.0
  framework: Next.js 14 (App Router)
  ui_library: React 18
  styling: Tailwind CSS 3.3
  state_management: Zustand 4.4
  data_fetching: SWR 2.2
  testing: Jest, React Testing Library
  
database:
  primary: Supabase (PostgreSQL 15)
  connection_pool: PgBouncer
  backup: Daily snapshots
  
infrastructure:
  platform: Ubuntu 22.04 LTS
  runtime: Node.js 20 LTS
  containerization: Docker (optional)
  monitoring: 
    - Grafana (ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ–)
    - Prometheus (ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†)
    - Sentry (ã‚¨ãƒ©ãƒ¼ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°)
```

### 3.2 ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Daily Batch Process (03:00-06:00)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Phase 1       â”‚     Phase 2      â”‚      Phase 3           â”‚
â”‚   Data Import   â”‚     Scoring      â”‚      Matching          â”‚
â”‚   CSVâ†’DB        â”‚  3 algorithms    â”‚    10K users           â”‚
â”‚   (100K jobs)   â”‚  (Basic,SEO,     â”‚    (40 jobs/user)      â”‚
â”‚                 â”‚   Personalized)  â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Email Generation (Phase 4)                   â”‚
â”‚          6 Sections Ã— 40 Jobs = Personalized Content         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ 1. Editorial Picks (5)  - fee Ã— clicks          â”‚      â”‚
â”‚   â”‚ 2. TOP5 (5)            - Personalized score     â”‚      â”‚
â”‚   â”‚ 3. Regional (10)       - Prefecture match       â”‚      â”‚
â”‚   â”‚ 4. Nearby (8)          - City area match        â”‚      â”‚
â”‚   â”‚ 5. High Income (7)     - Salary/Daily pay       â”‚      â”‚
â”‚   â”‚ 6. New (5)             - Within 7 days          â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                SQL Monitoring Interface                       â”‚
â”‚                    (Next.js + Supabase)                      â”‚
â”‚   - Real-time query execution                                â”‚
â”‚   - Data visualization dashboard                             â”‚
â”‚   - Error log viewer                                         â”‚
â”‚   - Manual batch trigger                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–æˆ¦ç•¥

```yaml
parallel_groups:
  group_a:  # ç‹¬ç«‹å®Ÿè¡Œå¯èƒ½
    - database_setup
    - frontend_base_setup
    - test_environment_setup
    max_parallel: 3
    estimated_time: 20åˆ†ï¼ˆé€šå¸¸1æ™‚é–“ï¼‰
    
  group_b:  # group_aå®Œäº†å¾Œ
    - csv_import
    - scoring_implementation
    - ui_implementation
    max_parallel: 3
    estimated_time: 1æ™‚é–“ï¼ˆé€šå¸¸3æ™‚é–“ï¼‰
    
  group_c:  # group_bå®Œäº†å¾Œ
    - matching_process
    - email_generation
    - integration_testing
    max_parallel: 2
    estimated_time: 1.5æ™‚é–“ï¼ˆé€šå¸¸3æ™‚é–“ï¼‰
    
performance_gains:
  traditional_sequential: 8æ™‚é–“
  with_parallelization: 3æ™‚é–“
  improvement: 62.5%
```

### 3.4 MCPã‚µãƒ¼ãƒãƒ¼æ´»ç”¨æˆ¦ç•¥

| ã‚µãƒ¼ãƒãƒ¼ | ç”¨é€” | å¯¾è±¡ã‚¿ã‚¹ã‚¯ | åŠ¹ç‡å‘ä¸Š |
|----------|------|-----------|---------
| **Sequential** | è¤‡é›‘ãªåˆ†æã€ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ | T011-T025 (ãƒ†ã‚¹ãƒˆè¨­è¨ˆ) | 30-50% |
| **Serena** | å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‰æ“ä½œã€ã‚·ãƒ³ãƒœãƒ«ç®¡ç† | T026-T045 (å®Ÿè£…) | 40-60% |
| **Magic** | UI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”Ÿæˆ | T051-T055 (Frontend) | 50-70% |
| **Context7** | ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§ | T003, T036-T040 (ä¾å­˜é–¢ä¿‚) | 20-30% |
| **Playwright** | E2Eãƒ†ã‚¹ãƒˆ | T056-T057 (çµ±åˆãƒ†ã‚¹ãƒˆ) | 60-80% |

---

## 4. ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ

### 4.1 ERå›³æ¦‚è¦

```mermaid
erDiagram
    users ||--o{ user_actions : "performs"
    users ||--o{ user_profiles : "has"
    users ||--o{ user_job_mapping : "matches"
    jobs ||--o{ job_enrichment : "has"
    jobs ||--o{ user_job_mapping : "recommended"
    user_job_mapping ||--o{ daily_job_picks : "selected"
    daily_job_picks ||--o{ daily_email_queue : "generates"
    
    jobs }o--|| prefecture_master : "located_in"
    jobs }o--|| city_master : "located_in"
    jobs }o--|| occupation_master : "categorized_as"
    jobs }o--|| employment_type_master : "has_type"
```

### 4.2 ä¸»è¦ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ï¼ˆ20ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

#### ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ8ï¼‰
1. **jobs** - æ±‚äººãƒã‚¹ã‚¿ãƒ¼ï¼ˆ10ä¸‡ä»¶ã€100+ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
2. **users** - ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸºæœ¬æƒ…å ±ï¼ˆ1ä¸‡ä»¶ï¼‰
3. **user_actions** - è¡Œå‹•å±¥æ­´ï¼ˆå¿œå‹Ÿã€ã‚¯ãƒªãƒƒã‚¯ã€é–‹å°ï¼‰
4. **user_profiles** - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¿œå‹Ÿå‚¾å‘é›†è¨ˆï¼‰
5. **user_job_mapping** - ãƒãƒƒãƒãƒ³ã‚°çµæœï¼ˆæ—¥æ¬¡40ä¸‡ä»¶ï¼‰
6. **daily_job_picks** - é¸å®šæ±‚äººï¼ˆé…ä¿¡ç”¨æ•´å½¢æ¸ˆã¿ï¼‰
7. **daily_email_queue** - ãƒ¡ãƒ¼ãƒ«é…ä¿¡ã‚­ãƒ¥ãƒ¼ï¼ˆ6ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆï¼‰
8. **job_enrichment** - æ±‚äººæ‹¡å¼µæƒ…å ±ï¼ˆã‚¹ã‚³ã‚¢ã€ã‚«ãƒ†ã‚´ãƒªï¼‰

#### ãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ10ï¼‰
9. **occupation_master** - è·ç¨®ãƒã‚¹ã‚¿ãƒ¼ï¼ˆå¤§ä¸­å°åˆ†é¡ï¼‰
10. **prefecture_master** - éƒ½é“åºœçœŒãƒã‚¹ã‚¿ãƒ¼
11. **city_master** - å¸‚åŒºç”ºæ‘ãƒã‚¹ã‚¿ãƒ¼
12. **adjacent_cities** - éš£æ¥å¸‚åŒºç”ºæ‘é–¢ä¿‚
13. **employment_type_master** - é›‡ç”¨å½¢æ…‹ï¼ˆã‚¢ãƒ«ãƒã‚¤ãƒˆã€ãƒ‘ãƒ¼ãƒˆç­‰ï¼‰
14. **salary_type_master** - çµ¦ä¸ã‚¿ã‚¤ãƒ—ï¼ˆæ™‚çµ¦ã€æ—¥çµ¦ã€æœˆçµ¦ï¼‰
15. **feature_master** - ç‰¹å¾´ãƒã‚¹ã‚¿ãƒ¼ï¼ˆ100+ç¨®é¡ï¼‰
16. **needs_category_master** - ãƒ‹ãƒ¼ã‚ºã‚«ãƒ†ã‚´ãƒªå®šç¾©
17. **semrush_keywords** - SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
18. **keyword_scoring** - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

#### è£œåŠ©ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ2ï¼‰
19. **jobs_match_raw** - ãƒãƒƒãƒãƒ³ã‚°ç”¨ç°¡æ˜“ãƒ‡ãƒ¼ã‚¿
20. **jobs_contents_raw** - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿

### 4.3 é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©

```sql
-- jobsãƒ†ãƒ¼ãƒ–ãƒ«ã®é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆ100+ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰æŠœç²‹ï¼‰
CREATE TABLE jobs (
    job_id BIGINT PRIMARY KEY,
    endcl_cd VARCHAR(20),  -- ã‚¨ãƒ³ãƒ‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚³ãƒ¼ãƒ‰ï¼ˆä¼æ¥­è­˜åˆ¥ï¼‰
    application_name TEXT,  -- æ±‚äººã‚¿ã‚¤ãƒˆãƒ«
    company_name VARCHAR(255),
    
    -- çµ¦ä¸æƒ…å ±
    min_salary INTEGER,
    max_salary INTEGER,
    fee INTEGER,  -- å¿œå‹Ÿä¿ƒé€²è²»ç”¨ï¼ˆ0-5000å††ï¼‰
    
    -- å ´æ‰€æƒ…å ±
    pref_cd CHAR(2),
    city_cd VARCHAR(5),
    station_name_eki VARCHAR(100),
    
    -- ã‚«ãƒ†ã‚´ãƒª
    occupation_cd1 INTEGER,  -- å¤§åˆ†é¡
    employment_type_cd INTEGER,  -- é›‡ç”¨å½¢æ…‹
    
    -- ç‰¹å¾´
    feature_codes TEXT,  -- ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
    hours TEXT,  -- å‹¤å‹™æ™‚é–“ï¼ˆHTMLå«ã‚€å¯èƒ½æ€§ï¼‰
    
    -- æ—¥ä»˜
    posting_date TIMESTAMPTZ,
    end_at TIMESTAMPTZ
);

-- user_profilesãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆé›†è¨ˆãƒ‡ãƒ¼ã‚¿ï¼‰
CREATE TABLE user_profiles (
    user_id INTEGER PRIMARY KEY,
    total_applications INTEGER,
    
    -- é »åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆ"ã‚³ãƒ¼ãƒ‰:å›æ•°,ã‚³ãƒ¼ãƒ‰:å›æ•°"å½¢å¼ï¼‰
    applied_pref_cds TEXT,  -- "13:5,14:3"
    applied_city_cds TEXT,
    applied_occupation_cd1s TEXT,
    applied_employment_type_cds TEXT,
    applied_endcl_cds TEXT,
    
    -- çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆJSONå½¢å¼ï¼‰
    applied_salary_stats JSONB,  -- {"avg": 1200, "min": 1000, "max": 2000}
    
    profile_updated_at TIMESTAMPTZ
);
```

### 4.4 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

#### é«˜é »åº¦ã‚¯ã‚¨ãƒªç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
```sql
-- åœ°åŸŸæ¤œç´¢ç”¨
CREATE INDEX idx_jobs_location ON jobs (pref_cd, city_cd, posting_date);

-- ã‚«ãƒ†ã‚´ãƒªæ¤œç´¢ç”¨
CREATE INDEX idx_jobs_category ON jobs (occupation_cd1, employment_type_cd);

-- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ç”¨
CREATE INDEX idx_user_job_mapping_user ON user_job_mapping (user_id, match_date, match_score DESC);

-- ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨
CREATE INDEX idx_job_enrichment_scores ON job_enrichment (basic_score DESC, seo_score DESC);
```

### 4.5 ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥

```sql
-- æ™‚ç³»åˆ—ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
-- user_actions: æœˆå˜ä½ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
CREATE TABLE user_actions_202509 PARTITION OF user_actions
FOR VALUES FROM ('2025-09-01') TO ('2025-10-01');

-- user_job_mapping: é€±å˜ä½ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
CREATE TABLE user_job_mapping_w38 PARTITION OF user_job_mapping
FOR VALUES FROM ('2025-09-15') TO ('2025-09-22');

-- ä¿æŒæœŸé–“
-- user_actions: 6ãƒ¶æœˆ
-- user_job_mapping: 1ãƒ¶æœˆ
-- daily_email_queue: 7æ—¥
```

---

## 5. ã‚³ã‚¢æ©Ÿèƒ½è©³ç´°

### 5.1 æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ©Ÿèƒ½

#### æ©Ÿèƒ½æ¦‚è¦
æ—¥æ¬¡ã§10ä¸‡ä»¶ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã€‚

#### å®Ÿè£…è©³ç´°

```python
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
import logging

def import_jobs_csv(csv_path: str, batch_size: int = 1000) -> Dict[str, int]:
    """
    CSVãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†
    
    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000ä»¶ï¼‰
        
    Returns:
        å‡¦ç†çµæœçµ±è¨ˆ
    """
    logger = logging.getLogger(__name__)
    stats = {'total': 0, 'imported': 0, 'skipped': 0, 'errors': 0}
    
    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªèª­ã¿è¾¼ã¿è¨­å®š
    dtype_config = {
        'job_id': 'uint32',
        'endcl_cd': 'category',
        'pref_cd': 'category',
        'city_cd': 'category',
        'occupation_cd1': 'uint16',
        'employment_type_cd': 'uint8',
        'min_salary': 'uint32',
        'max_salary': 'uint32',
        'fee': 'uint16'
    }
    
    try:
        # 1. CSVã‚’1000ä»¶å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
        for chunk_num, chunk in enumerate(pd.read_csv(
            csv_path, 
            chunksize=batch_size,
            dtype=dtype_config,
            low_memory=False
        )):
            logger.info(f"Processing chunk {chunk_num + 1}, size: {len(chunk)}")
            
            # 2. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            chunk = clean_salary_data(chunk)
            chunk = validate_location_data(chunk)
            chunk = parse_feature_codes(chunk)
            
            # 3. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            valid_chunk = apply_business_rules(chunk)
            
            # 4. Supabaseã¸ä¸€æ‹¬æŒ¿å…¥
            if len(valid_chunk) > 0:
                insert_result = batch_insert_jobs(valid_chunk)
                stats['imported'] += insert_result['success']
                stats['errors'] += insert_result['errors']
            
            stats['total'] += len(chunk)
            stats['skipped'] += len(chunk) - len(valid_chunk)
            
            # 5. ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            del chunk, valid_chunk
            
            logger.info(f"Chunk {chunk_num + 1} completed: {stats}")
            
    except Exception as e:
        logger.error(f"Import failed: {str(e)}")
        raise
        
    return stats

def clean_salary_data(df: pd.DataFrame) -> pd.DataFrame:
    """çµ¦ä¸ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–"""
    # çµ¦ä¸ã®ç•°å¸¸å€¤å‡¦ç†
    df.loc[df['min_salary'] > 5000, 'min_salary'] = np.nan
    df.loc[df['max_salary'] > 5000, 'max_salary'] = np.nan
    df.loc[df['min_salary'] < 800, 'min_salary'] = np.nan
    
    # æœ€ä½è³ƒé‡‘ãƒã‚§ãƒƒã‚¯ï¼ˆéƒ½é“åºœçœŒåˆ¥ï¼‰
    prefecture_min_wages = load_prefecture_min_wages()
    for pref_cd, min_wage in prefecture_min_wages.items():
        mask = (df['pref_cd'] == pref_cd) & (df['min_salary'] < min_wage)
        df.loc[mask, 'min_salary'] = min_wage
    
    return df

def validate_location_data(df: pd.DataFrame) -> pd.DataFrame:
    """å ´æ‰€ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰æ¤œè¨¼
    valid_pref_cds = get_valid_prefecture_codes()
    df = df[df['pref_cd'].isin(valid_pref_cds)]
    
    # å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰æ¤œè¨¼
    valid_city_mapping = get_prefecture_city_mapping()
    for _, row in df.iterrows():
        if row['city_cd'] not in valid_city_mapping.get(row['pref_cd'], []):
            df.drop(row.name, inplace=True)
    
    return df

def parse_feature_codes(df: pd.DataFrame) -> pd.DataFrame:
    """ç‰¹å¾´ã‚³ãƒ¼ãƒ‰ã®å±•é–‹"""
    # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ç‰¹å¾´ã‚³ãƒ¼ãƒ‰ã‚’é…åˆ—ã«å¤‰æ›
    df['feature_list'] = df['feature_codes'].str.split(',').fillna([])
    
    # ç‰¹å¾´ãƒ•ãƒ©ã‚°ã®ä½œæˆ
    feature_mappings = {
        'daily_payment': ['103', 'æ—¥æ‰•ã„'],
        'weekly_payment': ['104', 'é€±æ‰•ã„'],
        'no_experience': ['105', 'æœªçµŒé¨“'],
        'student_welcome': ['106', 'å­¦ç”Ÿæ­“è¿'],
        'remote_work': ['107', 'åœ¨å®…']
    }
    
    for feature_name, codes in feature_mappings.items():
        df[f'has_{feature_name}'] = df['feature_list'].apply(
            lambda x: any(code in str(x) for code in codes)
        )
    
    return df

def apply_business_rules(df: pd.DataFrame) -> pd.DataFrame:
    """ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ã®é©ç”¨"""
    # 1. é›‡ç”¨å½¢æ…‹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚¢ãƒ«ãƒã‚¤ãƒˆã€ãƒ‘ãƒ¼ãƒˆç­‰ã®ã¿ï¼‰
    valid_employment_types = [1, 3, 6, 8]
    df = df[df['employment_type_cd'].isin(valid_employment_types)]
    
    # 2. å¿œå‹Ÿä¿ƒé€²è²»ç”¨ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ500å††ä»¥ä¸Šï¼‰
    df = df[df['fee'] >= 500]
    
    # 3. æŠ•ç¨¿æ—¥ãƒã‚§ãƒƒã‚¯ï¼ˆ30æ—¥ä»¥å†…ï¼‰
    cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=30)
    df['posting_date'] = pd.to_datetime(df['posting_date'])
    df = df[df['posting_date'] >= cutoff_date]
    
    # 4. é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆjob_id + endcl_cdï¼‰
    df = df.drop_duplicates(subset=['job_id', 'endcl_cd'])
    
    return df

def batch_insert_jobs(df: pd.DataFrame) -> Dict[str, int]:
    """Supabaseã¸ã®ãƒãƒƒãƒæŒ¿å…¥"""
    from src.db import get_supabase_client
    
    client = get_supabase_client()
    records = df.to_dict('records')
    result = {'success': 0, 'errors': 0}
    
    try:
        # upsertå‡¦ç†ï¼ˆé‡è¤‡æ™‚ã¯æ›´æ–°ï¼‰
        response = client.table('jobs').upsert(
            records,
            on_conflict='job_id,endcl_cd'
        ).execute()
        
        result['success'] = len(records)
        logging.info(f"Successfully inserted {len(records)} jobs")
        
    except Exception as e:
        result['errors'] = len(records)
        logging.error(f"Batch insert failed: {str(e)}")
        
    return result
```

### 5.2 ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•åˆ†é¡

#### 14ãƒ‹ãƒ¼ã‚ºãƒ™ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒª

| ã‚«ãƒ†ã‚´ãƒªå | åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ | å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å®Ÿè£…ä¾‹ |
|-----------|------------|--------------|--------|
| æ—¥æ‰•ã„ãƒ»é€±æ‰•ã„ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name, salary, features | `has_daily_payment OR contains("æ—¥æ‰•ã„")` |
| çŸ­æœŸãƒ»å˜ç™ºOK | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name, hours | `contains("çŸ­æœŸ", "å˜ç™º", "1æ—¥")` |
| é«˜æ™‚çµ¦ | ã‚¨ãƒªã‚¢å¹³å‡Ã—1.2ä»¥ä¸Š | min_salary, max_salary | `avg_salary >= area_avg * 1.2` |
| ã‚·ãƒ•ãƒˆè‡ªç”± | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | hours, features | `contains("ã‚·ãƒ•ãƒˆè‡ªç”±", "é€±1")` |
| æœªçµŒé¨“æ­“è¿ | feature_code: 103 | feature_codes | `"103" in feature_codes` |
| åœ¨å®…ãƒ»ãƒªãƒ¢ãƒ¼ãƒˆ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name, features | `contains("åœ¨å®…", "ãƒªãƒ¢ãƒ¼ãƒˆ")` |
| å­¦ç”Ÿæ­“è¿ | feature_code: 104 | feature_codes | `"104" in feature_codes` |
| é«˜æ ¡ç”Ÿæ­“è¿ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name, features | `contains("é«˜æ ¡ç”Ÿ")` |
| ä¸»å©¦æ­“è¿ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‹æ™‚é–“å¸¯ | features, hours | `contains("ä¸»å©¦") AND daytime_hours` |
| ã‚·ãƒ‹ã‚¢æ­“è¿ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name, features | `contains("ã‚·ãƒ‹ã‚¢", "60æ­³")` |
| åœŸæ—¥ã®ã¿OK | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | hours | `contains("åœŸæ—¥ã®ã¿")` |
| å‰¯æ¥­ãƒ»Wãƒ¯ãƒ¼ã‚¯OK | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name, features | `contains("å‰¯æ¥­", "Wãƒ¯ãƒ¼ã‚¯")` |
| äº¤é€šè²»æ”¯çµ¦ | ãƒ•ãƒ©ã‚°ç¢ºèª | feature_codes | `"108" in feature_codes` |
| å³æ—¥å‹¤å‹™OK | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ | application_name | `contains("å³æ—¥")` |

#### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
def categorize_jobs(df: pd.DataFrame) -> pd.DataFrame:
    """æ±‚äººã®è‡ªå‹•ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
    
    # ãƒ‹ãƒ¼ã‚ºã‚«ãƒ†ã‚´ãƒªã®åˆ¤å®š
    needs_categories = []
    
    for index, job in df.iterrows():
        job_categories = []
        
        # 1. æ—¥æ‰•ã„ãƒ»é€±æ‰•ã„
        if (job.get('has_daily_payment', False) or 
            any(keyword in str(job.get('application_name', '')).lower() 
                for keyword in ['æ—¥æ‰•ã„', 'é€±æ‰•ã„'])):
            job_categories.append('æ—¥æ‰•ã„ãƒ»é€±æ‰•ã„')
        
        # 2. çŸ­æœŸãƒ»å˜ç™ºOK
        if any(keyword in str(job.get('application_name', '')).lower() 
               for keyword in ['çŸ­æœŸ', 'å˜ç™º', '1æ—¥ã®ã¿']):
            job_categories.append('çŸ­æœŸãƒ»å˜ç™ºOK')
        
        # 3. é«˜æ™‚çµ¦ï¼ˆã‚¨ãƒªã‚¢å¹³å‡Ã—1.2ä»¥ä¸Šï¼‰
        avg_salary = (job.get('min_salary', 0) + job.get('max_salary', 0)) / 2
        area_avg = get_area_average_salary(job.get('pref_cd'))
        if avg_salary >= area_avg * 1.2:
            job_categories.append('é«˜æ™‚çµ¦')
        
        # 4. ã‚·ãƒ•ãƒˆè‡ªç”±
        hours_text = str(job.get('hours', '')).lower()
        if any(keyword in hours_text for keyword in ['ã‚·ãƒ•ãƒˆè‡ªç”±', 'é€±1', 'é€±2']):
            job_categories.append('ã‚·ãƒ•ãƒˆè‡ªç”±')
        
        # 5. æœªçµŒé¨“æ­“è¿
        if '103' in str(job.get('feature_codes', '')):
            job_categories.append('æœªçµŒé¨“æ­“è¿')
        
        # 6. åœ¨å®…ãƒ»ãƒªãƒ¢ãƒ¼ãƒˆ
        if any(keyword in str(job.get('application_name', '')).lower()
               for keyword in ['åœ¨å®…', 'ãƒªãƒ¢ãƒ¼ãƒˆ', 'ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯']):
            job_categories.append('åœ¨å®…ãƒ»ãƒªãƒ¢ãƒ¼ãƒˆ')
        
        # 7. å­¦ç”Ÿæ­“è¿
        if '104' in str(job.get('feature_codes', '')):
            job_categories.append('å­¦ç”Ÿæ­“è¿')
        
        # 8. é«˜æ ¡ç”Ÿæ­“è¿
        if 'é«˜æ ¡ç”Ÿ' in str(job.get('application_name', '')):
            job_categories.append('é«˜æ ¡ç”Ÿæ­“è¿')
        
        # 9. ä¸»å©¦æ­“è¿ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‹æ™‚é–“å¸¯ï¼‰
        if ('ä¸»å©¦' in str(job.get('application_name', '')) and
            is_daytime_shift(job.get('hours', ''))):
            job_categories.append('ä¸»å©¦æ­“è¿')
        
        # 10. ã‚·ãƒ‹ã‚¢æ­“è¿
        if any(keyword in str(job.get('application_name', '')).lower()
               for keyword in ['ã‚·ãƒ‹ã‚¢', '60æ­³', 'å¹´é½¢ä¸å•']):
            job_categories.append('ã‚·ãƒ‹ã‚¢æ­“è¿')
        
        # 11. åœŸæ—¥ã®ã¿OK
        if 'åœŸæ—¥ã®ã¿' in str(job.get('hours', '')):
            job_categories.append('åœŸæ—¥ã®ã¿OK')
        
        # 12. å‰¯æ¥­ãƒ»Wãƒ¯ãƒ¼ã‚¯OK
        if any(keyword in str(job.get('application_name', '')).lower()
               for keyword in ['å‰¯æ¥­', 'wãƒ¯ãƒ¼ã‚¯', 'ãƒ€ãƒ–ãƒ«ãƒ¯ãƒ¼ã‚¯']):
            job_categories.append('å‰¯æ¥­ãƒ»Wãƒ¯ãƒ¼ã‚¯OK')
        
        # 13. äº¤é€šè²»æ”¯çµ¦
        if '108' in str(job.get('feature_codes', '')):
            job_categories.append('äº¤é€šè²»æ”¯çµ¦')
        
        # 14. å³æ—¥å‹¤å‹™OK
        if 'å³æ—¥' in str(job.get('application_name', '')):
            job_categories.append('å³æ—¥å‹¤å‹™OK')
        
        needs_categories.append(','.join(job_categories))
    
    df['needs_categories'] = needs_categories
    
    # è·ç¨®ã‚«ãƒ†ã‚´ãƒªã®åˆ¤å®š
    df['occupation_category'] = df['occupation_cd1'].apply(
        lambda x: get_occupation_category_name(x)
    )
    
    return df

def get_area_average_salary(pref_cd: str) -> float:
    """éƒ½é“åºœçœŒåˆ¥å¹³å‡æ™‚çµ¦ã®å–å¾—"""
    # éƒ½é“åºœçœŒåˆ¥å¹³å‡æ™‚çµ¦ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯DBã‹ã‚‰å–å¾—ï¼‰
    average_salaries = {
        '13': 1200,  # æ±äº¬éƒ½
        '14': 1100,  # ç¥å¥ˆå·çœŒ
        '27': 1050,  # å¤§é˜ªåºœ
        # ãã®ä»–ã®éƒ½é“åºœçœŒ...
    }
    return average_salaries.get(pref_cd, 1000)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000å††

def is_daytime_shift(hours_text: str) -> bool:
    """æ—¥ä¸­ã‚·ãƒ•ãƒˆã‹ã©ã†ã‹ã®åˆ¤å®š"""
    if not hours_text:
        return False
    
    # 9:00-17:00ã®ã‚ˆã†ãªæ™‚é–“å¸¯ã‚’æ¤œå‡º
    import re
    daytime_patterns = [
        r'[89][:ï¼š]\d{2}.*1[4-7][:ï¼š]\d{2}',  # 8:00-16:00ç­‰
        r'10[:ï¼š]\d{2}.*1[56][:ï¼š]\d{2}',    # 10:00-15:00ç­‰
    ]
    
    return any(re.search(pattern, hours_text) for pattern in daytime_patterns)
```

#### 12è·ç¨®ã‚«ãƒ†ã‚´ãƒª

| ã‚³ãƒ¼ãƒ‰ | ã‚«ãƒ†ã‚´ãƒªå | å«ã¾ã‚Œã‚‹è·ç¨® |
|--------|-----------|------------|
| 100 | é£²é£Ÿãƒ»ãƒ•ãƒ¼ãƒ‰ç³» | ãƒ›ãƒ¼ãƒ«ã€ã‚­ãƒƒãƒãƒ³ã€ã‚«ãƒ•ã‚§ |
| 200 | è²©å£²ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ç³» | ã‚¢ãƒ‘ãƒ¬ãƒ«ã€ã‚³ãƒ³ãƒ“ãƒ‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ |
| 300 | é…é€ãƒ»ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç³» | å®…é…ã€å¼•è¶Šã—ã€ãƒ‡ãƒªãƒãƒªãƒ¼ |
| 400 | ã‚ªãƒ•ã‚£ã‚¹ãƒ¯ãƒ¼ã‚¯ç³» | äº‹å‹™ã€ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã€å—ä»˜ |
| 500 | åŒ»ç™‚ãƒ»ä»‹è­·ãƒ»ä¿è‚²ç³» | çœ‹è­·åŠ©æ‰‹ã€ä»‹è­·ã€ä¿è‚²è£œåŠ© |
| 600 | è»½ä½œæ¥­ãƒ»å·¥å ´ç³» | æ¢±åŒ…ã€ä»•åˆ†ã‘ã€æ¤œå“ |
| 700 | ç¾å®¹ãƒ»ç†å®¹ç³» | ç¾å®¹å¸«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ã‚¨ã‚¹ãƒ† |
| 800 | æ•™è‚²ç³» | å¡¾è¬›å¸«ã€å®¶åº­æ•™å¸« |
| 900 | ã‚¨ãƒ³ã‚¿ãƒ¡ç³» | ã‚¤ãƒ™ãƒ³ãƒˆã€éŠåœ’åœ°ã€ã‚«ãƒ©ã‚ªã‚± |
| 1000 | ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ç³» | è¨­å–¶ã€é‹å–¶ã€PR |
| 1100 | ITãƒ»ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ç³» | Webåˆ¶ä½œã€ãƒ‡ã‚¶ã‚¤ãƒ³ã€å‹•ç”»ç·¨é›† |
| 1200+ | ãã®ä»– | ä¸Šè¨˜ä»¥å¤–ã®è·ç¨® |

---

## 6. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°

### 6.1 åŸºç¤ã‚¹ã‚³ã‚¢ï¼ˆBasic Scoreï¼‰

```python
def calculate_basic_score(job, area_stats, company_popularity):
    """
    åŸºç¤ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ç‚¹ï¼‰
    æ§‹æˆè¦ç´ ï¼šæ™‚çµ¦40%ã€å¿œå‹Ÿå˜ä¾¡å ±é…¬30%ã€ä¼æ¥­äººæ°—åº¦30%
    """
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
    VALID_EMPLOYMENT_TYPE_CDS = [1, 3, 6, 8]  # ã‚¢ãƒ«ãƒã‚¤ãƒˆã€ãƒ‘ãƒ¼ãƒˆç­‰
    MIN_FEE_THRESHOLD = 500  # 500å††ä»¥ä¸‹ã¯é™¤å¤–
    
    if job.employment_type_cd not in VALID_EMPLOYMENT_TYPE_CDS:
        return 0
    if job.fee <= MIN_FEE_THRESHOLD:
        return 0
    
    # æ™‚çµ¦ã‚¹ã‚³ã‚¢ï¼ˆã‚¨ãƒªã‚¢å¹³å‡åŸºæº–ï¼‰- 40%
    avg_wage = (job.min_salary + job.max_salary) / 2 if job.max_salary else job.min_salary
    area_average = area_stats.get(job.pref_cd, 1000)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000å††
    
    if avg_wage >= area_average * 1.5:  # 1.5å€ä»¥ä¸Š
        hourly_wage_score = 100
    elif avg_wage >= area_average * 1.2:  # 1.2å€ä»¥ä¸Š
        hourly_wage_score = 80
    elif avg_wage >= area_average:  # å¹³å‡ä»¥ä¸Š
        hourly_wage_score = 60
    else:  # å¹³å‡æœªæº€
        hourly_wage_score = max(0, (avg_wage / area_average) * 60)
    
    # å¿œå‹Ÿå˜ä¾¡å ±é…¬ã‚¹ã‚³ã‚¢ - 30%
    fee_score = normalize_fee(job.fee)
    
    # ä¼æ¥­äººæ°—åº¦ã‚¹ã‚³ã‚¢ï¼ˆ360æ—¥é–“ã®å¿œå‹Ÿç‡ï¼‰- 30%
    popularity_score = calculate_company_popularity_score(
        job.endcl_cd, company_popularity
    )
    
    # åŠ é‡å¹³å‡
    basic_score = (
        hourly_wage_score * 0.40 +
        fee_score * 0.30 +
        popularity_score * 0.30
    )
    
    return min(100, max(0, basic_score))

def normalize_fee(fee):
    """
    å¿œå‹Ÿå˜ä¾¡å ±é…¬ã®æ­£è¦åŒ–
    500å††ä»¥ä¸‹: 0ç‚¹ã€5000å††ä»¥ä¸Š: 100ç‚¹
    """
    if fee <= 500:
        return 0
    elif fee >= 5000:
        return 100
    else:
        return (fee - 500) / (5000 - 500) * 100

def calculate_company_popularity_score(endcl_cd, company_popularity):
    """
    ä¼æ¥­äººæ°—åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
    éå»360æ—¥é–“ã®å¿œå‹Ÿç‡ã«åŸºã¥ã
    """
    if endcl_cd not in company_popularity:
        return 30  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
    
    stats = company_popularity[endcl_cd]
    application_rate = stats.get('application_rate', 0)
    
    if application_rate >= 0.15:  # 15%ä»¥ä¸Š
        return 100
    elif application_rate >= 0.10:  # 10%ä»¥ä¸Š
        return 80
    elif application_rate >= 0.05:  # 5%ä»¥ä¸Š
        return 60
    elif application_rate >= 0.02:  # 2%ä»¥ä¸Š
        return 40
    else:
        return 20
```

### 6.2 SEOã‚¹ã‚³ã‚¢

```python
# ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥é‡ã¿ä»˜ã‘è¨­å®š
FIELD_WEIGHT_CONFIG = {
    'application_name': 1.5,    # æ±‚äººã‚¿ã‚¤ãƒˆãƒ« - é«˜ã„é‡ã¿
    'company_name': 1.5,        # ä¼æ¥­å - é«˜ã„é‡ã¿
    'salary': 0.3,              # çµ¦ä¸ - å°ã•ã„é‡ã¿
    'hours': 0.3,               # å‹¤å‹™æ™‚é–“ - å°ã•ã„é‡ã¿
    'station_name_eki': 0.5,    # æœ€å¯„é§… - ä¸­ç¨‹åº¦
    'feature_codes': 0.8        # ç‰¹å¾´ - ã‚„ã‚„é«˜ã„é‡ã¿
}

def calculate_seo_score(job, keywords_df):
    """
    SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ï¼ˆ0-100ç‚¹ï¼‰
    """
    total_score = 0
    matched_keywords = []
    
    for field_name, weight in FIELD_WEIGHT_CONFIG.items():
        field_value = normalize_text(getattr(job, field_name, ''))
        
        for _, keyword_row in keywords_df.iterrows():
            keyword = keyword_row['processed']  # æ­£è¦åŒ–æ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            
            if keyword in field_value:
                # æ¤œç´¢ãƒœãƒªãƒ¥ãƒ¼ãƒ ã«åŸºã¥ãåŸºæœ¬ã‚¹ã‚³ã‚¢
                volume = keyword_row['volume']
                if volume >= 10000:
                    base_score = 15
                elif volume >= 5000:
                    base_score = 10
                elif volume >= 1000:
                    base_score = 7
                else:
                    base_score = 3
                
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰é‡ã¿ã‚’é©ç”¨
                field_score = base_score * weight
                total_score += field_score
                matched_keywords.append({
                    'keyword': keyword_row['keyword'],
                    'field': field_name,
                    'score': field_score
                })
                
                # æœ€å¤§7ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ã§
                if len(matched_keywords) >= 7:
                    break
        
        if len(matched_keywords) >= 7:
            break
    
    return min(100, total_score), matched_keywords

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
    if not text:
        return ''
    
    import re
    
    # HTMLã‚¿ã‚°ã®é™¤å»
    text = re.sub(r'<[^>]+>', '', str(text))
    
    # æ”¹è¡Œãƒ»ã‚¿ãƒ–ã®é™¤å»
    text = re.sub(r'[\r\n\t]', ' ', text)
    
    # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
    text = re.sub(r'\s+', ' ', text)
    
    # å°æ–‡å­—åŒ–
    text = text.lower().strip()
    
    return text

def load_semrush_keywords():
    """SEMrushã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿"""
    from src.db import get_supabase_client
    
    client = get_supabase_client()
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰å–å¾—
    response = client.table('semrush_keywords').select(
        'keyword', 'processed', 'volume', 'difficulty'
    ).order('volume', desc=True).limit(1000).execute()
    
    import pandas as pd
    return pd.DataFrame(response.data)
```

### 6.3 ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢

```python
def calculate_personalized_score(job, user_profile):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ï¼ˆ0-100ç‚¹ï¼‰
    """
    if not user_profile or user_profile.total_applications == 0:
        return 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
    
    score_components = []
    weights = []
    
    # éƒ½é“åºœçœŒãƒãƒƒãƒãƒ³ã‚°ï¼ˆ20%ï¼‰
    if user_profile.applied_pref_cds:
        pref_matches = calculate_location_match(
            job.pref_cd, 
            parse_frequency_string(user_profile.applied_pref_cds)
        )
        score_components.append(pref_matches)
        weights.append(0.20)
    
    # å¸‚åŒºç”ºæ‘ãƒãƒƒãƒãƒ³ã‚°ï¼ˆ15%ï¼‰
    if user_profile.applied_city_cds:
        city_matches = calculate_location_match(
            job.city_cd,
            parse_frequency_string(user_profile.applied_city_cds)
        )
        score_components.append(city_matches)
        weights.append(0.15)
    
    # è·ç¨®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆ20%ï¼‰
    if user_profile.applied_occupation_cd1s:
        occupation_matches = calculate_category_match(
            job.occupation_cd1,
            parse_frequency_string(user_profile.applied_occupation_cd1s)
        )
        score_components.append(occupation_matches)
        weights.append(0.20)
    
    # çµ¦ä¸ãƒ¬ãƒ³ã‚¸ãƒãƒƒãƒãƒ³ã‚°ï¼ˆ15%ï¼‰
    if user_profile.applied_salary_stats:
        import json
        salary_stats = json.loads(user_profile.applied_salary_stats)
        salary_matches = calculate_salary_range_match(
            job.min_salary, job.max_salary, salary_stats
        )
        score_components.append(salary_matches)
        weights.append(0.15)
    
    # ã‚¨ãƒ³ãƒ‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆ15%ï¼‰
    # 2é€±é–“ä»¥å†…å¿œå‹Ÿä¼æ¥­ã«ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
    if user_profile.applied_endcl_cds:
        endcl_freq = parse_frequency_string(user_profile.applied_endcl_cds)
        if was_applied_within_2weeks(job.endcl_cd, user_profile.user_id):
            score_components.append(10)  # å¤§å¹…æ¸›ç‚¹
        else:
            score_components.append(50)  # é€šå¸¸ã‚¹ã‚³ã‚¢
        weights.append(0.15)
    
    # é›‡ç”¨å½¢æ…‹ãƒãƒƒãƒãƒ³ã‚°ï¼ˆ15%ï¼‰
    if user_profile.applied_employment_type_cds:
        employment_matches = calculate_category_match(
            job.employment_type_cd,
            parse_frequency_string(user_profile.applied_employment_type_cds)
        )
        score_components.append(employment_matches)
        weights.append(0.15)
    
    # é‡ã¿ä»˜ãå¹³å‡
    if score_components:
        total_weight = sum(weights)
        weighted_score = sum(s * w for s, w in zip(score_components, weights))
        final_score = weighted_score / total_weight if total_weight > 0 else 50
    else:
        final_score = 50
    
    return min(100, max(0, final_score))

def parse_frequency_string(freq_str):
    """
    é »åº¦æ–‡å­—åˆ—ã®ãƒ‘ãƒ¼ã‚¹
    "13:5,14:3" -> {13: 5, 14: 3}
    """
    if not freq_str:
        return {}
    
    result = {}
    try:
        for pair in freq_str.split(','):
            if ':' in pair:
                code, count = pair.split(':')
                result[code.strip()] = int(count.strip())
    except (ValueError, AttributeError):
        pass
    
    return result

def calculate_location_match(job_location, user_frequencies):
    """å ´æ‰€ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
    if str(job_location) in user_frequencies:
        frequency = user_frequencies[str(job_location)]
        total_applications = sum(user_frequencies.values())
        match_rate = frequency / total_applications
        
        # ãƒãƒƒãƒç‡ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        if match_rate >= 0.5:  # 50%ä»¥ä¸Š
            return 100
        elif match_rate >= 0.3:  # 30%ä»¥ä¸Š
            return 80
        elif match_rate >= 0.1:  # 10%ä»¥ä¸Š
            return 60
        else:
            return 40
    else:
        return 20  # æœªçµŒé¨“åœ°åŸŸ

def calculate_salary_range_match(job_min, job_max, user_salary_stats):
    """çµ¦ä¸ãƒ¬ãƒ³ã‚¸ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
    user_avg = user_salary_stats.get('avg', 0)
    user_min = user_salary_stats.get('min', 0)
    user_max = user_salary_stats.get('max', 0)
    
    if not user_avg:
        return 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    job_avg = (job_min + job_max) / 2 if job_max else job_min
    
    # çµ¦ä¸å·®ã®å‰²åˆã‚’è¨ˆç®—
    diff_ratio = abs(job_avg - user_avg) / user_avg if user_avg > 0 else 1
    
    if diff_ratio <= 0.1:  # Â±10%ä»¥å†…
        return 100
    elif diff_ratio <= 0.2:  # Â±20%ä»¥å†…
        return 80
    elif diff_ratio <= 0.3:  # Â±30%ä»¥å†…
        return 60
    else:
        return 30

def was_applied_within_2weeks(endcl_cd, user_id):
    """2é€±é–“ä»¥å†…ã®å¿œå‹Ÿå±¥æ­´ãƒã‚§ãƒƒã‚¯"""
    from src.db import get_supabase_client
    from datetime import datetime, timedelta
    
    client = get_supabase_client()
    
    cutoff_date = datetime.now() - timedelta(days=14)
    
    response = client.table('user_actions').select('action_id').match({
        'user_id': user_id,
        'endcl_cd': endcl_cd,
        'action_type': 'applied'
    }).gte('action_timestamp', cutoff_date.isoformat()).execute()
    
    return len(response.data) > 0
```

---

## 7. 6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ¡ãƒ¼ãƒ«æ§‹æˆ

### 7.1 ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã¨é¸å®šãƒ­ã‚¸ãƒƒã‚¯

| # | ã‚»ã‚¯ã‚·ãƒ§ãƒ³å | ä»¶æ•° | é¸å®šåŸºæº– | å„ªå…ˆåº¦ |
|---|------------|------|---------|--------|
| 1 | ç·¨é›†éƒ¨ãŠã™ã™ã‚ (editorial_picks) | 5ä»¶ | fee Ã— å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•° | æœ€é«˜ |
| 2 | ã‚ãªãŸã«ãŠã™ã™ã‚TOP5 (top5) | 5ä»¶ | ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢ä¸Šä½ | é«˜ |
| 3 | åœ°åŸŸåˆ¥æ±‚äºº (regional) | 10ä»¶ | éƒ½é“åºœçœŒå†…ï¼‹è·ç¨®ãƒãƒƒãƒ | ä¸­ |
| 4 | è¿‘éš£æ±‚äºº (nearby) | 8ä»¶ | å¸‚åŒºç”ºæ‘å‘¨è¾º | ä¸­ |
| 5 | é«˜åå…¥ãƒ»æ—¥æ‰•ã„ (high_income) | 7ä»¶ | é«˜æ™‚çµ¦ OR æ—¥æ‰•ã„å¯ | ä½ |
| 6 | æ–°ç€æ±‚äºº (new) | 5ä»¶ | 7æ—¥ä»¥å†…æŠ•ç¨¿ | æœ€ä½ |

### 7.2 é‡è¤‡é™¤å¤–å‡¦ç†

```python
def select_40_jobs_with_sections(user_id, all_candidates):
    """
    40ä»¶é¸å®šï¼ˆ6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€é‡è¤‡ãªã—ï¼‰
    """
    # å„ªå…ˆé †ä½é †ã«å‡¦ç†
    priority_order = [
        'editorial_picks',  # æœ€å„ªå…ˆ
        'top5',
        'regional',
        'nearby',
        'high_income',
        'new'
    ]
    
    section_counts = {
        'editorial_picks': 5,
        'top5': 5,
        'regional': 10,
        'nearby': 8,
        'high_income': 7,
        'new': 5
    }
    
    selected_job_ids = set()
    section_results = {}
    
    for section in priority_order:
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å€™è£œã‚’å–å¾—
        candidates = get_candidates_for_section(section, user_id, all_candidates)
        
        # æ—¢é¸æŠæ±‚äººã‚’é™¤å¤–
        candidates = candidates[~candidates['job_id'].isin(selected_job_ids)]
        
        # å¿…è¦ä»¶æ•°ã‚’é¸å®š
        section_count = section_counts[section]
        
        if len(candidates) >= section_count:
            if section == 'editorial_picks':
                selected = candidates.nlargest(section_count, 'editorial_score')
            elif section == 'top5':
                selected = candidates.nlargest(section_count, 'personalized_score')
            else:
                selected = candidates.nlargest(section_count, 'total_score')
        else:
            # ä¸è¶³åˆ†ã¯ä»–ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰è£œå®Œ
            selected = candidates
            shortage = section_count - len(candidates)
            
            # è£œå®Œå€™è£œï¼ˆæ—¢é¸æŠã‚’é™¤ãï¼‰
            fallback_candidates = all_candidates[
                ~all_candidates['job_id'].isin(selected_job_ids) &
                ~all_candidates['job_id'].isin(candidates['job_id'])
            ]
            
            if len(fallback_candidates) >= shortage:
                fallback = fallback_candidates.nlargest(shortage, 'total_score')
                selected = pd.concat([selected, fallback])
        
        # çµæœã‚’ä¿å­˜
        section_results[section] = selected
        selected_job_ids.update(selected['job_id'])
        
        logging.info(f"Section {section}: selected {len(selected)} jobs")
    
    # ç·ä»¶æ•°ç¢ºèª
    total_selected = sum(len(jobs) for jobs in section_results.values())
    logging.info(f"Total jobs selected: {total_selected}")
    
    return section_results

def get_candidates_for_section(section, user_id, all_candidates):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å€™è£œå–å¾—"""
    
    user_profile = get_user_profile(user_id)
    
    if section == 'editorial_picks':
        # ç·¨é›†éƒ¨ãŠã™ã™ã‚: fee Ã— ã‚¯ãƒªãƒƒã‚¯æ•°é †
        candidates = all_candidates.copy()
        candidates['editorial_score'] = calculate_editorial_popularity_score(
            candidates, user_profile
        )
        return candidates[candidates['editorial_score'] > 0]
    
    elif section == 'top5':
        # TOP5: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢é †
        return all_candidates.nlargest(50, 'personalized_score')
    
    elif section == 'regional':
        # åœ°åŸŸåˆ¥: éƒ½é“åºœçœŒä¸€è‡´
        user_pref = user_profile.get('pref_cd') if user_profile else None
        if user_pref:
            regional_jobs = all_candidates[
                all_candidates['pref_cd'] == user_pref
            ]
            return regional_jobs.nlargest(20, 'total_score')
        else:
            return all_candidates.nlargest(20, 'total_score')
    
    elif section == 'nearby':
        # è¿‘éš£: å¸‚åŒºç”ºæ‘ï¼‹éš£æ¥ã‚¨ãƒªã‚¢
        user_city = user_profile.get('city_cd') if user_profile else None
        if user_city:
            # éš£æ¥å¸‚åŒºç”ºæ‘ã‚’å–å¾—
            adjacent_cities = get_adjacent_cities(user_city)
            nearby_jobs = all_candidates[
                all_candidates['city_cd'].isin([user_city] + adjacent_cities)
            ]
            return nearby_jobs.nlargest(15, 'total_score')
        else:
            return all_candidates.nlargest(15, 'total_score')
    
    elif section == 'high_income':
        # é«˜åå…¥ãƒ»æ—¥æ‰•ã„
        high_income_jobs = all_candidates[
            (all_candidates['is_high_salary'] == True) |
            (all_candidates['has_daily_payment'] == True)
        ]
        return high_income_jobs.nlargest(15, 'total_score')
    
    elif section == 'new':
        # æ–°ç€: 7æ—¥ä»¥å†…
        from datetime import datetime, timedelta
        cutoff_date = datetime.now() - timedelta(days=7)
        new_jobs = all_candidates[
            all_candidates['posting_date'] >= cutoff_date
        ]
        return new_jobs.nlargest(15, 'total_score')
    
    return all_candidates.head(0)  # ç©ºã®DataFrame
```

### 7.3 ç·¨é›†éƒ¨ãŠã™ã™ã‚ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆNEWï¼‰

```python
def calculate_editorial_popularity_score(candidates_df, user_profile):
    """
    ç·¨é›†éƒ¨ãŠã™ã™ã‚ã‚¹ã‚³ã‚¢è¨ˆç®—
    feeï¼ˆå¿œå‹Ÿä¿ƒé€²è²»ç”¨ï¼‰Ã— å®Ÿéš›ã®å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•°
    """
    scores = []
    
    for _, job in candidates_df.iterrows():
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ = fee Ã— å¿œå‹Ÿã‚¯ãƒªãƒƒã‚¯æ•°
        fee = job.get('fee', 0)
        recent_applications = get_recent_application_count(job['job_id'])
        
        base_score = fee * recent_applications
        
        # åœ°åŸŸã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        location_weight = get_location_weight(job, user_profile)
        # åŒä¸€å¸‚åŒºç”ºæ‘: 1.0
        # è¿‘éš£å¸‚åŒºç”ºæ‘: 0.7
        # åŒä¸€éƒ½é“åºœçœŒ: 0.5
        # ãã®ä»–: 0.3
        
        # 2é€±é–“ä»¥å†…å¿œå‹Ÿä¼æ¥­ã¯é™¤å¤–
        if was_applied_within_2weeks(job['endcl_cd'], user_profile.get('user_id')):
            final_score = 0
        else:
            final_score = base_score * location_weight
        
        scores.append(final_score)
    
    return scores

def get_recent_application_count(job_id):
    """æœ€è¿‘30æ—¥é–“ã®å¿œå‹Ÿæ•°å–å¾—"""
    from src.db import get_supabase_client
    from datetime import datetime, timedelta
    
    client = get_supabase_client()
    
    cutoff_date = datetime.now() - timedelta(days=30)
    
    response = client.table('user_actions').select('action_id').match({
        'job_id': job_id,
        'action_type': 'applied'
    }).gte('action_timestamp', cutoff_date.isoformat()).execute()
    
    return len(response.data)

def get_location_weight(job, user_profile):
    """åœ°åŸŸã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘è¨ˆç®—"""
    if not user_profile:
        return 0.3
    
    user_city = user_profile.get('city_cd')
    user_pref = user_profile.get('pref_cd')
    
    job_city = job.get('city_cd')
    job_pref = job.get('pref_cd')
    
    if user_city and job_city == user_city:
        return 1.0  # åŒä¸€å¸‚åŒºç”ºæ‘
    
    if user_city and job_city in get_adjacent_cities(user_city):
        return 0.7  # è¿‘éš£å¸‚åŒºç”ºæ‘
    
    if user_pref and job_pref == user_pref:
        return 0.5  # åŒä¸€éƒ½é“åºœçœŒ
    
    return 0.3  # ãã®ä»–

def get_adjacent_cities(city_cd):
    """éš£æ¥å¸‚åŒºç”ºæ‘ã®å–å¾—"""
    from src.db import get_supabase_client
    
    client = get_supabase_client()
    
    response = client.table('adjacent_cities').select('adjacent_city_cd').match({
        'city_cd': city_cd
    }).execute()
    
    return [row['adjacent_city_cd'] for row in response.data]
```

### 7.4 GPT-5 nanoçµ±åˆã«ã‚ˆã‚‹ä»¶åç”Ÿæˆ

```python
import openai
from typing import Optional, Dict, Any
import logging

class EmailSubjectGenerator:
    """GPT-5 nanoã‚’ä½¿ç”¨ã—ãŸãƒ¡ãƒ¼ãƒ«ä»¶åç”Ÿæˆ"""
    
    def __init__(self):
        self.client = openai.OpenAI()
        self.logger = logging.getLogger(__name__)
    
    def generate_subject(self, user_profile: Dict, selected_jobs: Dict) -> str:
        """
        ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ä»¶åã‚’ç”Ÿæˆ
        
        Args:
            user_profile: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            selected_jobs: 6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é¸å®šæ±‚äºº
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ä»¶å
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            prompt = self._build_prompt(user_profile, selected_jobs)
            
            # GPT-5 nanoå‘¼ã³å‡ºã—
            response = self.client.chat.completions.create(
                model="gpt-5-nano",  # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã«ç½®ãæ›ãˆ
                messages=[
                    {
                        "role": "system", 
                        "content": "ã‚ãªãŸã¯æ—¥æœ¬ã®ã‚¢ãƒ«ãƒã‚¤ãƒˆæ±‚äººãƒ¡ãƒ¼ãƒ«ã®ä»¶åã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚é­…åŠ›çš„ã§é–‹å°ç‡ã®é«˜ã„ä»¶åã‚’50æ–‡å­—ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=50,
                temperature=0.7,
                top_p=0.9
            )
            
            subject = response.choices[0].message.content.strip()
            
            # æ–‡å­—æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(subject) > 50:
                subject = subject[:47] + "..."
            
            self.logger.info(f"Generated subject: {subject}")
            return subject
            
        except Exception as e:
            self.logger.error(f"GPT-5 nano error: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            return self._generate_fallback_subject(user_profile, selected_jobs)
    
    def _build_prompt(self, user_profile: Dict, selected_jobs: Dict) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
        location = f"{user_profile.get('prefecture_name', '')} {user_profile.get('city_name', '')}"
        age_group = user_profile.get('age_group', '')
        
        # æ±‚äººã®ç‰¹å¾´æŠ½å‡º
        editorial_picks = selected_jobs.get('editorial_picks', [])
        top_jobs = selected_jobs.get('top5', [])
        
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        high_salary_count = 0
        daily_payment_count = 0
        
        for job in editorial_picks + top_jobs[:3]:  # ä¸Šä½8ä»¶ã‹ã‚‰ç‰¹å¾´æŠ½å‡º
            if job.get('is_high_salary'):
                high_salary_count += 1
            if job.get('has_daily_payment'):
                daily_payment_count += 1
            
            # æ¥­ç•Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            occupation = job.get('occupation_category_name', '')
            if occupation and occupation not in keywords:
                keywords.append(occupation)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt_parts = [
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±: {location}åœ¨ä½ã€{age_group}",
            f"æ±‚äººã®ç‰¹å¾´: {', '.join(keywords[:2])}ãŒä¸­å¿ƒ",
        ]
        
        if high_salary_count >= 3:
            prompt_parts.append("é«˜æ™‚çµ¦æ¡ˆä»¶å¤šæ•°")
        if daily_payment_count >= 3:
            prompt_parts.append("æ—¥æ‰•ã„å¯èƒ½æ±‚äººå¤šæ•°")
        
        prompt_parts.extend([
            "ä»Šæ—¥ã®æ—¥ä»˜ã‚’å«ã‚ã¦ã€é­…åŠ›çš„ã§é–‹å°ã—ãŸããªã‚‹ãƒ¡ãƒ¼ãƒ«ä»¶åã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
            "å­£ç¯€æ„Ÿã‚„ç·Šæ€¥æ€§ã‚’æ¼”å‡ºã—ã€40æ±‚äººã®ä¾¡å€¤ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚"
        ])
        
        return " ".join(prompt_parts)
    
    def _generate_fallback_subject(self, user_profile: Dict, selected_jobs: Dict) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»¶åç”Ÿæˆï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        from datetime import datetime
        import random
        
        # åŸºæœ¬ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        templates = [
            "ğŸ¯ {location}ã§è¦‹ã¤ã‘ãŸï¼é«˜æ™‚çµ¦ãƒã‚¤ãƒˆ{count}ä»¶",
            "ğŸ’° {location}é™å®šï¼ä»Šã™ãç¨¼ã’ã‚‹ãƒã‚¤ãƒˆ{count}é¸",
            "âš¡ {today}æ›´æ–°ï¼{location}ã®å³é¸ãƒã‚¤ãƒˆ{count}ä»¶",
            "ğŸ”¥ {location}ã§å¤§äººæ°—ï¼ãŠã™ã™ã‚ãƒã‚¤ãƒˆ{count}ä»¶",
            "âœ¨ {location}åœ¨ä½å¿…è¦‹ï¼ä»Šé€±ã®ãƒã‚¤ãƒˆ{count}ä»¶"
        ]
        
        # å¤‰æ•°è¨­å®š
        location = user_profile.get('prefecture_name', 'å…¨å›½')
        count = "40"
        today = datetime.now().strftime("%m/%d")
        
        # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
        template = random.choice(templates)
        subject = template.format(location=location, count=count, today=today)
        
        return subject
    
    def generate_batch_subjects(self, user_profiles: list, selected_jobs_list: list) -> list:
        """ãƒãƒƒãƒã§ã®ä»¶åç”Ÿæˆ"""
        subjects = []
        
        for user_profile, selected_jobs in zip(user_profiles, selected_jobs_list):
            try:
                subject = self.generate_subject(user_profile, selected_jobs)
                subjects.append(subject)
            except Exception as e:
                self.logger.error(f"Failed to generate subject for user {user_profile.get('user_id')}: {str(e)}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                subjects.append(self._generate_fallback_subject(user_profile, selected_jobs))
        
        return subjects

# ä½¿ç”¨ä¾‹
def generate_email_content(user_id, selected_jobs):
    """ãƒ¡ãƒ¼ãƒ«å†…å®¹ã®ç”Ÿæˆ"""
    
    user_profile = get_user_profile(user_id)
    
    # ä»¶åç”Ÿæˆ
    generator = EmailSubjectGenerator()
    subject = generator.generate_subject(user_profile, selected_jobs)
    
    # HTMLå†…å®¹ç”Ÿæˆ
    html_content = build_html_email_content(user_profile, selected_jobs)
    
    return {
        'user_id': user_id,
        'subject': subject,
        'html_content': html_content,
        'sections': {
            'editorial_picks': len(selected_jobs.get('editorial_picks', [])),
            'top5': len(selected_jobs.get('top5', [])),
            'regional': len(selected_jobs.get('regional', [])),
            'nearby': len(selected_jobs.get('nearby', [])),
            'high_income': len(selected_jobs.get('high_income', [])),
            'new': len(selected_jobs.get('new', []))
        }
    }
```

---

## 8. ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 8.1 æ—¥æ¬¡å‡¦ç†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

```yaml
daily_batch_schedule:
  03:00-03:30: "Phase 1: ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
    - CSVãƒ‡ãƒ¼ã‚¿å–å¾—
    - ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    - Supabaseã¸ã®ä¸€æ‹¬æŠ•å…¥
    
  03:30-04:30: "Phase 2: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"
    - åŸºç¤ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ™‚çµ¦ã€feeã€äººæ°—åº¦ï¼‰
    - SEOã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼‰
    - ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
    - job_enrichmentãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
    
  04:30-05:30: "Phase 3: ãƒãƒƒãƒãƒ³ã‚°"
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
    - 10ä¸‡Ã—1ä¸‡ã®ãƒãƒƒãƒãƒ³ã‚°è¨ˆç®—ï¼ˆä¸¦åˆ—5ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰
    - user_job_mappingä½œæˆï¼ˆ40ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰
    - å„ãƒ¦ãƒ¼ã‚¶ãƒ¼40ä»¶é¸å®šï¼ˆ6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
    
  05:30-06:00: "Phase 4: ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆ"
    - 6ã‚»ã‚¯ã‚·ãƒ§ãƒ³HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
    - ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºä»¶åç”Ÿæˆï¼ˆGPT-5 nanoï¼‰
    - daily_email_queueã¸ã®æ ¼ç´
    - é…ä¿¡æº–å‚™å®Œäº†é€šçŸ¥
```

### 8.2 ä¸¦åˆ—å‡¦ç†å®Ÿè£…

```python
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp
import logging
from typing import List, Dict, Any
import time

class ParallelBatchProcessor:
    """ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(5, mp.cpu_count())
        self.logger = logging.getLogger(__name__)
    
    def run_daily_batch(self) -> Dict[str, Any]:
        """æ—¥æ¬¡ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ"""
        
        start_time = time.time()
        self.logger.info(f"Starting daily batch with {self.max_workers} workers")
        
        try:
            # Phase 1: ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹ï¼‰
            import_stats = self._run_data_import()
            
            # Phase 2: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
            scoring_stats = self._run_parallel_scoring()
            
            # Phase 3: ãƒãƒƒãƒãƒ³ã‚°ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
            matching_stats = self._run_parallel_matching()
            
            # Phase 4: ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
            email_stats = self._run_parallel_email_generation()
            
            elapsed_time = time.time() - start_time
            
            result = {
                'success': True,
                'elapsed_time': elapsed_time,
                'phases': {
                    'import': import_stats,
                    'scoring': scoring_stats,
                    'matching': matching_stats,
                    'email_generation': email_stats
                }
            }
            
            self.logger.info(f"Batch completed successfully in {elapsed_time:.2f} seconds")
            return result
            
        except Exception as e:
            self.logger.error(f"Batch processing failed: {str(e)}")
            raise
    
    def _run_data_import(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ•ã‚§ãƒ¼ã‚º"""
        
        self.logger.info("Phase 1: Starting data import")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ãƒ‘ã‚¹
        csv_path = '/Users/naoki/000_PROJECT/job-score-for-mail-system-20250914/data/sample_job_data.csv'
        
        from .import_jobs import import_jobs_csv
        stats = import_jobs_csv(csv_path, batch_size=1000)
        
        self.logger.info(f"Data import completed: {stats}")
        return stats
    
    def _run_parallel_scoring(self) -> Dict[str, Any]:
        """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰"""
        
        self.logger.info("Phase 2: Starting parallel scoring")
        
        # æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        jobs_df = self._get_jobs_for_processing()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã§åˆ†å‰²
        job_chunks = self._split_dataframe(jobs_df, self.max_workers)
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._process_scoring_batch, chunk, i)
                for i, chunk in enumerate(job_chunks)
            ]
            
            # çµæœåé›†
            results = []
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=600)  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                    self.logger.info(f"Scoring chunk {i} completed: {result}")
                except Exception as e:
                    self.logger.error(f"Scoring chunk {i} failed: {str(e)}")
                    raise
        
        # çµ±è¨ˆæƒ…å ±é›†è¨ˆ
        total_scored = sum(r['scored'] for r in results)
        total_errors = sum(r['errors'] for r in results)
        
        return {
            'total_scored': total_scored,
            'total_errors': total_errors,
            'chunks': len(results)
        }
    
    def _run_parallel_matching(self) -> Dict[str, Any]:
        """ãƒãƒƒãƒãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰"""
        
        self.logger.info("Phase 3: Starting parallel matching")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’å–å¾—
        users = self._get_active_users()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åˆ†å‰²
        user_chunks = self._split_list(users, self.max_workers)
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._process_matching_batch, chunk, i)
                for i, chunk in enumerate(user_chunks)
            ]
            
            # çµæœåé›†
            results = []
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=900)  # 15åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                    self.logger.info(f"Matching chunk {i} completed: {result}")
                except Exception as e:
                    self.logger.error(f"Matching chunk {i} failed: {str(e)}")
                    raise
        
        # çµ±è¨ˆæƒ…å ±é›†è¨ˆ
        total_users = sum(r['users_processed'] for r in results)
        total_matches = sum(r['matches_created'] for r in results)
        
        return {
            'total_users': total_users,
            'total_matches': total_matches,
            'chunks': len(results)
        }
    
    def _run_parallel_email_generation(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰"""
        
        self.logger.info("Phase 4: Starting parallel email generation")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸å®šæ¸ˆã¿æ±‚äººã‚’å–å¾—
        user_job_mappings = self._get_daily_job_picks()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«åˆ†å‰²
        user_chunks = self._split_list(user_job_mappings, self.max_workers)
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._process_email_generation_batch, chunk, i)
                for i, chunk in enumerate(user_chunks)
            ]
            
            # çµæœåé›†
            results = []
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                    self.logger.info(f"Email generation chunk {i} completed: {result}")
                except Exception as e:
                    self.logger.error(f"Email generation chunk {i} failed: {str(e)}")
                    raise
        
        # çµ±è¨ˆæƒ…å ±é›†è¨ˆ
        total_emails = sum(r['emails_generated'] for r in results)
        
        return {
            'total_emails': total_emails,
            'chunks': len(results)
        }
    
    def _process_scoring_batch(self, jobs_chunk: List[Dict], chunk_id: int) -> Dict[str, int]:
        """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†"""
        
        from .scoring import ScoringEngine
        
        engine = ScoringEngine()
        scored = 0
        errors = 0
        
        for job in jobs_chunk:
            try:
                scores = engine.calculate_all_scores(job)
                engine.save_job_enrichment(job['job_id'], scores)
                scored += 1
            except Exception as e:
                logging.error(f"Scoring failed for job {job.get('job_id')}: {str(e)}")
                errors += 1
        
        return {'scored': scored, 'errors': errors, 'chunk_id': chunk_id}
    
    def _process_matching_batch(self, users_chunk: List[Dict], chunk_id: int) -> Dict[str, int]:
        """ãƒãƒƒãƒãƒ³ã‚°ãƒãƒƒãƒå‡¦ç†"""
        
        from .matching import MatchingEngine
        
        engine = MatchingEngine()
        users_processed = 0
        matches_created = 0
        
        for user in users_chunk:
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®40ä»¶é¸å®š
                selected_jobs = engine.select_40_jobs_for_user(user['user_id'])
                
                # çµæœã‚’ä¿å­˜
                engine.save_daily_job_picks(user['user_id'], selected_jobs)
                
                users_processed += 1
                matches_created += sum(len(jobs) for jobs in selected_jobs.values())
                
            except Exception as e:
                logging.error(f"Matching failed for user {user.get('user_id')}: {str(e)}")
        
        return {
            'users_processed': users_processed, 
            'matches_created': matches_created,
            'chunk_id': chunk_id
        }
    
    def _process_email_generation_batch(self, user_mappings_chunk: List[Dict], chunk_id: int) -> Dict[str, int]:
        """ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆãƒãƒƒãƒå‡¦ç†"""
        
        from .email_generation import EmailGenerator
        
        generator = EmailGenerator()
        emails_generated = 0
        
        for mapping in user_mappings_chunk:
            try:
                # ãƒ¡ãƒ¼ãƒ«å†…å®¹ç”Ÿæˆ
                email_content = generator.generate_email_content(
                    mapping['user_id'], 
                    mapping['selected_jobs']
                )
                
                # é…ä¿¡ã‚­ãƒ¥ãƒ¼ã«ä¿å­˜
                generator.save_to_email_queue(email_content)
                
                emails_generated += 1
                
            except Exception as e:
                logging.error(f"Email generation failed for user {mapping.get('user_id')}: {str(e)}")
        
        return {'emails_generated': emails_generated, 'chunk_id': chunk_id}
    
    def _split_dataframe(self, df, num_chunks):
        """DataFrameã®åˆ†å‰²"""
        import numpy as np
        return np.array_split(df, num_chunks)
    
    def _split_list(self, lst, num_chunks):
        """ãƒªã‚¹ãƒˆã®åˆ†å‰²"""
        chunk_size = len(lst) // num_chunks
        return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]
    
    def _get_jobs_for_processing(self):
        """å‡¦ç†å¯¾è±¡æ±‚äººã®å–å¾—"""
        from src.db import get_supabase_client
        
        client = get_supabase_client()
        
        response = client.table('jobs').select(
            'job_id, endcl_cd, application_name, company_name, '
            'min_salary, max_salary, fee, pref_cd, city_cd, '
            'occupation_cd1, employment_type_cd, feature_codes, '
            'posting_date'
        ).eq('is_active', True).execute()
        
        return response.data
    
    def _get_active_users(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å–å¾—"""
        from src.db import get_supabase_client
        
        client = get_supabase_client()
        
        response = client.table('users').select(
            'user_id, pref_cd, city_cd'
        ).eq('is_active', True).eq('email_subscription', True).execute()
        
        return response.data
    
    def _get_daily_job_picks(self):
        """æ—¥æ¬¡é¸å®šæ±‚äººã®å–å¾—"""
        from src.db import get_supabase_client
        from datetime import date
        
        client = get_supabase_client()
        
        today = date.today()
        
        response = client.table('daily_job_picks').select(
            'user_id, job_id, section, section_rank'
        ).eq('pick_date', today).execute()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        user_mappings = {}
        for pick in response.data:
            user_id = pick['user_id']
            if user_id not in user_mappings:
                user_mappings[user_id] = {
                    'user_id': user_id,
                    'selected_jobs': {}
                }
            
            section = pick['section']
            if section not in user_mappings[user_id]['selected_jobs']:
                user_mappings[user_id]['selected_jobs'][section] = []
            
            user_mappings[user_id]['selected_jobs'][section].append(pick)
        
        return list(user_mappings.values())

# ä½¿ç”¨ä¾‹
def run_daily_batch():
    """æ—¥æ¬¡ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ"""
    
    processor = ParallelBatchProcessor(max_workers=5)
    
    try:
        result = processor.run_daily_batch()
        
        print(f"âœ… Batch completed successfully")
        print(f"â±ï¸  Total time: {result['elapsed_time']:.2f} seconds")
        print(f"ğŸ“Š Import: {result['phases']['import']}")
        print(f"ğŸ¯ Scoring: {result['phases']['scoring']}")
        print(f"ğŸ”„ Matching: {result['phases']['matching']}")
        print(f"ğŸ“§ Email generation: {result['phases']['email_generation']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Batch failed: {str(e)}")
        raise

if __name__ == '__main__':
    run_daily_batch()
```

### 8.3 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
ERROR_HANDLING_CONFIG = {
    'csv_parse_error': {
        'action': 'skip_row',
        'log': True,
        'alert': False,
        'retry': False
    },
    'db_connection_error': {
        'action': 'retry',
        'max_retries': 3,
        'backoff': 'exponential',  # 1ç§’â†’2ç§’â†’4ç§’
        'log': True,
        'alert': True
    },
    'matching_failure': {
        'action': 'fallback',
        'fallback_strategy': 'use_previous_day',
        'log': True,
        'alert': True
    },
    'memory_overflow': {
        'action': 'graceful_degradation',
        'strategy': 'process_in_chunks',
        'chunk_size': 1000,
        'log': True,
        'alert': True
    },
    'gpt_api_error': {
        'action': 'fallback',
        'fallback_strategy': 'rule_based_generation',
        'log': True,
        'alert': False
    }
}

class BatchErrorHandler:
    """ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.alert_sender = AlertSender()
    
    def handle_error(self, error_type: str, context: Dict[str, Any], exception: Exception) -> bool:
        """
        ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        
        Returns:
            bool: å‡¦ç†ã‚’ç¶šè¡Œã™ã‚‹ã‹ã©ã†ã‹
        """
        config = ERROR_HANDLING_CONFIG.get(error_type, {})
        action = config.get('action', 'fail')
        
        # ãƒ­ã‚°è¨˜éŒ²
        if config.get('log', True):
            self.logger.error(
                f"Error occurred: {error_type}",
                extra={
                    'context': context,
                    'exception': str(exception),
                    'action': action
                }
            )
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        if config.get('alert', False):
            self.alert_sender.send_error_alert(error_type, context, exception)
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        if action == 'skip_row':
            return True  # å‡¦ç†ç¶šè¡Œ
        
        elif action == 'retry':
            return self._handle_retry(config, context, exception)
        
        elif action == 'fallback':
            return self._handle_fallback(config, context, exception)
        
        elif action == 'graceful_degradation':
            return self._handle_degradation(config, context, exception)
        
        else:  # fail
            return False
    
    def _handle_retry(self, config: Dict, context: Dict, exception: Exception) -> bool:
        """ãƒªãƒˆãƒ©ã‚¤å‡¦ç†"""
        max_retries = config.get('max_retries', 3)
        current_attempt = context.get('attempt', 0)
        
        if current_attempt >= max_retries:
            self.logger.error(f"Max retries ({max_retries}) exceeded")
            return False
        
        # ãƒãƒƒã‚¯ã‚ªãƒ•
        backoff_type = config.get('backoff', 'linear')
        if backoff_type == 'exponential':
            delay = 2 ** current_attempt
        else:
            delay = current_attempt + 1
        
        self.logger.info(f"Retrying in {delay} seconds (attempt {current_attempt + 1})")
        time.sleep(delay)
        
        return True
    
    def _handle_fallback(self, config: Dict, context: Dict, exception: Exception) -> bool:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
        strategy = config.get('fallback_strategy')
        
        if strategy == 'use_previous_day':
            return self._use_previous_day_data(context)
        elif strategy == 'rule_based_generation':
            return self._use_rule_based_generation(context)
        else:
            return False
    
    def _handle_degradation(self, config: Dict, context: Dict, exception: Exception) -> bool:
        """æ®µéšçš„å‡¦ç†ç¸®å°"""
        strategy = config.get('strategy')
        
        if strategy == 'process_in_chunks':
            new_chunk_size = config.get('chunk_size', 1000)
            context['chunk_size'] = min(context.get('chunk_size', 5000), new_chunk_size)
            self.logger.info(f"Reducing chunk size to {new_chunk_size}")
            return True
        
        return False
    
    def _use_previous_day_data(self, context: Dict) -> bool:
        """å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã®ä½¿ç”¨"""
        # å®Ÿè£…çœç•¥
        return True
    
    def _use_rule_based_generation(self, context: Dict) -> bool:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        # å®Ÿè£…çœç•¥
        return True

class AlertSender:
    """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
    
    def send_error_alert(self, error_type: str, context: Dict, exception: Exception):
        """ã‚¨ãƒ©ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒˆã®é€ä¿¡"""
        # Slackã€ãƒ¡ãƒ¼ãƒ«ç­‰ã¸ã®é€šçŸ¥å®Ÿè£…
        pass
```

---

## 9. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

### 9.1 SQLãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

#### åŸºæœ¬æ©Ÿèƒ½
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆSELECTå°‚ç”¨ï¼‰
- ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãƒ“ãƒ¥ãƒ¼ã‚¢
- ãƒãƒƒãƒå‡¦ç†çŠ¶æ³ãƒ¢ãƒ‹ã‚¿ãƒ¼

#### å®Ÿè£…ä¾‹ï¼ˆNext.jsï¼‰

```typescript
// app/monitoring/query/page.tsx
'use client'

import { useState, useEffect } from 'react'
import { createClient } from '@supabase/supabase-js'
import { Card, CardHeader, CardTitle, CardContent } from '@/components/ui/card'
import { Button } from '@/components/ui/button'
import { Textarea } from '@/components/ui/textarea'
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '@/components/ui/table'
import { AlertCircle, Play, Download } from 'lucide-react'

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
)

interface QueryResult {
  data: any[] | null
  error: string | null
  execution_time: number
  rows_affected: number
}

export default function QueryPage() {
  const [query, setQuery] = useState('SELECT COUNT(*) FROM jobs WHERE is_active = true;')
  const [results, setResults] = useState<QueryResult | null>(null)
  const [isLoading, setIsLoading] = useState(false)
  const [history, setHistory] = useState<string[]>([])

  // ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
  const executeQuery = async () => {
    if (!query.trim()) return

    // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
    if (!query.toLowerCase().trim().startsWith('select')) {
      setResults({
        data: null,
        error: 'SELECTæ–‡ã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™',
        execution_time: 0,
        rows_affected: 0
      })
      return
    }

    setIsLoading(true)
    const startTime = performance.now()

    try {
      const { data, error } = await supabase
        .rpc('execute_readonly_query', { query_text: query })

      const executionTime = performance.now() - startTime

      setResults({
        data: data || [],
        error: error?.message || null,
        execution_time: executionTime,
        rows_affected: data?.length || 0
      })

      // å±¥æ­´ã«è¿½åŠ 
      if (!error) {
        setHistory(prev => [query, ...prev.slice(0, 9)]) // æœ€æ–°10ä»¶
      }

    } catch (err) {
      setResults({
        data: null,
        error: `å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: ${err instanceof Error ? err.message : 'Unknown error'}`,
        execution_time: performance.now() - startTime,
        rows_affected: 0
      })
    } finally {
      setIsLoading(false)
    }
  }

  // ã‚ˆãä½¿ã†ã‚¯ã‚¨ãƒª
  const commonQueries = [
    {
      name: 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ±‚äººæ•°',
      query: 'SELECT COUNT(*) as active_jobs FROM jobs WHERE is_active = true;'
    },
    {
      name: 'ä»Šæ—¥ã®ãƒãƒƒãƒãƒ³ã‚°çŠ¶æ³',
      query: `SELECT 
        COUNT(DISTINCT user_id) as users_matched,
        COUNT(*) as total_matches
      FROM daily_job_picks 
      WHERE pick_date = CURRENT_DATE;`
    },
    {
      name: 'ã‚¹ã‚³ã‚¢åˆ†å¸ƒ',
      query: `SELECT 
        CASE 
          WHEN basic_score >= 80 THEN '80-100'
          WHEN basic_score >= 60 THEN '60-80'
          WHEN basic_score >= 40 THEN '40-60'
          ELSE '0-40'
        END as score_range,
        COUNT(*) as count
      FROM job_enrichment
      GROUP BY score_range
      ORDER BY score_range;`
    },
    {
      name: 'ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ä»¶æ•°',
      query: `SELECT 
        section,
        COUNT(*) as job_count
      FROM daily_job_picks 
      WHERE pick_date = CURRENT_DATE
      GROUP BY section;`
    }
  ]

  return (
    <div className="container mx-auto p-6 space-y-6">
      <Card>
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            <Play className="h-5 w-5" />
            SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œ
          </CardTitle>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="space-y-2">
            <label className="text-sm font-medium">SQLã‚¯ã‚¨ãƒª</label>
            <Textarea
              value={query}
              onChange={(e) => setQuery(e.target.value)}
              placeholder="SELECTæ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."
              className="font-mono text-sm h-32"
            />
          </div>
          
          <div className="flex gap-2">
            <Button onClick={executeQuery} disabled={isLoading}>
              {isLoading ? 'å®Ÿè¡Œä¸­...' : 'å®Ÿè¡Œ'}
            </Button>
            <Button 
              variant="outline" 
              onClick={() => setQuery('')}
            >
              ã‚¯ãƒªã‚¢
            </Button>
          </div>
        </CardContent>
      </Card>

      {/* ã‚ˆãä½¿ã†ã‚¯ã‚¨ãƒª */}
      <Card>
        <CardHeader>
          <CardTitle>ã‚ˆãä½¿ã†ã‚¯ã‚¨ãƒª</CardTitle>
        </CardHeader>
        <CardContent>
          <div className="grid grid-cols-2 md:grid-cols-4 gap-2">
            {commonQueries.map((item, index) => (
              <Button
                key={index}
                variant="outline"
                size="sm"
                onClick={() => setQuery(item.query)}
                className="text-left h-auto p-2"
              >
                {item.name}
              </Button>
            ))}
          </div>
        </CardContent>
      </Card>

      {/* å®Ÿè¡Œçµæœ */}
      {results && (
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center justify-between">
              å®Ÿè¡Œçµæœ
              <div className="text-sm text-muted-foreground">
                å®Ÿè¡Œæ™‚é–“: {results.execution_time.toFixed(2)}ms | 
                è¡Œæ•°: {results.rows_affected}
              </div>
            </CardTitle>
          </CardHeader>
          <CardContent>
            {results.error ? (
              <div className="flex items-center gap-2 text-red-600">
                <AlertCircle className="h-4 w-4" />
                {results.error}
              </div>
            ) : results.data && results.data.length > 0 ? (
              <div className="space-y-4">
                <div className="overflow-auto max-h-96">
                  <Table>
                    <TableHeader>
                      <TableRow>
                        {Object.keys(results.data[0]).map((column) => (
                          <TableHead key={column}>{column}</TableHead>
                        ))}
                      </TableRow>
                    </TableHeader>
                    <TableBody>
                      {results.data.map((row, index) => (
                        <TableRow key={index}>
                          {Object.values(row).map((value, cellIndex) => (
                            <TableCell key={cellIndex} className="font-mono text-sm">
                              {String(value)}
                            </TableCell>
                          ))}
                        </TableRow>
                      ))}
                    </TableBody>
                  </Table>
                </div>
                
                {results.data.length > 10 && (
                  <div className="text-sm text-muted-foreground">
                    æœ€åˆã®{Math.min(results.data.length, 100)}è¡Œã‚’è¡¨ç¤º
                  </div>
                )}
              </div>
            ) : (
              <div className="text-muted-foreground">ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“</div>
            )}
          </CardContent>
        </Card>
      )}

      {/* ã‚¯ã‚¨ãƒªå±¥æ­´ */}
      {history.length > 0 && (
        <Card>
          <CardHeader>
            <CardTitle>å®Ÿè¡Œå±¥æ­´</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="space-y-2">
              {history.map((historyQuery, index) => (
                <Button
                  key={index}
                  variant="ghost"
                  className="w-full text-left h-auto p-2 font-mono text-sm"
                  onClick={() => setQuery(historyQuery)}
                >
                  {historyQuery}
                </Button>
              ))}
            </div>
          </CardContent>
        </Card>
      )}
    </div>
  )
}
```

### 9.2 ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–é …ç›®
- å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå®Ÿè¡Œä¸­/å®Œäº†/ã‚¨ãƒ©ãƒ¼ï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆ4GBé–¾å€¤ï¼‰
- ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆ1%é–¾å€¤ï¼‰
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæ•°ï¼ˆ100æ¥ç¶šä¸Šé™ï¼‰

#### æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- å‡¦ç†æ±‚äººæ•°: 10ä¸‡ä»¶
- ãƒãƒƒãƒãƒ³ã‚°ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: 1ä¸‡äºº
- ç”Ÿæˆãƒ¡ãƒ¼ãƒ«æ•°: 1ä¸‡é€š
- å¹³å‡ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³å……è¶³ç‡

#### å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
- ã‚¯ãƒªãƒƒã‚¯ç‡ï¼ˆCTRï¼‰
- å¿œå‹Ÿç‡ï¼ˆCVRï¼‰
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã‚¹ã‚³ã‚¢
- é…ä¿¡åœæ­¢ç‡

### 9.3 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```yaml
alert_conditions:
  critical:
    - condition: "processing_time > 30 minutes"
      action: ["email", "slack", "pager"]
    - condition: "memory_usage > 4GB"
      action: ["email", "slack"]
    - condition: "error_rate > 1%"
      action: ["email", "slack"]
      
  warning:
    - condition: "processing_time > 20 minutes"
      action: ["email"]
    - condition: "memory_usage > 3GB"
      action: ["log"]
    - condition: "matched_users < 9000"
      action: ["email"]
```

---

## 10. å®Ÿè£…è¨ˆç”»

### 10.1 ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| ãƒ•ã‚§ãƒ¼ã‚º | æœŸé–“ | ä¸»è¦ã‚¿ã‚¹ã‚¯ | æˆæœç‰© |
|---------|------|-----------|--------|
| Phase 0 | 1æ—¥ | è¦ä»¶ç¢ºå®šã€æŠ€è¡“èª¿æŸ» | research.md âœ… |
| Phase 1 | 2æ—¥ | DBæ§‹ç¯‰ã€ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ | 20ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ |
| Phase 2 | 3æ—¥ | ãƒãƒƒãƒå‡¦ç†å®Ÿè£… | ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã€ãƒãƒƒãƒãƒ³ã‚° |
| Phase 3 | 2æ—¥ | Frontendå®Ÿè£… | ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç”»é¢ |
| Phase 4 | 1æ—¥ | çµ±åˆãƒ†ã‚¹ãƒˆ | E2Eãƒ†ã‚¹ãƒˆå®Œäº† |
| Phase 5 | 1æ—¥ | æœ€é©åŒ–ã€æ–‡æ›¸åŒ– | æœ¬ç•ªæº–å‚™å®Œäº† |

### 10.2 ã‚¿ã‚¹ã‚¯ä¸¦åˆ—åŒ–æˆ¦ç•¥

```mermaid
gantt
    title å®Ÿè£…ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
    dateFormat  YYYY-MM-DD
    section Phase1
    DBæ§‹ç¯‰           :a1, 2025-09-17, 1d
    ãƒ‡ãƒ¼ã‚¿æŠ•å…¥       :a2, after a1, 1d
    Frontendç’°å¢ƒ     :a3, 2025-09-17, 1d
    
    section Phase2
    ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°     :b1, after a2, 1.5d
    ãƒãƒƒãƒãƒ³ã‚°       :b2, after b1, 1.5d
    UIåŸºæœ¬å®Ÿè£…       :b3, after a3, 2d
    
    section Phase3
    ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆ       :c1, after b2, 1d
    UIé«˜åº¦æ©Ÿèƒ½       :c2, after b3, 1d
    
    section Phase4
    çµ±åˆãƒ†ã‚¹ãƒˆ       :d1, after c1 c2, 1d
```

### 10.3 ç¶™ç¶šçš„æ¤œè¨¼æˆ¦ç•¥ï¼ˆtasks.mdã‹ã‚‰çµ±åˆï¼‰

#### æ¤œè¨¼ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- **T010å®Œäº†æ™‚ç‚¹**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã‚’ç¢ºèªå¯èƒ½
- **T010-Bå®Œäº†æ™‚ç‚¹**: SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œç”»é¢ã§æ¤œè¨¼é–‹å§‹å¯èƒ½
- **T036-CHECK**: CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœã‚’å³åº§ã«ç¢ºèª
- **T037-CHECK**: ã‚¹ã‚³ã‚¢è¨ˆç®—çµæœã‚’å³åº§ã«ç¢ºèª
- **T038-CHECK**: ã‚«ãƒ†ã‚´ãƒªåˆ†é¡çµæœã‚’å³åº§ã«ç¢ºèª
- **T039-CHECK**: ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å³åº§ã«ç¢ºèª
- **T040-CHECK**: ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆçµæœã‚’å³åº§ã«ç¢ºèª

#### æ¤œè¨¼ç”¨SQLã‚¯ã‚¨ãƒª

```sql
-- T036-CHECK: CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆå‹•ä½œç¢ºèª
SELECT COUNT(*) FROM jobs WHERE created_at >= CURRENT_DATE;
-- Expected: sample_job_data.csvã®ä»¶æ•°ã¨ä¸€è‡´

SELECT job_id, application_name, company_name FROM jobs LIMIT 5;
-- ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

-- T037-CHECK: ã‚¹ã‚³ã‚¢è¨ˆç®—ç¢ºèª
SELECT job_id, basic_score, seo_score, personalized_score 
FROM job_enrichment LIMIT 10;
-- 3ç¨®é¡ã®ã‚¹ã‚³ã‚¢ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

SELECT AVG(basic_score), MIN(basic_score), MAX(basic_score) 
FROM job_enrichment;
-- ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒãŒå¦¥å½“ã‹ç¢ºèª

-- T038-CHECK: ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ç¢ºèª
SELECT need_category_id, COUNT(*) as job_count 
FROM job_need_categories 
GROUP BY need_category_id;
-- 14ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

-- T039-CHECK: ãƒãƒƒãƒãƒ³ã‚°çµæœç¢ºèª
SELECT user_id, COUNT(*) as match_count 
FROM user_job_mapping 
WHERE created_at >= CURRENT_DATE 
GROUP BY user_id LIMIT 10;
-- å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç´„40ä»¶ãƒãƒƒãƒã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

SELECT section_type, COUNT(*) as job_count 
FROM daily_job_picks 
WHERE user_id = 1 
GROUP BY section_type;
-- editorial_picks:5ä»¶, TOP5:5ä»¶, regional:10ä»¶, nearby:8ä»¶, high_income:7ä»¶, new:5ä»¶

-- T040-CHECK: ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆç¢ºèª
SELECT user_id, LENGTH(email_content) as content_size 
FROM daily_email_queue 
WHERE created_at >= CURRENT_DATE 
LIMIT 10;
-- HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

SELECT COUNT(*) FROM daily_email_queue 
WHERE created_at >= CURRENT_DATE;
-- å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
```

### 10.4 æŠ€è¡“çš„æ„æ€æ±ºå®šè¨˜éŒ²

| æ±ºå®šäº‹é … | é¸æŠ | æ ¹æ‹  | ä»£æ›¿æ¡ˆ |
|---------|------|------|--------|
| ãƒãƒƒãƒå‡¦ç†è¨€èª | Python | pandasã®åŠ¹ç‡æ€§ | Node.js |
| Frontend FW | Next.js 14 | App Routerã€RSC | Remix |
| DB | Supabase | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã€èªè¨¼ | Firebase |
| ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° | å®Ÿæ•°è¨ˆç®— | ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ | æ©Ÿæ¢°å­¦ç¿’ |
| ä¸¦åˆ—å‡¦ç† | 5ãƒ¯ãƒ¼ã‚«ãƒ¼ | CPU/ãƒ¡ãƒ¢ãƒªãƒãƒ©ãƒ³ã‚¹ | 10ãƒ¯ãƒ¼ã‚«ãƒ¼ |
| AIçµ±åˆ | GPT-5 nano | æ—¥æœ¬èªæœ€é©åŒ– | GPT-4 |

---

## 11. å“è³ªä¿è¨¼

### 11.1 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

#### ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰
```
        /E2E\         5%ï¼ˆ5ãƒ†ã‚¹ãƒˆï¼‰
       /çµ±åˆ \       25%ï¼ˆ25ãƒ†ã‚¹ãƒˆï¼‰
      /      \
     /å¥‘ç´„    \     30%ï¼ˆ30ãƒ†ã‚¹ãƒˆï¼‰
    /_________ \
   /   å˜ä½“     \   40%ï¼ˆ40ãƒ†ã‚¹ãƒˆï¼‰
```

#### TDDå®Ÿè£…ãƒ•ãƒ­ãƒ¼
1. RED: ãƒ†ã‚¹ãƒˆä½œæˆï¼ˆå¤±æ•—ç¢ºèªï¼‰
2. GREEN: æœ€å°å®Ÿè£…ï¼ˆãƒ†ã‚¹ãƒˆé€šéï¼‰
3. REFACTOR: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

### 11.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
# è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
def test_full_batch_performance():
    """
    10ä¸‡æ±‚äººÃ—1ä¸‡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è² è·ãƒ†ã‚¹ãƒˆ
    """
    # æº–å‚™
    load_test_data(jobs=100000, users=10000)
    
    # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
    start_time = time.time()
    result = run_daily_batch()
    elapsed_time = time.time() - start_time
    
    # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
    assert elapsed_time < 1800  # 30åˆ†ä»¥å†…
    assert get_memory_usage() < 4 * 1024 * 1024 * 1024  # 4GBä»¥å†…
    assert len(result['emails']) == 10000  # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†ç”Ÿæˆ
    assert all(len(e['jobs']) == 40 for e in result['emails'])  # 40ä»¶é¸å®š

def test_scoring_performance():
    """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    jobs = load_test_jobs(10000)
    
    start_time = time.time()
    scores = calculate_scores_batch(jobs)
    elapsed_time = time.time() - start_time
    
    # 1ä¸‡ä»¶ã‚’5åˆ†ä»¥å†…
    assert elapsed_time < 300
    assert len(scores) == len(jobs)
    assert all(0 <= score['total_score'] <= 100 for score in scores)

def test_matching_performance():
    """ãƒãƒƒãƒãƒ³ã‚°æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    users = load_test_users(1000)  # 1000ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
    jobs = load_test_jobs(10000)
    
    start_time = time.time()
    matches = run_matching_batch(users, jobs)
    elapsed_time = time.time() - start_time
    
    # 1000ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’1åˆ†ä»¥å†…
    assert elapsed_time < 60
    assert len(matches) == len(users)
    assert all(len(match['selected_jobs']) == 40 for match in matches)
```

### 11.3 ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å¯¾å¿œ

| ã‚±ãƒ¼ã‚¹ | ç™ºç”Ÿæ¡ä»¶ | å¯¾å¿œæ–¹æ³• | ãƒ†ã‚¹ãƒˆ |
|--------|---------|---------|--------|
| æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ | å¿œå‹Ÿå±¥æ­´0 | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨ | âœ… |
| æ±‚äººä¸è¶³ | <40ä»¶è©²å½“ | ã‚¨ãƒªã‚¢æ‹¡å¤§ã€æ¡ä»¶ç·©å’Œ | âœ… |
| CSVä¸æ­£ | ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼ | è©²å½“è¡Œã‚¹ã‚­ãƒƒãƒ—ã€ãƒ­ã‚° | âœ… |
| é‡è¤‡æ±‚äºº | åŒä¸€job_id | upsertå‡¦ç† | âœ… |
| ãƒ¡ãƒ¢ãƒªä¸è¶³ | >4GBä½¿ç”¨ | ãƒãƒ£ãƒ³ã‚¯å‡¦ç†åˆ‡æ›¿ | âœ… |
| GPT APIéšœå®³ | AIæœå‹™åœæ­¢ | ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ | âœ… |

### 11.4 å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
def calculate_quality_metrics(batch_result):
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
    
    metrics = {}
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ª
    metrics['data_quality'] = {
        'import_success_rate': batch_result['import']['imported'] / batch_result['import']['total'],
        'scoring_coverage': batch_result['scoring']['scored'] / batch_result['import']['imported'],
        'matching_completion_rate': batch_result['matching']['users'] / 10000
    }
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å“è³ª
    metrics['performance'] = {
        'total_processing_time': batch_result['elapsed_time'],
        'memory_efficiency': get_peak_memory_usage() / (4 * 1024 * 1024 * 1024),  # 4GBæ¯”
        'throughput': batch_result['matching']['matches'] / batch_result['elapsed_time']
    }
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ª
    metrics['content_quality'] = {
        'email_generation_success_rate': batch_result['email_generation']['emails'] / 10000,
        'avg_personalized_score': calculate_avg_personalized_score(),
        'section_coverage': calculate_section_coverage()
    }
    
    return metrics

def validate_6_section_structure(selected_jobs):
    """6ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã®æ¤œè¨¼"""
    expected_sections = {
        'editorial_picks': 5,
        'top5': 5,
        'regional': 10,
        'nearby': 8,
        'high_income': 7,
        'new': 5
    }
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å­˜åœ¨ç¢ºèª
    for section, expected_count in expected_sections.items():
        assert section in selected_jobs
        actual_count = len(selected_jobs[section])
        assert actual_count <= expected_count, f"{section}: expected <={expected_count}, got {actual_count}"
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    all_job_ids = []
    for section_jobs in selected_jobs.values():
        all_job_ids.extend([job['job_id'] for job in section_jobs])
    
    assert len(all_job_ids) == len(set(all_job_ids)), "Duplicate jobs found across sections"
    
    # ç·ä»¶æ•°ãƒã‚§ãƒƒã‚¯
    total_jobs = sum(len(jobs) for jobs in selected_jobs.values())
    assert total_jobs == 40, f"Expected 40 total jobs, got {total_jobs}"
```

---

## 12. é‹ç”¨è¦ä»¶

### 12.1 ã‚¤ãƒ³ãƒ•ãƒ©è¦ä»¶

| é …ç›® | è¦ä»¶ | æ¨å¥¨ |
|------|------|------|
| CPU | 4ã‚³ã‚¢ä»¥ä¸Š | 8ã‚³ã‚¢ |
| ãƒ¡ãƒ¢ãƒª | 8GBä»¥ä¸Š | 16GB |
| ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | 100GBä»¥ä¸Š | 200GB SSD |
| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ | 100Mbpsä»¥ä¸Š | 1Gbps |
| OS | Ubuntu 22.04 | - |
| ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ | PostgreSQL 15 | - |

### 12.2 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

```yaml
backup_policy:
  database:
    frequency: daily
    retention: 30 days
    method: pg_dump
    storage: S3
    
  application:
    frequency: weekly
    retention: 90 days
    method: full snapshot
    
  logs:
    frequency: continuous
    retention: 90 days
    method: log shipping
    storage: CloudWatch
```

### 12.3 ç½å®³å¾©æ—§è¨ˆç”»

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç›®æ¨™å€¤ | å®Ÿç¾æ–¹æ³• |
|-----------|--------|---------
| RTO | 4æ™‚é–“ | ãƒ›ãƒƒãƒˆã‚¹ã‚¿ãƒ³ãƒã‚¤ |
| RPO | 1æ™‚é–“ | å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— |
| å¯ç”¨æ€§ | 99.5% | å†—é•·æ§‹æˆ |

### 12.4 ãƒ‡ãƒ¼ã‚¿ä¿æŒãƒãƒªã‚·ãƒ¼

| ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ | ä¿æŒæœŸé–“ | ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å…ˆ | å‰Šé™¤æ–¹æ³• |
|-----------|---------|-------------|----------|
| user_actions | 365æ—¥ | S3 Glacier | è‡ªå‹•å‰Šé™¤ |
| daily_job_picks | 30æ—¥ | ãªã— | ç‰©ç†å‰Šé™¤ |
| daily_email_queue | 7æ—¥ | ãªã— | ç‰©ç†å‰Šé™¤ |
| ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« | 90æ—¥ | S3 Standard | è‡ªå‹•ç§»è¡Œ |
| ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— | 30æ—¥ | S3 Standard-IA | æ‰‹å‹•å‰Šé™¤ |

---

## 13. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

### 13.1 å‰ææ¡ä»¶

- Python 3.11+ ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- Node.js 20+ ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- Supabaseã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆæ¸ˆã¿
- Git ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿

### 13.2 30åˆ†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

#### Step 1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ (2åˆ†)

```bash
git clone https://github.com/your-org/job-matching-system.git
cd job-matching-system
```

#### Step 2: Supabaseãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ (5åˆ†)

```bash
# Supabase CLIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install -g supabase

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
supabase init
supabase start

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
cat > .env.local << EOF
SUPABASE_URL=http://localhost:54321
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_KEY=your-service-key
DATABASE_URL=postgresql://postgres:postgres@localhost:54322/postgres
EOF
```

#### Step 3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (8åˆ†)

```bash
# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
supabase db push

# ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
python scripts/import_master_data.py

# ã‚µãƒ³ãƒ—ãƒ«æ±‚äººãƒ‡ãƒ¼ã‚¿æŠ•å…¥
python scripts/import_sample_jobs.py \
  --file data/sample_job_data.csv \
  --batch-size 1000
```

#### Step 4: Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (8åˆ†)

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# åˆå›ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ
python src/batch/scoring.py --initial-run
```

#### Step 5: Next.js ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (5åˆ†)

```bash
cd frontend
npm install
npm run dev
```

#### Step 6: å‹•ä½œç¢ºèª (2åˆ†)

```bash
# ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
curl http://localhost:3000/api/monitoring/health

# SQLãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç”»é¢
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:3000/monitoring ã‚’é–‹ã

# ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªå®Ÿè¡Œ
SELECT COUNT(*) FROM jobs WHERE is_active = true;
```

### 13.3 å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ

#### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª

```python
# Python shell ã§å®Ÿè¡Œ
from src.db import get_supabase_client

client = get_supabase_client()
response = client.table('jobs').select('count').execute()
print(f"æ±‚äººãƒ‡ãƒ¼ã‚¿: {response.data[0]['count']}ä»¶")
```

#### APIå‹•ä½œç¢ºèª

```bash
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:3000/api/monitoring/health

# çµ±è¨ˆæƒ…å ±å–å¾—
curl http://localhost:3000/api/monitoring/stats

# ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒƒãƒãƒ³ã‚°çµæœç¢ºèª
curl http://localhost:3000/api/matching/results/1
```

---

## 14. ä»˜éŒ²

### 14.1 ç”¨èªé›†

| ç”¨èª | èª¬æ˜ |
|------|------|
| endcl_cd | ã‚¨ãƒ³ãƒ‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚³ãƒ¼ãƒ‰ï¼ˆä¼æ¥­ã®ä¸€æ„è­˜åˆ¥å­ï¼‰ |
| fee | å¿œå‹Ÿä¿ƒé€²è²»ç”¨ï¼ˆä¼æ¥­ãŒæ”¯æ‰•ã†å¿œå‹Ÿå˜ä¾¡ï¼‰ |
| occupation_cd1 | è·ç¨®å¤§åˆ†é¡ã‚³ãƒ¼ãƒ‰ |
| pref_cd | éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ |
| city_cd | å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ |
| editorial_picks | ç·¨é›†éƒ¨ãŠã™ã™ã‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆv3.0ã§è¿½åŠ ï¼‰ |
| GPT-5 nano | OpenAIã®è»½é‡AI ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¡ãƒ¼ãƒ«ä»¶åç”Ÿæˆç”¨ï¼‰ |

### 14.2 å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **ERå›³**: [`20250904_er_complete_v2.0.mmd`](./20250904_er_complete_v2.0.mmd)
- **ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿**: [`sample_job_data.csv`](../../data/sample_job_data.csv)
- **è·ç¨®ãƒã‚¹ã‚¿ãƒ¼**: [`occupation_view.csv`](../../data/occupation_view.csv)
- **é›‡ç”¨å½¢æ…‹ãƒã‚¹ã‚¿ãƒ¼**: [`employment_type_view.csv`](../../data/employment_type_view.csv)

### 14.3 APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ |
|---------------|---------|------|
| /jobs/import | POST | CSVä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ |
| /jobs/scoring | POST | ã‚¹ã‚³ã‚¢è¨ˆç®—å®Ÿè¡Œ |
| /jobs/categorize | POST | ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å®Ÿè¡Œ |
| /matching/execute | POST | ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†å®Ÿè¡Œ |
| /matching/results/:userId | GET | ãƒãƒƒãƒãƒ³ã‚°çµæœå–å¾— |
| /email/generate | POST | ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆï¼ˆå˜ä½“ï¼‰ |
| /email/batch-generate | POST | ãƒ¡ãƒ¼ãƒ«ä¸€æ‹¬ç”Ÿæˆ |
| /monitoring/query | POST | SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œ |
| /monitoring/stats | GET | çµ±è¨ˆæƒ…å ±å–å¾— |
| /monitoring/health | GET | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ |

### 14.4 ç’°å¢ƒå¤‰æ•°è¨­å®š

```bash
# .env.local
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

# ãƒãƒƒãƒå‡¦ç†è¨­å®š
BATCH_SIZE=1000
PARALLEL_WORKERS=5
MAX_MEMORY_GB=4
BATCH_SCHEDULE="0 3 * * *"  # æ¯æ—¥03:00

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
MONITORING_PORT=3001
ALERT_EMAIL=admin@example.com
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...

# AIè¨­å®šï¼ˆãƒ¡ãƒ¼ãƒ«ä»¶åç”Ÿæˆï¼‰
OPENAI_API_KEY=sk-...
GPT5_NANO_MAX_TOKENS=50
GPT5_NANO_TEMPERATURE=0.7
```

### 14.5 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|---------
| ãƒãƒƒãƒå‡¦ç†ãŒ30åˆ†è¶…é | ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ  | ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°å¢—åŠ ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ– |
| ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ | ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºå¤§ | BATCH_SIZE ã‚’500ã«å‰Šæ¸› |
| ã‚¹ã‚³ã‚¢è¨ˆç®—ãŒ0 | feeâ‰¤500 | feeé–¾å€¤ã®èª¿æ•´æ¤œè¨ |
| ãƒãƒƒãƒãƒ³ã‚°çµæœ<40ä»¶ | æ¡ä»¶å³ã—ã™ã | ã‚¨ãƒªã‚¢æ‹¡å¤§ã€æ¡ä»¶ç·©å’Œ |
| SQLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸è¶³ | EXPLAINåˆ†æã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ  |
| GPT API ã‚¨ãƒ©ãƒ¼ | ãƒ¬ãƒ¼ãƒˆåˆ¶é™ | ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ |

### 14.6 ä»Šå¾Œã®æ‹¡å¼µè¨ˆç”»

#### Phase 1ï¼ˆ3ãƒ¶æœˆä»¥å†…ï¼‰
- ãƒ¡ãƒ¼ãƒ«é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ é€£æºï¼ˆSendGrid/SESï¼‰
- A/Bãƒ†ã‚¹ãƒˆæ©Ÿèƒ½
- ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®MLåŒ–

#### Phase 2ï¼ˆ6ãƒ¶æœˆä»¥å†…ï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒƒãƒãƒ³ã‚°
- ãƒ—ãƒƒã‚·ãƒ¥é€šçŸ¥å¯¾å¿œ
- ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªé–‹ç™º

#### Phase 3ï¼ˆ12ãƒ¶æœˆä»¥å†…ï¼‰
- å¤šè¨€èªå¯¾å¿œ
- ä¼æ¥­å‘ã‘ç®¡ç†ç”»é¢
- AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆçµ±åˆ

### 14.7 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

#### ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

```python
# Pandasæœ€é©åŒ–è¨­å®š
dtype_config = {
    'job_id': 'uint32',
    'endcl_cd': 'category',
    'pref_cd': 'category',
    'city_cd': 'category',
    'min_salary': 'uint32',
    'max_salary': 'uint32',
    'fee': 'uint16'
}

# PyArrow backendä½¿ç”¨
pd.options.mode.string_storage = "pyarrow"
```

#### å‡¦ç†æ™‚é–“æœ€é©åŒ–

```python
# ä¸¦åˆ—å‡¦ç†è¨­å®š
import multiprocessing as mp
max_workers = min(5, mp.cpu_count())

# ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æœ€é©åŒ–
optimal_chunk_size = min(1000, total_records // max_workers)
```

### 14.8 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

#### SQL ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–

```typescript
// SELECTæ–‡ã®ã¿è¨±å¯
if (!query.toLowerCase().trim().startsWith('select')) {
  throw new Error('SELECTæ–‡ã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™');
}

// ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªä½¿ç”¨
const { data } = await supabase
  .rpc('execute_safe_query', { query_text: query });
```

#### èªè¨¼ãƒ»èªå¯

```python
# Supabase RLSï¼ˆRow Level Securityï¼‰ä½¿ç”¨
CREATE POLICY "Users can only access their own data" ON user_profiles
FOR ALL USING (auth.uid() = user_id);
```

---

## æ”¹è¨‚å±¥æ­´

| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | æ—¥ä»˜ | å¤‰æ›´å†…å®¹ | ä½œæˆè€… |
|-----------|------|---------|--------|
| v1.0 | 2025-09-01 | åˆç‰ˆä½œæˆï¼ˆ5ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆï¼‰ | - |
| v2.0 | 2025-09-04 | ERå›³å®Œæˆã€è©³ç´°è¨­è¨ˆè¿½åŠ  | - |
| v3.0 | 2025-09-16 | 6ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆå¤‰æ›´ã€editorial_picksè¿½åŠ  | - |
| v4.0 | 2025-09-16 | å®Œå…¨çµ±åˆç‰ˆã€å®Ÿè£…è©³ç´°è¿½åŠ  | Claude Code |
| v5.0 | 2025-09-16 | **æœ€çµ‚çµ±åˆç‰ˆ**ã€å…¨ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã€GPT-5 nanoçµ±åˆã€ç¶™ç¶šçš„æ¤œè¨¼æˆ¦ç•¥ | Claude Code |

---

**ğŸ¯ æœ€çµ‚çµ±åˆä»•æ§˜æ›¸ v5.0 å®Œæˆ**

*ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ãƒã‚¤ãƒˆæ±‚äººãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã¦ã®ä»•æ§˜ãƒ»å®Ÿè£…è©³ç´°ãƒ»é‹ç”¨è¦ä»¶ã‚’è¨˜è¼‰ã—ãŸæœ€çµ‚çµ±åˆç‰ˆã§ã™ã€‚*  
*v1.0ã‹ã‚‰v4.0ã®å…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€answers.mdã€asks.mdã€data-model.mdã€plan.mdã€tasks.mdã€research.mdã€quickstart.mdç­‰ã®å…¨æƒ…å ±ã‚’çµ±åˆã—ã€å®Ÿè£…å¯èƒ½ãªå®Œå…¨ä»•æ§˜æ›¸ã¨ã—ã¦ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚*

*å•é¡Œã‚„ææ¡ˆãŒã‚ã‚‹å ´åˆã¯ GitHub Issues ã¸ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚*