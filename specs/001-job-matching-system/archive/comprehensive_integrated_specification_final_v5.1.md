# バイト求人マッチングシステム 最終統合仕様書 v5.0（最適化版）

**作成日**: 2025-09-16  
**文書タイプ**: 最終統合仕様書（コード削減・日本語説明版）  
**ステータス**: Production Ready  
**対象読者**: 開発チーム、プロジェクトマネージャー、ステークホルダー  

---

## 目次

1. [エグゼクティブサマリー](#1-エグゼクティブサマリー)
2. [システム概要](#2-システム概要)
3. [技術アーキテクチャ](#3-技術アーキテクチャ)
4. [データモデル設計](#4-データモデル設計)
5. [コア機能詳細](#5-コア機能詳細)
6. [スコアリングアルゴリズム詳細](#6-スコアリングアルゴリズム詳細)
7. [6セクションメール構成](#7-6セクションメール構成)
8. [バッチ処理パイプライン](#8-バッチ処理パイプライン)
9. [モニタリングシステム](#9-モニタリングシステム)
10. [実装計画](#10-実装計画)
11. [品質保証](#11-品質保証)
12. [運用要件](#12-運用要件)
13. [クイックスタートガイド](#13-クイックスタートガイド)
14. [付録](#14-付録)

---

## 1. エグゼクティブサマリー

### 1.1 プロジェクト概要
本システムは、10万件のバイト求人データから1万人のユーザーそれぞれに最適な40件を毎日自動選定し、パーソナライズされたメール配信準備を行う大規模マッチングシステムです。

### 1.2 ビジネス価値
- **応募率向上**: 従来比150%の応募率向上を目標
- **ユーザー満足度**: パーソナライゼーションによる関連性の高い求人提供
- **運用効率化**: 完全自動化により手動作業を排除

### 1.3 主要成果物
- 毎日1万通のパーソナライズメール生成（6セクション×40求人）
- リアルタイムSQLモニタリングダッシュボード
- 30分以内の全処理完了保証

### 1.4 主要変更点（v4.0→v5.0）
- **実装詳細の完全統合**: answers.mdの全実装コードを統合
- **GPT-5 nano統合**: メール件名生成の詳細実装
- **継続的検証戦略**: tasks.mdのCHECKポイント手法統合
- **MCPサーバー活用**: Sequential, Serena, Magic等の効果的活用戦略
- **クイックスタートガイド**: 30分セットアップ手順の統合

---

## 2. システム概要


### 2.1 システムの目的
求職者により良いバイト求人情報を届けることで、マッチング精度を向上させ、応募率と採用成功率を最大化する。

### 2.2 主要機能一覧

| 機能カテゴリ | 機能名 | 説明 |
|------------|--------|------|
| データ処理 | CSVインポート | 10万件の求人データを日次でインポート |
| スコアリング | 3段階スコア計算 | 基礎・SEO・パーソナライズスコアの算出 |
| カテゴリ分類 | 自動分類 | 14ニーズ×12職種カテゴリへの分類 |
| マッチング | 最適化選定 | 各ユーザーに最適な40件を選定 |
| メール生成 | 6セクション構成 | パーソナライズされたHTML生成 |
| モニタリング | SQL実行画面 | リアルタイムデータ確認 |

### 2.3 システム利用者

#### エンドユーザー（求職者）
- **規模**: 1万人
- **特徴**: 18-65歳、アルバイト・パート希望者
- **地域**: 全国（主に都市部）

#### システム管理者
- **規模**: 5-10名
- **役割**: データ監視、エラー対応、パフォーマンス管理

### 2.4 配信メール構成（6セクション）
- 編集部おすすめ人気バイト TOP5【NEW】- fee×応募数で選定
- あなたにおすすめ求人 TOP5 - パーソナライズスコア上位
- 東京都おすすめ求人 TOP10 - 都道府県内＋職種マッチング
- 小金井市で人気のバイト 8選 - 市区町村周辺求人
- 編集部おすすめ "高収入・日払い" TOP7 - 高時給 OR 日払い可能
- 新着求人 TOP5 - 7日以内投稿求人

---

## 3. 技術アーキテクチャ

### 3.1 技術スタック
- **バックエンド**: Python 3.11, FastAPI, APScheduler, pandas with PyArrow, scikit-learn, implicit(協調フィルタリング)
- **AI統合**: OpenAI GPT-5 nano (フォールバック: ルールベース生成)
- **フロントエンド**: TypeScript 5.0, Next.js 14 App Router, React 18, Tailwind CSS 3.3, Zustand 4.4
- **データベース**: Supabase (PostgreSQL 15), PgBouncer, Daily snapshots
- **インフラ**: Ubuntu 22.04 LTS, Node.js 20 LTS, Docker (optional)
- **モニタリング**: Grafana, Prometheus, Sentry

### 3.2 システム処理フロー
```
Daily Batch (03:00-06:00)
├── Phase 1: データインポート (CSV→DB, 100K jobs)
├── Phase 2: スコアリング (Basic, SEO, Personalized)
├── Phase 3: マッチング (10K users × 40 jobs)
└── Phase 4: メール生成 (6 sections × 40 jobs)
```

### 3.3 システム構成図

```
┌─────────────────────────────────────────────────────────────┐
│                    Daily Batch Process (03:00-06:00)          │
├─────────────────┬──────────────────┬────────────────────────┤
│   Phase 1       │     Phase 2      │      Phase 3           │
│   Data Import   │     Scoring      │      Matching          │
│   CSV→DB        │  3 algorithms    │    10K users           │
│   (100K jobs)   │  (Basic,SEO,     │    (40 jobs/user)      │
│                 │   Personalized)  │                        │
└─────────────────┴──────────────────┴────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                  Email Generation (Phase 4)                   │
│          6 Sections × 40 Jobs = Personalized Content         │
│   ┌──────────────────────────────────────────────────┐      │
│   │ 1. Editorial Picks (5)  - fee × clicks          │      │
│   │ 2. TOP5 (5)            - Personalized score     │      │
│   │ 3. Regional (10)       - Prefecture match       │      │
│   │ 4. Nearby (8)          - City area match        │      │
│   │ 5. High Income (7)     - Salary/Daily pay       │      │
│   │ 6. New (5)             - Within 7 days          │      │
│   └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                SQL Monitoring Interface                       │
│                    (Next.js + Supabase)                      │
│   - Real-time query execution                                │
│   - Data visualization dashboard                             │
│   - Error log viewer                                         │
│   - Manual batch trigger                                     │
└─────────────────────────────────────────────────────────────┘
```
### 3.4 並列処理最適化戦略
- **Group A**: 独立実行可能タスク (DB設定、Frontend設定、テスト環境) - 20分
- **Group B**: Group A完了後 (CSVインポート、スコアリング、UI実装) - 1時間
- **Group C**: Group B完了後 (マッチング、メール生成、統合テスト) - 1.5時間
- **効率向上**: 従来8時間 → 並列化で3時間（62.5%削減）

### 3.5 MCPサーバー活用戦略
| サーバー | 用途 | 効率向上 |
|----------|------|----------|
| Sequential | 複雑な分析、システム設計 | 30-50% |
| Serena | 大規模コード操作、シンボル管理 | 40-60% |
| Magic | UI コンポーネント生成 | 50-70% |
| Context7 | ライブラリドキュメント参照 | 20-30% |
| Playwright | E2Eテスト | 60-80% |

---

## 4. データモデル設計

### 4.1 テーブル構成（20テーブル）

#### トランザクションテーブル（8）
1. **jobs** - 求人マスター（10万件、100+フィールド）
2. **users** - ユーザー基本情報（1万件）
3. **user_actions** - 行動履歴（応募、クリック、開封）
4. **user_profiles** - ユーザープロファイル（応募傾向集計）
5. **user_job_mapping** - マッチング結果（日次40万件）
6. **daily_job_picks** - 選定求人（配信用整形済み）
7. **daily_email_queue** - メール配信キュー（6セクション構成）
8. **job_enrichment** - 求人拡張情報（スコア、カテゴリ）

#### マスターテーブル（10）
9-18. 職種、地域、雇用形態、特徴、SEOキーワード等のマスターデータ

#### 補助テーブル（2）
19-20. マッチング用簡易データ、コンテンツ表示用データ

### 4.2 重要フィールド定義

**jobsテーブル主要フィールド**
- 識別: job_id, endcl_cd (企業識別)
- 内容: application_name, company_name
- 給与: min_salary, max_salary, fee (応募促進費用)
- 場所: pref_cd, city_cd, station_name_eki
- カテゴリ: occupation_cd1, employment_type_cd
- 特徴: feature_codes (カンマ区切り), hours
- 日付: posting_date, end_at

**user_profilesテーブル**
- 基本: user_id, total_applications
- 頻度データ: applied_pref_cds, applied_city_cds等（"コード:回数"形式）
- 統計: applied_salary_stats (JSON形式)

### 4.3 インデックス・パーティション戦略
- **地域検索用**: (pref_cd, city_cd, posting_date)
- **カテゴリ検索用**: (occupation_cd1, employment_type_cd)
- **時系列パーティション**: user_actions(月単位), user_job_mapping(週単位)
- **保持期間**: user_actions(6ヶ月), user_job_mapping(1ヶ月), daily_email_queue(7日)

---

## 5. コア機能詳細

### 5.1 求人データインポート機能

#### 処理フロー
1. **CSVチャンク読込**: 1000件単位でメモリ効率的に処理
2. **データクレンジング**:
   - 給与異常値処理（800円未満、5000円超を除外）
   - 都道府県別最低賃金チェック
   - 場所データ検証（都道府県・市区町村コードの整合性）
3. **特徴コード展開**: カンマ区切りコードを個別フラグに変換
4. **ビジネスルール適用**:
   - 雇用形態フィルタ（アルバイト・パートのみ）
   - 応募促進費用500円以上
   - 投稿日30日以内
   - 重複除外（job_id + endcl_cd）
5. **Supabase一括挿入**: upsert処理で重複時は更新

#### データ型最適化設定
- job_id: uint32
- endcl_cd, pref_cd, city_cd: category型
- occupation_cd1: uint16
- employment_type_cd: uint8
- 給与・fee: uint32/uint16

### 5.2 カテゴリ自動分類

#### 14ニーズベースカテゴリ判定ロジック

| カテゴリ | 判定条件 | 実装方法 |
|---------|----------|----------|
| 日払い・週払い | feature_code: 103, 104 またはキーワード | フラグとテキスト検索 |
| 短期・単発OK | キーワード: 短期、単発、1日 | テキスト検索 |
| 高時給 | エリア平均×1.2以上 | 統計計算 |
| シフト自由 | キーワード: シフト自由、週1 | テキスト検索 |
| 未経験歓迎 | feature_code: 105 | フラグチェック |
| 在宅・リモート | キーワード: 在宅、リモート | テキスト検索 |
| 学生歓迎 | feature_code: 106 | フラグチェック |
| 高校生歓迎 | キーワード: 高校生 | テキスト検索 |
| 主婦歓迎 | キーワード＋日中シフト判定 | 複合条件 |
| シニア歓迎 | キーワード: シニア、60歳 | テキスト検索 |
| 土日のみOK | キーワード: 土日のみ | テキスト検索 |
| 副業・WワークOK | キーワード: 副業、Wワーク | テキスト検索 |
| 交通費支給 | feature_code: 108 | フラグチェック |
| 即日勤務OK | キーワード: 即日 | テキスト検索 |

#### 12職種カテゴリ
- 100番台: 飲食・フード系
- 200番台: 販売・サービス系
- 300番台: 配送・ドライバー系
- 400番台: オフィスワーク系
- 500番台: 医療・介護・保育系
- 600番台: 軽作業・工場系
- 700-1200+: 美容、教育、エンタメ、イベント、IT、その他

---

## 6. スコアリングアルゴリズム詳細

### 6.1 基礎スコア（0-100点）

#### 構成要素と重み付け
- **時給スコア（40%）**:
  - エリア平均1.5倍以上: 100点
  - エリア平均1.2倍以上: 80点
  - エリア平均以上: 60点
  - エリア平均未満: 比例計算
- **応募単価報酬スコア（30%）**:
  - 500円以下: 0点
  - 5000円以上: 100点
  - 中間: 線形補間
- **企業人気度スコア（30%）**:
  - 過去360日間の応募率に基づく
  - 15%以上: 100点
  - 10%以上: 80点
  - 5%以上: 60点

#### フィルタリング条件
- 雇用形態: アルバイト・パート等のみ（コード: 1,3,6,8）
- 応募促進費用: 500円以上必須

### 6.2 SEOスコア（0-100点）

#### フィールド別重み付け
- application_name, company_name: 1.5倍
- station_name_eki: 0.5倍
- salary, hours: 0.3倍
- feature_codes: 0.8倍

#### キーワードマッチング処理
1. テキスト正規化（HTMLタグ除去、小文字化）
2. SEMrushキーワード（1000件）との照合
3. 検索ボリュームベースのスコア算出:
   - 10000以上: 15点
   - 5000以上: 10点
   - 1000以上: 7点
   - その他: 3点
4. 最大7キーワードまで加算

### 6.3 パーソナライズスコア（0-100点）

#### 評価要素と重み
- **都道府県マッチング（20%）**: 応募履歴との一致度
- **市区町村マッチング（15%）**: 近隣エリアの考慮
- **職種マッチング（20%）**: 過去の応募職種との一致
- **給与レンジマッチング（15%）**: ±10%以内で満点
- **エンドクライアント重複チェック（15%）**: 2週間以内応募は大幅減点
- **雇用形態マッチング（15%）**: 希望雇用形態との一致

#### デフォルト処理
- プロファイル未設定時: 50点
- データ不足時: 部分評価で補完

---

## 7. 6セクションメール構成

### 7.1 セクション構成と選定ロジック

| セクション | 件数 | 選定基準 | 優先度 |
|-----------|------|----------|--------|
| 編集部おすすめ | 5 | fee×応募クリック数=endcl_cd＋市区町村＞隣接エリア＞都道府県内 | 最高 |
| TOP5 | 5 | パーソナライズスコア上位 | 高 |
| 地域別 | 10 | 都道府県内＋職種マッチ＋総合スコア上位 | 中 |
| 近隣 | 8 | 市区町村＋隣接エリア＋総合スコア上位 | 中 |
| 高収入・日払い | 7 | 高時給 OR 日払い可＋総合スコア上位＋市区町村＞隣接エリア＞都道府県内 | 低 |
| 新着 | 5 | 7日以内投稿＋総合スコア上位＋市区町村＞隣接エリア＞都道府県内 | 最低 |

### 7.2 重複除外処理
1. 優先順位順にセクション処理
2. 既選択job_idを記録・除外
3. 不足分は他セクションから補完
4. 合計40件を確保

### 7.3 編集部おすすめロジック
- **基本スコア**: fee × 最近30日間の応募数
- **地域重み付け**:
  - 同一市区町村: 1.0
  - 近隣市区町村: 0.7
  - 同一都道府県: 0.5
  - その他: 0.3
- **除外条件**: 2週間以内応募企業

### 7.4 GPT-5 nano統合による件名生成

#### 処理フロー
1. **プロンプト構築**:
   - ユーザー情報（地域、年齢層）
   - 上位求人の特徴抽出
   - 高時給・日払い案件数の集計
2. **GPT-5 nano呼び出し**:
   - モデル: gpt-5-nano
   - 最大トークン: 50
   - 温度: 0.7
3. **フォールバック処理**:
   - API失敗時はルールベース生成
   - テンプレート5種類からランダム選択
   - 変数: 地域、件数、日付

#### 生成例
- "🎯 東京都で見つけた！高時給バイト40件"
- "💰 7/15更新！小金井市限定おすすめバイト40選"

---

## 8. バッチ処理パイプライン

### 8.1 日次処理スケジュール
| 時間 | フェーズ | 処理内容 |
|------|----------|----------|
| 03:00-03:30 | Phase 1 | CSVインポート、クレンジング、DB投入 |
| 03:30-04:30 | Phase 2 | 3種類スコア計算、job_enrichment更新 |
| 04:30-05:30 | Phase 3 | 1万ユーザー×40件マッチング |
| 05:30-06:00 | Phase 4 | HTML生成、件名生成、配信準備 |

### 8.2 並列処理実装

#### ParallelBatchProcessor概要
- **最大ワーカー数**: 5（CPU数とmin(5, cpu_count())）
- **プロセスプール**: ProcessPoolExecutor使用
- **タイムアウト設定**: 
  - スコアリング: 10分
  - マッチング: 15分
  - メール生成: 5分

#### 並列化戦略
1. **Phase 1**: 単一プロセス（順次処理必須）
2. **Phase 2-4**: 並列処理
   - データをワーカー数で均等分割
   - 各ワーカーが独立処理
   - 結果を集約して統計情報生成

#### エラーハンドリング
- **CSV解析エラー**: 行スキップ、ログ記録
- **DB接続エラー**: 指数バックオフで3回リトライ
- **マッチング失敗**: 前日データフォールバック
- **メモリ不足**: チャンクサイズ縮小で段階的処理
- **GPT APIエラー**: ルールベース生成にフォールバック

### 8.3 バッチ処理統計
- 総処理件数、エラー数、スキップ数を記録
- 各フェーズの処理時間を測定
- 完了通知とエラーアラート送信

---

## 9. モニタリングシステム

### 9.1 SQLモニタリングインターフェース

#### 基本機能
- リアルタイムSQLクエリ実行（SELECT専用）
- クエリ結果のテーブル表示・CSV出力
- クエリ履歴の保存と再実行
- よく使うクエリのテンプレート化

#### Next.js実装概要
- **フロントエンド**: React Query(SWR)でデータフェッチ
- **バックエンド**: API Routes経由でSupabase接続
- **セキュリティ**: 読み取り専用ロール、SQLインジェクション対策
- **UI**: Monaco Editorでシンタックスハイライト

### 9.2 監視メトリクス

#### リアルタイム監視項目
- アクティブユーザー数
- 日次処理進捗
- エラー発生率
- API応答時間
- DB接続プール状況

#### 日次メトリクス
- インポート求人数
- マッチング成功率
- メール生成数
- 平均スコア分布
- 処理時間統計

#### 品質メトリクス
- カバレッジ（全ユーザーへの配信率）
- 多様性（求人の重複率）
- 関連性（パーソナライズスコア平均）
- 鮮度（新着求人の割合）

### 9.3 アラート設定

| 条件 | 閾値 | アクション |
|------|------|-----------|
| エラー率 | >5% | Slack通知 |
| 処理時間 | >30分 | メール通知 |
| メモリ使用率 | >90% | 自動スケールアップ |
| DB接続失敗 | 3回連続 | 緊急コール |

---

## 10. 実装計画

### 10.1 フェーズ別実装スケジュール
- **Phase 1（1週間）**: 環境構築、DB設計、マスターデータ準備
- **Phase 2（2週間）**: インポート機能、スコアリング実装
- **Phase 3（2週間）**: マッチング処理、メール生成
- **Phase 4（1週間）**: モニタリング画面、テスト、デプロイ

### 10.2 タスク並列化戦略
- **独立タスク並列実行**: DB設計とフロントエンド開発を同時進行
- **MCPサーバー活用**: UI生成はMagic、テストはPlaywright
- **継続的検証**: 各フェーズ完了時にCHECKポイント実施

### 10.3 継続的検証戦略

#### 検証チェックポイント
-  DB接続・基本CRUD確認
-  インポート処理の正常動作
-  スコアリング精度検証
-  マッチング結果の妥当性
-  E2E統合テスト

#### 検証用SQLクエリ例
```sql
-- 求人分布確認
SELECT pref_cd, COUNT(*) FROM jobs GROUP BY pref_cd;

-- スコア分布確認
SELECT 
  FLOOR(basic_score/10)*10 as score_range,
  COUNT(*) as count
FROM job_enrichment
GROUP BY score_range;

-- ユーザーマッチング確認
SELECT user_id, COUNT(*) as job_count
FROM user_job_mapping
WHERE match_date = CURRENT_DATE
GROUP BY user_id;
```

### 10.4 技術的意思決定記録
- **Pandas over Spark**: データ量が10万件なのでPandasで十分
- **Supabase選択**: RLS、リアルタイム機能、管理画面が決め手
- **Next.js App Router**: RSC対応、最新のReact機能活用
- **並列処理5ワーカー**: メモリとCPUバランスの最適値

---

## 11. 品質保証

### 11.1 テスト戦略

#### テストピラミッド
- **単体テスト（70%）**: pytest, Jest
- **統合テスト（20%）**: API連携、DB操作
- **E2Eテスト（10%）**: Playwright使用

#### TDD実装フロー
1. 失敗するテストを書く
2. 最小限の実装
3. リファクタリング
4. カバレッジ90%以上を維持

### 11.2 パフォーマンステスト

#### 負荷テストシナリオ
- **通常負荷**: 1万ユーザー×40件を30分で処理
- **ピーク負荷**: 2万ユーザー同時処理
- **限界テスト**: システムの破綻点を特定

#### パフォーマンス目標
- CSVインポート: 10万件/5分
- スコアリング: 10万件/10分
- マッチング: 1万ユーザー/15分
- API応答: 95%タイルで200ms以下

### 11.3 エッジケース対応
- 空データ処理
- 重複データ処理
- 異常値処理（負の給与等）
- タイムアウト処理
- メモリ不足対応

### 11.4 品質メトリクス計算方法
- **カバレッジ**: 配信ユーザー数 ÷ 全アクティブユーザー数
- **多様性**: ユニーク求人数 ÷ 全選定求人数
- **関連性**: 平均パーソナライズスコア ÷ 100
- **鮮度**: 7日以内求人数 ÷ 全選定求人数

---

## 12. 運用要件

### 12.1 インフラ要件
- **CPU**: 8コア以上推奨
- **メモリ**: 16GB以上（並列処理時）
- **ストレージ**: 100GB SSD
- **ネットワーク**: 100Mbps以上
- **OS**: Ubuntu 22.04 LTS

### 12.2 バックアップ戦略
- **データベース**: 日次フルバックアップ、6時間ごと差分
- **アプリケーション**: Gitによるバージョン管理
- **設定ファイル**: 暗号化して別途保管
- **保持期間**: フル30日、差分7日

### 12.3 災害復旧計画
- **RTO**: 4時間以内
- **RPO**: 6時間以内
- **復旧手順**: 文書化済み、四半期ごとに訓練

### 12.4 データ保持ポリシー
- **求人データ**: 90日
- **ユーザー行動ログ**: 180日
- **マッチング結果**: 30日
- **メール履歴**: 7日
- **エラーログ**: 30日

---

## 13. クイックスタートガイド

### 13.1 前提条件
- Node.js 20 LTS、Python 3.11
- Supabaseアカウント
- 8GB以上のメモリ

### 13.2 30分セットアップ手順

#### Step 1: リポジトリクローン（2分）
```bash
git clone <repository-url>
cd job-matching-system
```

#### Step 2: Supabaseプロジェクト作成（5分）
1. Supabase CLIインストール
2. プロジェクト初期化
3. 環境変数設定（.env.local）

#### Step 3: データベースセットアップ（8分）
1. マイグレーション実行
2. マスターデータ投入
3. サンプル求人データインポート

#### Step 4: Pythonバックエンド（8分）
1. 仮想環境作成
2. 依存関係インストール（requirements.txt）
3. 初回スコアリング実行

#### Step 5: Next.jsフロントエンド（5分）
1. npm install
2. npm run dev
3. http://localhost:3000 でアクセス確認

#### Step 6: 動作確認（2分）
1. SQLモニタリング画面確認
2. テストクエリ実行
3. ヘルスチェックAPI確認

### 13.3 動作確認コマンド
```python
# DB接続確認
from src.db import get_supabase_client
client = get_supabase_client()
print(client.table('jobs').select('count').execute())

# API確認
curl http://localhost:3000/api/health
curl http://localhost:3000/api/stats
```

---

## 14. 付録

### 14.1 用語集
- **エンドクライアント**: 求人を出している企業
- **fee**: 応募促進費用（0-5000円）
- **パーソナライズスコア**: ユーザー個別の適合度
- **編集部おすすめ**: fee×応募数で算出する人気度

### 14.2 参照ドキュメント
- Supabase公式ドキュメント
- Next.js App Router ガイド
- Pandas最適化ベストプラクティス
- PostgreSQLパフォーマンスチューニング

### 14.3 APIエンドポイント一覧
| エンドポイント | メソッド | 説明 |
|---------------|---------|------|
| /api/import | POST | CSVインポート開始 |
| /api/scoring | POST | スコアリング実行 |
| /api/matching | POST | マッチング実行 |
| /api/email/generate | POST | メール生成 |
| /api/stats | GET | 統計情報取得 |
| /api/monitoring/query | POST | SQLクエリ実行 |
| /api/health | GET | ヘルスチェック |

### 14.4 環境変数設定
```env
# Supabase
NEXT_PUBLIC_SUPABASE_URL=
NEXT_PUBLIC_SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=

# バッチ処理
BATCH_WORKER_COUNT=5
BATCH_CHUNK_SIZE=1000

# モニタリング
MONITORING_ENABLED=true
ALERT_WEBHOOK_URL=

# AI設定
OPENAI_API_KEY=
GPT_MODEL_NAME=gpt-5-nano
```

### 14.5 トラブルシューティング
| 問題 | 原因 | 解決方法 |
|------|------|----------|
| インポート失敗 | CSV形式不正 | ログ確認、形式修正 |
| スコアリング遅延 | メモリ不足 | チャンクサイズ削減 |
| マッチング不正 | プロファイル欠損 | デフォルト値設定 |
| メール生成エラー | API制限 | フォールバック使用 |

### 14.6 今後の拡張計画

#### Phase 1（3ヶ月以内）
- リアルタイムマッチング
- A/Bテスト機能
- 詳細分析ダッシュボード

#### Phase 2（6ヶ月以内）
- 機械学習モデル導入
- マルチ言語対応
- モバイルアプリ開発

#### Phase 3（12ヶ月以内）
- グローバル展開
- AIチャットボット統合
- 予測分析機能

### 14.7 パフォーマンス最適化ガイド

#### メモリ最適化
- Pandasのdtype最適化
- category型の活用
- PyArrow backend使用
- 不要なカラム即座削除

#### 処理時間最適化
- 並列処理の活用（max_workers調整）
- チャンク処理の最適化
- インデックスの適切な設定
- クエリの最適化

### 14.8 セキュリティガイドライン

#### SQLインジェクション対策
- パラメータ化クエリ使用
- 入力値検証
- エスケープ処理
- 最小権限の原則

#### 認証・認可
- Supabase RLS使用
- JWT トークン管理
- セッション管理
- 監査ログ記録

---

## 改訂履歴
- v5.0 (2025-09-16): コード削減・日本語説明版作成
- v4.0 (2025-09-15): GPT-5 nano統合、継続的検証戦略追加
- v3.0 (2025-09-14): 並列処理最適化、MCPサーバー活用追加
- v2.0 (2025-09-13): データモデル詳細化、スコアリング改善
- v1.0 (2025-09-12): 初版作成

---
*本仕様書は継続的に更新され、実装の進捗に応じて内容が改訂されます。*