#!/usr/bin/env python3
"""
ã‚¿ã‚¹ã‚¯é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
tasks.mdãƒ•ã‚¡ã‚¤ãƒ«ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèªã™ã‚‹
"""

import re
import os
from typing import List, Dict, Tuple
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PROJECT_ROOT = Path("/Users/furuyanaoki/Project/new.mail.score")

def parse_task_files(content: str) -> Dict[str, List[str]]:
    """ã‚¿ã‚¹ã‚¯å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡º"""
    files_by_task = {}
    current_task = None

    lines = content.split('\n')
    for line in lines:
        # ã‚¿ã‚¹ã‚¯IDã‚’æ¤œå‡º
        task_match = re.match(r'#### (T\d+):', line)
        if task_match:
            current_task = task_match.group(1)
            files_by_task[current_task] = []

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¤œå‡º
        if current_task:
            # å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã€ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã€ä»•æ§˜æ›¸ãªã©ã®ãƒ‘ã‚¹ã‚’æŠ½å‡º
            file_patterns = [
                r'å®Ÿè£…:\s*([^\s\[]+)',
                r'ãƒ†ã‚¹ãƒˆ:\s*([^\s\[]+)',
                r'ä»•æ§˜æ›¸:\s*([^\s\[]+)',
                r'`([^`]+\.(py|sql|md|ts|tsx|js|jsx|yaml|yml|json))`',
                r'- \*\*ãƒ•ã‚¡ã‚¤ãƒ«\*\*:\s*`([^`]+)`'
            ]

            for pattern in file_patterns:
                matches = re.findall(pattern, line)
                for match in matches:
                    file_path = match[0] if isinstance(match, tuple) else match
                    if file_path and not file_path.startswith('http'):
                        files_by_task[current_task].append(file_path)

    return files_by_task

def check_file_exists(file_path: str) -> Tuple[bool, str]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª"""
    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
    if not file_path.startswith('/'):
        full_path = PROJECT_ROOT / file_path
    else:
        full_path = Path(file_path)

    exists = full_path.exists()
    status = "âœ…å­˜åœ¨" if exists else "ğŸ”´ä¸æ˜"

    return exists, status

def analyze_dependencies(content: str) -> Dict[str, Dict[str, List[str]]]:
    """ã‚¿ã‚¹ã‚¯é–“ã®ä¾å­˜é–¢ä¿‚ã‚’è§£æ"""
    dependencies = {}
    current_task = None

    lines = content.split('\n')
    for line in lines:
        # ã‚¿ã‚¹ã‚¯IDã‚’æ¤œå‡º
        task_match = re.match(r'#### (T\d+):', line)
        if task_match:
            current_task = task_match.group(1)
            dependencies[current_task] = {
                'upstream': [],
                'downstream': [],
                'cascade': []
            }

        if current_task:
            # ä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º
            if 'å‰æ:' in line or 'â¬†ï¸' in line:
                deps = re.findall(r'T\d+', line)
                dependencies[current_task]['upstream'].extend(deps)
            elif 'ãƒ–ãƒ­ãƒƒã‚¯ä¸­:' in line or 'â¬‡ï¸' in line:
                deps = re.findall(r'T\d+', line)
                dependencies[current_task]['downstream'].extend(deps)
            elif 'å¤‰æ›´æ™‚è¦ç¢ºèª:' in line or 'ğŸ”„' in line:
                deps = re.findall(r'T\d+', line)
                dependencies[current_task]['cascade'].extend(deps)

    return dependencies

def generate_report(files_by_task: Dict[str, List[str]],
                   dependencies: Dict[str, Dict[str, List[str]]]) -> str:
    """ãƒã‚§ãƒƒã‚¯çµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report_lines = []
    report_lines.append("# ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆ\n")
    report_lines.append(f"ãƒã‚§ãƒƒã‚¯æ—¥æ™‚: {os.popen('date').read().strip()}\n\n")

    # ã‚µãƒãƒªãƒ¼
    total_tasks = len(files_by_task)
    total_files = sum(len(files) for files in files_by_task.values())
    existing_files = 0
    missing_files = 0

    # å„ã‚¿ã‚¹ã‚¯ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    report_lines.append("## ã‚¿ã‚¹ã‚¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹\n\n")

    for task_id in sorted(files_by_task.keys()):
        files = list(set(files_by_task[task_id]))  # é‡è¤‡é™¤å»
        if files:
            report_lines.append(f"### {task_id}\n")

            for file_path in files:
                exists, status = check_file_exists(file_path)
                if exists:
                    existing_files += 1
                else:
                    missing_files += 1

                report_lines.append(f"- {status} `{file_path}`\n")

            # ä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’è¿½åŠ 
            if task_id in dependencies:
                deps = dependencies[task_id]
                if deps['upstream']:
                    report_lines.append(f"  - â¬†ï¸ å‰æ: {', '.join(deps['upstream'])}\n")
                if deps['downstream']:
                    report_lines.append(f"  - â¬‡ï¸ ãƒ–ãƒ­ãƒƒã‚¯: {', '.join(deps['downstream'])}\n")
                if deps['cascade']:
                    report_lines.append(f"  - ğŸ”„ é€£é–: {', '.join(deps['cascade'])}\n")

            report_lines.append("\n")

    # ã‚µãƒãƒªãƒ¼ã‚’å…ˆé ­ã«è¿½åŠ 
    summary = [
        "## ğŸ“Š ã‚µãƒãƒªãƒ¼\n\n",
        f"- ç·ã‚¿ã‚¹ã‚¯æ•°: {total_tasks}\n",
        f"- ç·ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§æ•°: {total_files}\n",
        f"- âœ… å­˜åœ¨ç¢ºèª: {existing_files} ({existing_files*100//total_files if total_files else 0}%)\n",
        f"- ğŸ”´ ä¸æ˜: {missing_files} ({missing_files*100//total_files if total_files else 0}%)\n\n"
    ]

    # ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    if missing_files > 0:
        summary.append("## âš ï¸ ä½œæˆãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        for task_id in sorted(files_by_task.keys()):
            files = list(set(files_by_task[task_id]))
            for file_path in files:
                exists, _ = check_file_exists(file_path)
                if not exists:
                    summary.append(f"- [ ] {task_id}: `{file_path}`\n")
        summary.append("\n")

    return ''.join(summary + report_lines)

def main():
    # tasks.mdãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    tasks_file = os.path.join(base_dir, "tasks.md")

    if not os.path.exists(tasks_file):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tasks_file}")
        return

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(tasks_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡º
    files_by_task = parse_task_files(content)

    # ä¾å­˜é–¢ä¿‚ã‚’è§£æ
    dependencies = analyze_dependencies(content)

    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report = generate_report(files_by_task, dependencies)

    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    report_file = os.path.join(base_dir, "file_check_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯å®Œäº†")
    print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    total_files = sum(len(files) for files in files_by_task.values())
    existing_files = 0
    missing_files = 0

    for task_id, files in files_by_task.items():
        for file_path in set(files):
            exists, _ = check_file_exists(file_path)
            if exists:
                existing_files += 1
            else:
                missing_files += 1

    print(f"\nğŸ“Š ã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§æ•°: {total_files}")
    print(f"  - âœ… å­˜åœ¨: {existing_files}")
    print(f"  - ğŸ”´ ä¸æ˜: {missing_files}")

if __name__ == "__main__":
    main()