# AlertManager configuration for job matching system notifications
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'smtp_password'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

route:
  group_by: ['alertname', 'severity', 'component']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default-receiver'
  routes:
    # Critical alerts go to PagerDuty and Slack immediately
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      group_interval: 5m
      repeat_interval: 5m

    # Warning alerts go to Slack
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      group_interval: 10m
      repeat_interval: 2h

    # Business alerts go to business team
    - match:
        team: data_science
      receiver: 'business-alerts'

    # Security alerts get immediate attention
    - match:
        team: security
      receiver: 'security-alerts'
      group_wait: 0s
      repeat_interval: 15m

receivers:
  - name: 'default-receiver'
    email_configs:
      - to: 'dev-team@company.com'
        subject: '[Alert] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}

  - name: 'critical-alerts'
    # Slack notification
    slack_configs:
      - api_url: '{{ .ExternalURL }}/slack/webhook'
        channel: '#alerts-critical'
        username: 'AlertManager'
        color: 'danger'
        title: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Component:* {{ .Labels.component }}
          {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
          {{ end }}
        actions:
          - type: button
            text: 'View in Grafana'
            url: 'http://localhost:3001'
          - type: button
            text: 'View Logs'
            url: 'http://localhost:5601'

    # PagerDuty integration
    pagerduty_configs:
      - service_key: 'your-pagerduty-service-key'
        description: '{{ .GroupLabels.alertname }}'
        details:
          component: '{{ .GroupLabels.component }}'
          severity: '{{ .GroupLabels.severity }}'

    # Email for critical issues
    email_configs:
      - to: 'oncall@company.com'
        subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL ALERT TRIGGERED

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Component: {{ .Labels.component }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ if .Annotations.runbook_url }}
          Runbook: {{ .Annotations.runbook_url }}
          {{ end }}
          {{ end }}

          Please take immediate action to resolve this issue.

  - name: 'warning-alerts'
    slack_configs:
      - api_url: '{{ .ExternalURL }}/slack/webhook'
        channel: '#alerts-warning'
        username: 'AlertManager'
        color: 'warning'
        title: '‚ö†Ô∏è WARNING: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Component:* {{ .Labels.component }}
          {{ end }}

  - name: 'business-alerts'
    email_configs:
      - to: 'data-science@company.com'
        subject: '[Business Alert] {{ .GroupLabels.alertname }}'
        body: |
          Business metric alert triggered:

          {{ range .Alerts }}
          {{ .Annotations.summary }}

          {{ .Annotations.description }}
          {{ end }}

    slack_configs:
      - api_url: '{{ .ExternalURL }}/slack/webhook'
        channel: '#business-metrics'
        username: 'AlertManager'
        color: 'good'
        title: 'üìä Business Alert: {{ .GroupLabels.alertname }}'

  - name: 'security-alerts'
    email_configs:
      - to: 'security@company.com'
        subject: 'üîê SECURITY ALERT: {{ .GroupLabels.alertname }}'
        body: |
          SECURITY ALERT - IMMEDIATE ATTENTION REQUIRED

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          Source: {{ .Labels.source }}
          {{ end }}

    slack_configs:
      - api_url: '{{ .ExternalURL }}/slack/webhook'
        channel: '#security-alerts'
        username: 'SecurityBot'
        color: 'danger'
        title: 'üîê SECURITY: {{ .GroupLabels.alertname }}'

inhibit_rules:
  # Inhibit warning alerts when critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['component']

  # Inhibit individual service alerts when general connectivity is down
  - source_match:
      alertname: 'APIHealthCheckFailing'
    target_match_re:
      alertname: '.*ProcessingRateDecline|.*ResponseTime.*'
    equal: ['instance']