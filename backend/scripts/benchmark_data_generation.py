#!/usr/bin/env python3
"""
æ±‚äººãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼ãƒ»æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Purpose: 10ä¸‡ä»¶ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®æ€§èƒ½æ¸¬å®šã€æœ€é©åŒ–ã€æ¤œè¨¼

Benchmark Targets:
- ç”Ÿæˆé€Ÿåº¦: 333 records/sec (ç›®æ¨™5åˆ†)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 512MBä»¥ä¸‹
- ãƒ‡ãƒ¼ã‚¿å“è³ª: åˆ¶ç´„é•å0ä»¶
- ã‚¨ãƒ©ãƒ¼ç‡: 0.1%ä»¥ä¸‹
"""

import os
import sys
import time
import psutil
import logging
import psycopg2
import pandas as pd
from typing import Dict, List, Tuple
from dataclasses import dataclass
import threading

# Add backend path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""
    test_record_counts: List[int] = None
    batch_sizes: List[int] = None
    parallel_workers: List[int] = None
    target_throughput: float = 333.0  # records/sec
    memory_limit_mb: int = 512
    output_dir: str = "/tmp/benchmark_results"

    def __post_init__(self):
        if self.test_record_counts is None:
            self.test_record_counts = [1000, 5000, 10000, 25000, 50000, 100000]
        if self.batch_sizes is None:
            self.batch_sizes = [1000, 2500, 5000, 7500, 10000]
        if self.parallel_workers is None:
            self.parallel_workers = [1, 2, 4, 6, 8]

@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    test_name: str
    record_count: int
    batch_size: int
    parallel_workers: int
    duration_seconds: float
    throughput_per_sec: float
    peak_memory_mb: float
    error_count: int
    success_rate: float
    constraint_violations: int

class PerformanceBenchmark:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.logger = self._setup_logger()
        self.results: List[BenchmarkResult] = []

        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ç”¨
        self.memory_monitor = None
        self.peak_memory = 0.0
        self.monitoring_active = False

    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
        os.makedirs(self.config.output_dir, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'{self.config.output_dir}/benchmark.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)

    def _get_db_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå–å¾—"""
        return psycopg2.connect(
            host=os.getenv('DB_HOST', 'localhost'),
            port=int(os.getenv('DB_PORT', '5432')),
            database=os.getenv('DB_NAME', 'mail_score_dev'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', 'password')
        )

    def _start_memory_monitoring(self):
        """ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹"""
        self.monitoring_active = True
        self.peak_memory = 0.0

        def monitor_memory():
            while self.monitoring_active:
                try:
                    process = psutil.Process()
                    memory_mb = process.memory_info().rss / 1024 / 1024
                    self.peak_memory = max(self.peak_memory, memory_mb)
                    time.sleep(0.1)
                except:
                    pass

        self.memory_monitor = threading.Thread(target=monitor_memory, daemon=True)
        self.memory_monitor.start()

    def _stop_memory_monitoring(self) -> float:
        """ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        if self.memory_monitor:
            self.memory_monitor.join(timeout=1.0)
        return self.peak_memory

    def _cleanup_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            with self._get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("DELETE FROM jobs WHERE endcl_cd LIKE 'TEST%'")
                    conn.commit()
        except Exception as e:
            self.logger.warning(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {str(e)}")

    def _validate_data_quality(self, record_count: int) -> Tuple[int, int]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
        try:
            with self._get_db_connection() as conn:
                with conn.cursor() as cur:
                    # åˆ¶ç´„é•åãƒã‚§ãƒƒã‚¯
                    cur.execute("""
                        SELECT COUNT(*) FROM jobs
                        WHERE endcl_cd LIKE 'TEST%'
                        AND (fee <= 500 OR fee > 5000)
                    """)
                    constraint_violations = cur.fetchone()[0]

                    # ç”Ÿæˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª
                    cur.execute("SELECT COUNT(*) FROM jobs WHERE endcl_cd LIKE 'TEST%'")
                    actual_count = cur.fetchone()[0]

                    return constraint_violations, actual_count

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return -1, -1

    def run_full_benchmark(self):
        """ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        self.logger.info("ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")

        try:
            # ç°¡æ˜“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = self._run_generation_test(10000, 5000, 4)
            self.results.append(result)

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self.generate_performance_report()

            self.logger.info("âœ… ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")

        except Exception as e:
            self.logger.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_test_data()

    def _run_generation_test(self, record_count: int, batch_size: int, workers: int) -> BenchmarkResult:
        """ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        test_name = f"records_{record_count}_batch_{batch_size}_workers_{workers}"
        self.logger.info(f"ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")

        # ãƒ†ã‚¹ãƒˆå‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._cleanup_test_data()

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        self._start_memory_monitoring()

        start_time = time.time()
        error_count = 0

        try:
            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰
            self._mock_data_generation(record_count, batch_size)

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            error_count += 1

        # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬
        duration = time.time() - start_time
        throughput = record_count / duration if duration > 0 else 0

        # ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢
        peak_memory = self._stop_memory_monitoring()

        # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
        constraint_violations, actual_count = self._validate_data_quality(record_count)
        success_rate = (actual_count / record_count * 100) if record_count > 0 else 0

        # çµæœè¨˜éŒ²
        result = BenchmarkResult(
            test_name=test_name,
            record_count=record_count,
            batch_size=batch_size,
            parallel_workers=workers,
            duration_seconds=duration,
            throughput_per_sec=throughput,
            peak_memory_mb=peak_memory,
            error_count=error_count,
            success_rate=success_rate,
            constraint_violations=constraint_violations
        )

        self.logger.info(
            f"ãƒ†ã‚¹ãƒˆå®Œäº†: {test_name} | "
            f"æ™‚é–“: {duration:.1f}s | "
            f"é€Ÿåº¦: {throughput:.0f} rec/s | "
            f"ãƒ¡ãƒ¢ãƒª: {peak_memory:.1f}MB | "
            f"æˆåŠŸç‡: {success_rate:.1f}%"
        )

        return result

    def _mock_data_generation(self, record_count: int, batch_size: int):
        """ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
        import random

        batch_count = (record_count + batch_size - 1) // batch_size

        with self._get_db_connection() as conn:
            with conn.cursor() as cur:
                for batch_num in range(batch_count):
                    start_id = batch_num * batch_size + 1
                    current_batch_size = min(batch_size, record_count - batch_num * batch_size)

                    # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                    values = []
                    for i in range(current_batch_size):
                        job_id = start_id + i
                        values.append((
                            job_id,
                            f'TEST{job_id:06d}',  # TEST prefix
                            f'ãƒ†ã‚¹ãƒˆä¼šç¤¾{job_id}',
                            f'APP{job_id:08d}',
                            f'ãƒ†ã‚¹ãƒˆæ±‚äºº{job_id}',
                            '13',  # æ±äº¬éƒ½
                            '13101',  # åƒä»£ç”°åŒº
                            'ãƒ†ã‚¹ãƒˆé§…',
                            'ãƒ†ã‚¹ãƒˆä½æ‰€',
                            'hourly',
                            random.randint(1000, 2000),
                            random.randint(2000, 3000),
                            random.randint(501, 5000),  # feeåˆ¶ç´„æº€ãŸã™
                            '8æ™‚é–“',
                            'é€±5æ—¥',
                            100,  # occupation_cd1
                            1,    # employment_type_cd
                            '{D01,N01}',  # feature_codes
                            'ãƒ†ã‚¹ãƒˆèª¬æ˜',
                            'ãƒ†ã‚¹ãƒˆå¾…é‡'
                        ))

                    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
                    for value in values:
                        cur.execute("""
                            INSERT INTO jobs (
                                job_id, endcl_cd, company_name, application_id, application_name,
                                pref_cd, city_cd, station_name_eki, address,
                                salary_type, min_salary, max_salary, fee, hours, work_days,
                                occupation_cd1, employment_type_cd, feature_codes,
                                description, benefits
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, value)

                conn.commit()

    def generate_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        df = pd.DataFrame([
            {
                'test_name': r.test_name,
                'record_count': r.record_count,
                'batch_size': r.batch_size,
                'parallel_workers': r.parallel_workers,
                'duration_seconds': r.duration_seconds,
                'throughput_per_sec': r.throughput_per_sec,
                'peak_memory_mb': r.peak_memory_mb,
                'success_rate': r.success_rate,
                'constraint_violations': r.constraint_violations
            }
            for r in self.results
        ])

        # CSVä¿å­˜
        report_path = f"{self.config.output_dir}/benchmark_results.csv"
        df.to_csv(report_path, index=False)

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
        self._generate_summary_report(df)

        self.logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {self.config.output_dir}")

    def _generate_summary_report(self, df: pd.DataFrame):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "# æ±‚äººãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ",
            f"## å®Ÿè¡Œæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼",
            ""
        ]

        if not df.empty:
            result = df.iloc[0]
            target_time = result['record_count'] / self.config.target_throughput

            report_lines.extend([
                "### ğŸ¯ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                f"- **ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: {result['record_count']:,}ä»¶",
                f"- **å®Ÿè¡Œæ™‚é–“**: {result['duration_seconds']:.1f}ç§’",
                f"- **ç›®æ¨™æ™‚é–“**: {target_time:.1f}ç§’",
                f"- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {result['throughput_per_sec']:.0f} records/sec",
                f"- **ç›®æ¨™é”æˆ**: {'âœ…' if result['duration_seconds'] <= target_time else 'âŒ'}",
                f"- **ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª**: {result['peak_memory_mb']:.1f}MB",
                f"- **ãƒ¡ãƒ¢ãƒªåˆ¶é™å†…**: {'âœ…' if result['peak_memory_mb'] <= self.config.memory_limit_mb else 'âŒ'}",
                f"- **æˆåŠŸç‡**: {result['success_rate']:.1f}%",
                f"- **åˆ¶ç´„é•å**: {result['constraint_violations']}ä»¶",
                ""
            ])

        # æ¨å¥¨è¨­å®š
        report_lines.extend([
            "## ğŸš€ æ¨å¥¨è¨­å®š",
            "",
            "```python",
            "RECOMMENDED_CONFIG = {",
            "    'batch_size': 5000,",
            "    'parallel_workers': 4,",
            "    'memory_limit_mb': 512,",
            "    'use_copy_from': True,",
            "    'disable_triggers': True,",
            "}",
            "```",
            ""
        ])

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(f"{self.config.output_dir}/benchmark_summary.md", 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    config = BenchmarkConfig()
    benchmark = PerformanceBenchmark(config)
    benchmark.run_full_benchmark()

if __name__ == "__main__":
    main()