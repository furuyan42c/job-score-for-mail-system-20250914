#!/usr/bin/env python3
"""
æ±‚äººã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

10ä¸‡ä»¶ã®æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆã—ã¦PostgreSQLã«æŠ•å…¥
å‡¦ç†æ™‚é–“ç›®æ¨™: 5åˆ†ä»¥å†…
"""

import asyncio
import random
import logging
import time
import json
import argparse
from typing import List, Dict, Any, Generator, Optional
from datetime import datetime, timedelta
from decimal import Decimal
import os
import sys
from io import StringIO
from contextlib import asynccontextmanager
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

from concurrent.futures import ThreadPoolExecutor

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncpg

try:
    from faker import Faker
    FAKER_AVAILABLE = True
except ImportError:
    FAKER_AVAILABLE = False
    Faker = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

def setup_logging(level: str = 'INFO', log_file: Optional[str] = None) -> logging.Logger:
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    log_level = getattr(logging, level.upper())

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )

    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logger = logging.getLogger(__name__)
    logger.setLevel(log_level)

    # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
    logger.handlers.clear()

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger

# ãƒ­ã‚°è¨­å®šï¼ˆå¾Œã§åˆæœŸåŒ–ï¼‰
logger = setup_logging()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šURL
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:postgres@localhost:5432/job_matching"
).replace('+asyncpg', '')  # asyncpgã¯+ã‚’é™¤ã

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
CONNECTION_TIMEOUT = 30
QUERY_TIMEOUT = 60
MAX_RETRIES = 3
RETRY_DELAY = 1.0

# =============================================================================
# è¨­å®šå®šæ•°
# =============================================================================

# ç”Ÿæˆè¨­å®š
TOTAL_JOBS = 100000  # ç”Ÿæˆã™ã‚‹æ±‚äººæ•°
BATCH_SIZE = 5000    # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
PARALLEL_WORKERS = min(4, psutil.cpu_count() if PSUTIL_AVAILABLE else 4)  # ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆCPUæ•°ã«å¿œã˜ã¦èª¿æ•´ï¼‰
PROGRESS_REPORT_INTERVAL = 5  # é€²æ—å ±å‘Šé–“éš”ï¼ˆãƒãƒƒãƒæ•°ï¼‰
MEMORY_THRESHOLD_MB = 500  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡é–¾å€¤ï¼ˆMBï¼‰

# ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒè¨­å®š
MIN_FEE = 501        # feeæœ€å°å€¤ï¼ˆ> 500ï¼‰
MAX_FEE = 50000      # feeæœ€å¤§å€¤

# åœ°åŸŸåˆ†å¸ƒï¼ˆäººå£æ¯”ç‡ãƒ™ãƒ¼ã‚¹ï¼‰
PREFECTURE_WEIGHTS = {
    '13': 0.15,  # æ±äº¬éƒ½ 15%
    '27': 0.08,  # å¤§é˜ªåºœ 8%
    '14': 0.07,  # ç¥å¥ˆå·çœŒ 7%
    '23': 0.06,  # æ„›çŸ¥çœŒ 6%
    '11': 0.05,  # åŸ¼ç‰çœŒ 5%
    '12': 0.05,  # åƒè‘‰çœŒ 5%
    '40': 0.04,  # ç¦å²¡çœŒ 4%
    '28': 0.04,  # å…µåº«çœŒ 4%
    '01': 0.03,  # åŒ—æµ·é“ 3%
    # ãã®ä»–ã¯å‡ç­‰åˆ†å¸ƒ
}

# è·ç¨®åˆ†å¸ƒï¼ˆãƒã‚¤ãƒˆå¸‚å ´ã®å®Ÿæ…‹ãƒ™ãƒ¼ã‚¹ï¼‰
OCCUPATION_WEIGHTS = {
    1: 0.25,   # é£²é£Ÿãƒ»ãƒ•ãƒ¼ãƒ‰ 25%
    2: 0.20,   # ã‚³ãƒ³ãƒ“ãƒ‹ãƒ»ã‚¹ãƒ¼ãƒ‘ãƒ¼ 20%
    3: 0.08,   # ã‚¢ãƒ‘ãƒ¬ãƒ«ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ 8%
    4: 0.12,   # è»½ä½œæ¥­ãƒ»ç‰©æµ 12%
    5: 0.08,   # å·¥å ´ãƒ»è£½é€  8%
    6: 0.07,   # äº‹å‹™ãƒ»ãƒ‡ãƒ¼ã‚¿å…¥åŠ› 7%
    7: 0.05,   # ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼ 5%
    8: 0.05,   # å–¶æ¥­ãƒ»è²©å£² 5%
    9: 0.03,   # ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒƒãƒ• 3%
    10: 0.03,  # æ•™è‚²ãƒ»å¡¾è¬›å¸« 3%
    11: 0.02,  # ä»‹è­·ãƒ»çœ‹è­·åŠ©æ‰‹ 2%
    12: 0.02,  # æ¸…æƒãƒ»ãƒ“ãƒ«ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ 2%
}

# é›‡ç”¨å½¢æ…‹åˆ†å¸ƒï¼ˆãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã®ã¿ï¼‰
EMPLOYMENT_TYPE_WEIGHTS = {
    1: 0.60,  # ã‚¢ãƒ«ãƒã‚¤ãƒˆ 60%
    3: 0.30,  # ãƒ‘ãƒ¼ãƒˆ 30%
    8: 0.10,  # æ—¥é›‡ã„ 10%
}

# ä¼æ¥­åãƒ‘ã‚¿ãƒ¼ãƒ³
COMPANY_PATTERNS = [
    "æ ªå¼ä¼šç¤¾{}", "{}", "{}å•†åº—", "{}ã‚°ãƒ«ãƒ¼ãƒ—",
    "{}ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹", "{}ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼", "{}ã‚µãƒ¼ãƒ“ã‚¹",
    "æœ‰é™ä¼šç¤¾{}", "{}ã‚·ã‚¹ãƒ†ãƒ ", "{}ãƒ•ãƒ¼ã‚º",
    "{}ã‚¹ãƒˆã‚¢", "{}ãƒãƒ¼ãƒˆ", "{}ã‚»ãƒ³ã‚¿ãƒ¼"
]

# æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
JOB_TITLE_TEMPLATES = {
    1: ["{}ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ›ãƒ¼ãƒ«ãƒ»ã‚­ãƒƒãƒãƒ³ã‚¹ã‚¿ãƒƒãƒ•", "é£²é£Ÿåº—ã‚¹ã‚¿ãƒƒãƒ•", "èª¿ç†è£œåŠ©"],
    2: ["ã‚³ãƒ³ãƒ“ãƒ‹ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ¬ã‚¸ã‚¹ã‚¿ãƒƒãƒ•", "å“å‡ºã—ã‚¹ã‚¿ãƒƒãƒ•", "{}åº—å“¡"],
    3: ["ã‚¢ãƒ‘ãƒ¬ãƒ«è²©å£²å“¡", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³è²©å£²", "ã‚·ãƒ§ãƒƒãƒ—ã‚¹ã‚¿ãƒƒãƒ•"],
    4: ["å€‰åº«ä½œæ¥­å“¡", "ä»•åˆ†ã‘ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ”ãƒƒã‚­ãƒ³ã‚°ä½œæ¥­", "æ¢±åŒ…ã‚¹ã‚¿ãƒƒãƒ•"],
    5: ["å·¥å ´ä½œæ¥­å“¡", "è£½é€ ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ©ã‚¤ãƒ³ä½œæ¥­", "çµ„ç«‹ä½œæ¥­å“¡"],
    6: ["ãƒ‡ãƒ¼ã‚¿å…¥åŠ›", "ä¸€èˆ¬äº‹å‹™", "äº‹å‹™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "PCã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼"],
    7: ["ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼", "é›»è©±å¯¾å¿œ", "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ", "ãƒ†ãƒ¬ã‚ªãƒš"],
    8: ["å–¶æ¥­ã‚¹ã‚¿ãƒƒãƒ•", "è²©å£²å“¡", "ã‚»ãƒ¼ãƒ«ã‚¹ã‚¹ã‚¿ãƒƒãƒ•"],
    9: ["ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒƒãƒ•", "è¨­å–¶ã‚¹ã‚¿ãƒƒãƒ•", "é‹å–¶ã‚¹ã‚¿ãƒƒãƒ•"],
    10: ["å¡¾è¬›å¸«", "å®¶åº­æ•™å¸«", "æ•™è‚²ã‚¹ã‚¿ãƒƒãƒ•"],
    11: ["ä»‹è­·ã‚¹ã‚¿ãƒƒãƒ•", "çœ‹è­·åŠ©æ‰‹", "ãƒ˜ãƒ«ãƒ‘ãƒ¼"],
    12: ["æ¸…æƒã‚¹ã‚¿ãƒƒãƒ•", "ãƒ“ãƒ«æ¸…æƒ", "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¹ã‚¿ãƒƒãƒ•"],
}

# =============================================================================
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–¢æ•°
# =============================================================================

class JobDataGenerator:
    """æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    def __init__(self, progress_callback=None):
        if not FAKER_AVAILABLE:
            raise ImportError("Faker library is required for data generation. Install with: pip install faker")
        self.faker = Faker('ja_JP')
        self.master_data = {}
        self.company_names_cache = set()
        self.progress_callback = progress_callback
        self._generation_stats = {
            'total_generated': 0,
            'start_time': None,
            'last_progress_time': None
        }

    async def load_master_data(self, conn: asyncpg.Connection):
        """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # éƒ½é“åºœçœŒ
        rows = await conn.fetch("SELECT code, name FROM prefecture_master")
        self.master_data['prefectures'] = {row['code']: row['name'] for row in rows}

        # å¸‚åŒºç”ºæ‘
        rows = await conn.fetch("SELECT code, pref_cd, name FROM city_master")
        self.master_data['cities'] = [(row['code'], row['pref_cd']) for row in rows]

        # è·ç¨®
        rows = await conn.fetch("SELECT code FROM occupation_master WHERE is_active = TRUE")
        self.master_data['occupations'] = [row['code'] for row in rows]

        # é›‡ç”¨å½¢æ…‹
        rows = await conn.fetch("SELECT code FROM employment_type_master WHERE is_valid_for_matching = TRUE")
        self.master_data['employment_types'] = [row['code'] for row in rows]

        # ç‰¹å¾´
        rows = await conn.fetch("SELECT feature_code FROM feature_master WHERE is_active = TRUE")
        self.master_data['features'] = [row['feature_code'] for row in rows]

        logger.info(f"ğŸ“š ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {len(self.master_data['prefectures'])}éƒ½é“åºœçœŒ")

    def generate_company_name(self) -> str:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªä¼æ¥­åã‚’ç”Ÿæˆ"""
        attempts = 0
        while attempts < 100:
            base_name = self.faker.company()
            pattern = random.choice(COMPANY_PATTERNS)
            company_name = pattern.format(base_name.replace('æ ªå¼ä¼šç¤¾', '').replace('æœ‰é™ä¼šç¤¾', ''))

            if company_name not in self.company_names_cache:
                self.company_names_cache.add(company_name)
                return company_name
            attempts += 1

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä»˜åŠ 
        unique_name = f"{base_name}_{int(time.time() * 1000) % 100000}"
        self.company_names_cache.add(unique_name)
        return unique_name

    def generate_endcl_cd(self, index: int) -> str:
        """ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆï¼ˆCOMPANY_XXXXXå½¢å¼ï¼‰"""
        return f"COMPANY_{index:06d}"

    def select_prefecture(self) -> str:
        """é‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦éƒ½é“åºœçœŒã‚’é¸æŠ"""
        if random.random() < sum(PREFECTURE_WEIGHTS.values()):
            # é‡ã¿ä»˜ã‘é¸æŠ
            prefs = list(PREFECTURE_WEIGHTS.keys())
            weights = list(PREFECTURE_WEIGHTS.values())
            if NUMPY_AVAILABLE:
                return np.random.choice(prefs, p=np.array(weights)/sum(weights))
            else:
                # NumPyãŒãªã„å ´åˆã¯æ‰‹å‹•ã§é‡ã¿ä»˜ã‘é¸æŠ
                total_weight = sum(weights)
                rand_val = random.random() * total_weight
                cumulative = 0
                for pref, weight in zip(prefs, weights):
                    cumulative += weight
                    if rand_val <= cumulative:
                        return pref
                return prefs[-1]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        else:
            # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
            return random.choice(list(self.master_data['prefectures'].keys()))

    def select_city(self, pref_cd: str) -> str:
        """éƒ½é“åºœçœŒã«å¿œã˜ãŸå¸‚åŒºç”ºæ‘ã‚’é¸æŠ"""
        cities = [c[0] for c in self.master_data['cities'] if c[1] == pref_cd]
        if cities:
            return random.choice(cities)
        return f"{pref_cd}000"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def select_occupation(self) -> int:
        """é‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦è·ç¨®ã‚’é¸æŠ"""
        occs = list(OCCUPATION_WEIGHTS.keys())
        weights = list(OCCUPATION_WEIGHTS.values())
        if NUMPY_AVAILABLE:
            return np.random.choice(occs, p=weights)
        else:
            # NumPyãŒãªã„å ´åˆã¯æ‰‹å‹•ã§é‡ã¿ä»˜ã‘é¸æŠ
            rand_val = random.random()
            cumulative = 0
            for occ, weight in zip(occs, weights):
                cumulative += weight
                if rand_val <= cumulative:
                    return occ
            return occs[-1]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def select_employment_type(self) -> int:
        """é‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦é›‡ç”¨å½¢æ…‹ã‚’é¸æŠ"""
        types = list(EMPLOYMENT_TYPE_WEIGHTS.keys())
        weights = list(EMPLOYMENT_TYPE_WEIGHTS.values())
        if NUMPY_AVAILABLE:
            return np.random.choice(types, p=weights)
        else:
            # NumPyãŒãªã„å ´åˆã¯æ‰‹å‹•ã§é‡ã¿ä»˜ã‘é¸æŠ
            rand_val = random.random()
            cumulative = 0
            for emp_type, weight in zip(types, weights):
                cumulative += weight
                if rand_val <= cumulative:
                    return emp_type
            return types[-1]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def generate_salary(self, occupation: int) -> tuple:
        """è·ç¨®ã«å¿œã˜ãŸçµ¦ä¸ã‚’ç”Ÿæˆ"""
        # è·ç¨®åˆ¥ã®æ™‚çµ¦ãƒ¬ãƒ³ã‚¸
        salary_ranges = {
            1: (950, 1500),    # é£²é£Ÿ
            2: (900, 1300),    # ã‚³ãƒ³ãƒ“ãƒ‹
            3: (1000, 1600),   # ã‚¢ãƒ‘ãƒ¬ãƒ«
            4: (1000, 1800),   # è»½ä½œæ¥­
            5: (1100, 2000),   # å·¥å ´
            6: (1000, 1500),   # äº‹å‹™
            7: (1200, 1800),   # ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼
            8: (1100, 2000),   # å–¶æ¥­
            9: (1000, 1500),   # ã‚¤ãƒ™ãƒ³ãƒˆ
            10: (1500, 3000),  # å¡¾è¬›å¸«
            11: (1100, 1600),  # ä»‹è­·
            12: (950, 1400),   # æ¸…æƒ
        }

        min_range, max_range = salary_ranges.get(occupation, (1000, 1500))
        min_salary = random.randint(min_range, max_range - 200)
        max_salary = min_salary + random.randint(100, 500)

        return min_salary, max_salary

    def generate_features(self, salary_min: int) -> List[str]:
        """æ±‚äººã®ç‰¹å¾´ã‚’ç”Ÿæˆ"""
        features = []
        num_features = random.randint(3, 8)

        # é«˜æ™‚çµ¦ã®å ´åˆã¯å¿…ãšè¿½åŠ 
        if salary_min >= 1200:
            features.append('F03')  # é«˜åå…¥
            features.append('F04')  # æ™‚çµ¦1200å††ä»¥ä¸Š

        # ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´ã‚’è¿½åŠ 
        available_features = [f for f in self.master_data['features'] if f not in features]
        features.extend(random.sample(available_features, min(num_features - len(features), len(available_features))))

        return features[:num_features]

    def generate_job_title(self, occupation: int, company_name: str) -> str:
        """è·ç¨®ã«å¿œã˜ãŸæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        templates = JOB_TITLE_TEMPLATES.get(occupation, ["ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†"])
        template = random.choice(templates)

        if '{}' in template:
            return template.format(company_name.split('æ ªå¼ä¼šç¤¾')[-1].split('æœ‰é™ä¼šç¤¾')[-1][:10])
        return template

    def generate_catch_copy(self, features: List[str]) -> str:
        """ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆ"""
        copies = [
            "æœªçµŒé¨“OKï¼å……å®Ÿã®ç ”ä¿®ã§ã—ã£ã‹ã‚Šã‚µãƒãƒ¼ãƒˆ",
            "é«˜æ™‚çµ¦ï¼ãŒã£ã¤ã‚Šç¨¼ã’ã¾ã™",
            "é€±1æ—¥ã€œOKï¼è‡ªåˆ†ã®ãƒšãƒ¼ã‚¹ã§åƒã‘ã‚‹",
            "é§…ãƒã‚«ã§é€šå‹¤ä¾¿åˆ©ï¼",
            "ç¤¾å“¡ç™»ç”¨ã‚ã‚Šï¼ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒƒãƒ—å¯èƒ½",
            "æ—¥æ‰•ã„OKï¼ã™ãã«ãŠé‡‘ãŒå¿…è¦ãªæ–¹ã‚‚å®‰å¿ƒ",
            "ã‚·ãƒ•ãƒˆè‡ªç”±ï¼ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚‚å……å®Ÿ",
            "ã¾ã‹ãªã„ä»˜ãï¼ç¾å‘³ã—ã„é£Ÿäº‹ã‚‚æ¥½ã—ã‚ã‚‹",
            "å‹é”ã¨ä¸€ç·’ã«å¿œå‹ŸOKï¼",
            "çŸ­æœŸOKï¼1ãƒ¶æœˆã‹ã‚‰åƒã‘ã‚‹"
        ]

        # ç‰¹å¾´ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’é¸æŠ
        if 'F01' in features:
            return "æ—¥æ‰•ã„OKï¼ã™ãã«ãŠé‡‘ãŒå¿…è¦ãªæ–¹å¿…è¦‹ï¼"
        elif 'F03' in features:
            return "é«˜æ™‚çµ¦ï¼æœˆå25ä¸‡å††ä»¥ä¸Šå¯èƒ½ï¼"
        elif 'F10' in features:
            return "æœªçµŒé¨“å¤§æ­“è¿ï¼ä¸å¯§ãªç ”ä¿®ã§ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—ï¼"

        return random.choice(copies)

    def generate_job_description(self, occupation: int, company_name: str) -> str:
        """æ±‚äººèª¬æ˜æ–‡ã‚’ç”Ÿæˆ"""
        base_descriptions = {
            1: f"{company_name}ã§ä¸€ç·’ã«åƒãã¾ã›ã‚“ã‹ï¼Ÿæ¥å®¢ã‚„èª¿ç†è£œåŠ©ãªã©ã€ã‚„ã‚ŠãŒã„ã®ã‚ã‚‹ãŠä»•äº‹ã§ã™ã€‚",
            2: f"åœ°åŸŸã«æ„›ã•ã‚Œã‚‹{company_name}ã§ã®ãŠä»•äº‹ã€‚ãƒ¬ã‚¸ãƒ»å“å‡ºã—ãƒ»æ¸…æƒãªã©å¹…åºƒã„æ¥­å‹™ãŒã‚ã‚Šã¾ã™ã€‚",
            3: f"ãŠã—ã‚ƒã‚Œãª{company_name}ã§è²©å£²ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†ï¼ãŠå®¢æ§˜ã«ç´ æ•µãªå•†å“ã‚’ã”ææ¡ˆã™ã‚‹ãŠä»•äº‹ã§ã™ã€‚",
            4: f"{company_name}ã®ç‰©æµã‚»ãƒ³ã‚¿ãƒ¼ã§ã®è»½ä½œæ¥­ã€‚é‡ã„ã‚‚ã®ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚é»™ã€…ã¨ä½œæ¥­ã—ãŸã„æ–¹ã«æœ€é©ï¼",
            5: f"{company_name}ã®å·¥å ´ã§ã®ãƒ©ã‚¤ãƒ³ä½œæ¥­ã€‚ç°¡å˜ãªçµ„ç«‹ã‚„æ¤œæŸ»ã®ãŠä»•äº‹ã§ã™ã€‚",
            6: f"{company_name}ã§ã®äº‹å‹™ä½œæ¥­ã€‚PCã‚’ä½¿ã£ãŸç°¡å˜ãªãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãŒãƒ¡ã‚¤ãƒ³ã§ã™ã€‚",
            7: f"{company_name}ã®ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼ã§ãŠå®¢æ§˜å¯¾å¿œã€‚ç ”ä¿®å……å®Ÿã§æœªçµŒé¨“ã§ã‚‚å®‰å¿ƒï¼",
            8: f"{company_name}ã§å–¶æ¥­ãƒ»è²©å£²ã®ãŠä»•äº‹ã€‚äººã¨è©±ã™ã“ã¨ãŒå¥½ããªæ–¹å¤§æ­“è¿ï¼",
            9: f"å„ç¨®ã‚¤ãƒ™ãƒ³ãƒˆã§ã®{company_name}ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†ã€‚æ¥½ã—ãåƒã‘ã‚‹ç’°å¢ƒã§ã™ï¼",
            10: f"{company_name}ã§è¬›å¸«å‹Ÿé›†ã€‚ã‚ãªãŸã®çŸ¥è­˜ã‚’æ´»ã‹ã—ã¦ç”Ÿå¾’ã®æˆé•·ã‚’ã‚µãƒãƒ¼ãƒˆï¼",
            11: f"{company_name}ã§ã®ä»‹è­·ãƒ»çœ‹è­·è£œåŠ©ã€‚ã‚„ã‚ŠãŒã„ã®ã‚ã‚‹ç¦ç¥‰ã®ãŠä»•äº‹ã§ã™ã€‚",
            12: f"{company_name}ã§ã®æ¸…æƒæ¥­å‹™ã€‚ãã‚Œã„ãªç’°å¢ƒã¥ãã‚Šã®ãŠæ‰‹ä¼ã„ï¼",
        }

        base = base_descriptions.get(occupation, f"{company_name}ã§ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†ä¸­ï¼")
        additions = [
            "\n\nã€ä»•äº‹å†…å®¹ã€‘\nå…·ä½“çš„ãªæ¥­å‹™å†…å®¹ã¯é¢æ¥æ™‚ã«è©³ã—ãã”èª¬æ˜ã—ã¾ã™ã€‚",
            "\n\nã€ã“ã‚“ãªæ–¹æ­“è¿ã€‘\nãƒ»æ˜ã‚‹ãå…ƒæ°—ãªæ–¹\nãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¥½ããªæ–¹\nãƒ»è²¬ä»»æ„Ÿã®ã‚ã‚‹æ–¹",
            "\n\nã€å¾…é‡ã€‘\näº¤é€šè²»æ”¯çµ¦ã€åˆ¶æœè²¸ä¸ã€ç¤¾ä¼šä¿é™ºå®Œå‚™ï¼ˆæ¡ä»¶ã‚ã‚Šï¼‰",
            "\n\nã€å¿œå‹Ÿã€‘\nã¾ãšã¯ãŠæ°—è»½ã«ã”å¿œå‹Ÿãã ã•ã„ï¼å±¥æ­´æ›¸ä¸è¦ã®é¢æ¥ã‚‚å¯èƒ½ã§ã™ã€‚"
        ]

        return base + random.choice(additions)

    def generate_work_hours(self, employment_type: int) -> str:
        """å‹¤å‹™æ™‚é–“ã‚’ç”Ÿæˆ"""
        if employment_type == 1:  # ã‚¢ãƒ«ãƒã‚¤ãƒˆ
            patterns = [
                "9:00ï½18:00", "10:00ï½19:00", "11:00ï½20:00",
                "17:00ï½22:00", "18:00ï½23:00", "ã‚·ãƒ•ãƒˆåˆ¶"
            ]
        elif employment_type == 3:  # ãƒ‘ãƒ¼ãƒˆ
            patterns = [
                "9:00ï½14:00", "10:00ï½15:00", "9:00ï½13:00",
                "13:00ï½17:00", "14:00ï½18:00"
            ]
        else:  # æ—¥é›‡ã„
            patterns = [
                "8:00ï½17:00", "9:00ï½18:00", "æ—¥ã«ã‚ˆã£ã¦ç•°ãªã‚‹"
            ]

        return random.choice(patterns)

    def generate_single_job(self, job_id: int, company_index: int) -> Dict[str, Any]:
        """1ä»¶ã®æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        # åŸºæœ¬æƒ…å ±
        endcl_cd = self.generate_endcl_cd(company_index)
        company_name = self.generate_company_name()
        pref_cd = self.select_prefecture()
        city_cd = self.select_city(pref_cd)
        occupation = self.select_occupation()
        employment_type = self.select_employment_type()

        # çµ¦ä¸
        min_salary, max_salary = self.generate_salary(occupation)

        # ç‰¹å¾´
        features = self.generate_features(min_salary)

        # feeï¼ˆå¿œå‹Ÿä¿ƒé€²è²»ç”¨ï¼‰
        # é«˜æ™‚çµ¦ã‚„äººæ°—è·ç¨®ã¯é«˜ã‚ã®fee
        base_fee = random.randint(MIN_FEE, 5000)
        if min_salary >= 1500:
            base_fee += random.randint(1000, 3000)
        if occupation in [1, 2, 4]:  # äººæ°—è·ç¨®
            base_fee += random.randint(500, 2000)

        # æ²è¼‰æ—¥ï¼ˆéå»30æ—¥ã€œæœªæ¥7æ—¥ï¼‰
        posting_date = datetime.now() + timedelta(days=random.randint(-30, 7))

        # ãã®ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        job_data = {
            'job_id': job_id,
            'endcl_cd': endcl_cd,
            'company_name': company_name,
            'title': self.generate_job_title(occupation, company_name),
            'catch_copy': self.generate_catch_copy(features),
            'job_description': self.generate_job_description(occupation, company_name),
            'pref_cd': pref_cd,
            'city_cd': city_cd,
            'station_name': f"{self.faker.city()}é§…",
            'occupation_cd1': occupation,
            'occupation_cd2': None,
            'occupation_cd3': None,
            'employment_type_cd': employment_type,
            'min_salary': min_salary,
            'max_salary': max_salary,
            'salary_text': f"æ™‚çµ¦{min_salary}å††ã€œ{max_salary}å††",
            'work_hours': self.generate_work_hours(employment_type),
            'work_days_min': random.randint(1, 3),
            'work_days_max': random.randint(4, 7),
            'feature_codes': features,
            'fee': base_fee,
            'posting_date': posting_date.date(),
            'valid_until': (posting_date + timedelta(days=60)).date(),
            'is_active': True,
            'view_count': random.randint(0, 10000),
            'application_count': random.randint(0, 100),
            'created_at': datetime.now(),
            'updated_at': datetime.now()
        }

        return job_data

    def generate_batch(self, start_id: int, batch_size: int) -> Generator[Dict[str, Any], None, None]:
        """ãƒãƒƒãƒå˜ä½ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰"""
        if self._generation_stats['start_time'] is None:
            self._generation_stats['start_time'] = time.time()

        company_index_start = start_id // 10  # 1ä¼æ¥­ã‚ãŸã‚Šå¹³å‡10æ±‚äºº

        for i in range(batch_size):
            job_id = start_id + i
            company_index = company_index_start + (i // 10)

            try:
                job_data = self.generate_single_job(job_id, company_index)
                self._generation_stats['total_generated'] += 1

                # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if self.progress_callback and i % 100 == 0:
                    self.progress_callback('generating', {
                        'current': self._generation_stats['total_generated'],
                        'batch_progress': i + 1,
                        'batch_size': batch_size
                    })

                yield job_data

            except Exception as e:
                logger.error(f"æ±‚äººID {job_id} ã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚ç¶šè¡Œï¼ˆãƒ­ã‚°ã¯æ®‹ã™ï¼‰
                continue

    def get_generation_stats(self) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        elapsed = time.time() - self._generation_stats['start_time'] if self._generation_stats['start_time'] else 0
        speed = self._generation_stats['total_generated'] / elapsed if elapsed > 0 else 0

        memory_mb = 0
        if PSUTIL_AVAILABLE:
            try:
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
            except:
                memory_mb = 0

        return {
            'total_generated': self._generation_stats['total_generated'],
            'elapsed_time': elapsed,
            'generation_speed': speed,
            'memory_usage_mb': memory_mb
        }

# =============================================================================
# ãƒ‡ãƒ¼ã‚¿æŠ•å…¥é–¢æ•°
# =============================================================================

async def bulk_insert_jobs(conn: asyncpg.Connection, jobs: List[Dict[str, Any]], dry_run: bool = False) -> int:
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã§æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥"""
    if dry_run:
        logger.debug(f"DRY RUN: {len(jobs)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        await asyncio.sleep(0.01)  # å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        return len(jobs)

    try:
        # COPYç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        copy_data = []

        for job in jobs:
            # PostgreSQLé…åˆ—å½¢å¼ã«å¤‰æ›
            feature_codes_str = '{' + ','.join(job['feature_codes']) + '}'

            row = (
                job['job_id'],
                job['endcl_cd'],
                job['company_name'],
                job['title'],
                job['catch_copy'],
                job['job_description'],
                job['pref_cd'],
                job['city_cd'],
                job['station_name'],
                job['occupation_cd1'],
                job['occupation_cd2'],
                job['occupation_cd3'],
                job['employment_type_cd'],
                job['min_salary'],
                job['max_salary'],
                job['salary_text'],
                job['work_hours'],
                job['work_days_min'],
                job['work_days_max'],
                feature_codes_str,
                job['fee'],
                job['posting_date'],
                job['valid_until'],
                job['is_active'],
                job['view_count'],
                job['application_count'],
                job['created_at'],
                job['updated_at']
            )
            copy_data.append(row)

        # COPYæ–‡ã§ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
        await conn.copy_records_to_table(
            'jobs',
            records=copy_data,
            columns=[
                'job_id', 'endcl_cd', 'company_name', 'title', 'catch_copy',
                'job_description', 'pref_cd', 'city_cd', 'station_name',
                'occupation_cd1', 'occupation_cd2', 'occupation_cd3',
                'employment_type_cd', 'min_salary', 'max_salary', 'salary_text',
                'work_hours', 'work_days_min', 'work_days_max', 'feature_codes',
                'fee', 'posting_date', 'valid_until', 'is_active',
                'view_count', 'application_count', 'created_at', 'updated_at'
            ]
        )

        return len(jobs)

    except Exception as e:
        logger.error(f"ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"å¤±æ•—ã—ãŸãƒãƒƒãƒã‚µã‚¤ã‚º: {len(jobs)}ä»¶")
        raise

class ProgressTracker:
    """é€²æ—è¿½è·¡ã‚¯ãƒ©ã‚¹"""
    def __init__(self, total_records: int):
        self.total_records = total_records
        self.start_time = time.time()
        self.last_report_time = self.start_time
        self.processed_records = 0
        self.last_processed = 0

    def update(self, processed: int):
        """é€²æ—æ›´æ–°"""
        self.processed_records = processed

    def should_report(self, batch_interval: int) -> bool:
        """é€²æ—å ±å‘ŠãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯"""
        current_time = time.time()
        return (current_time - self.last_report_time) >= batch_interval

    def get_progress_info(self) -> Dict[str, Any]:
        """é€²æ—æƒ…å ±ã‚’å–å¾—"""
        current_time = time.time()
        elapsed_total = current_time - self.start_time
        elapsed_since_last = current_time - self.last_report_time

        # å…¨ä½“ã®é€²æ—ç‡
        progress_pct = (self.processed_records / self.total_records) * 100

        # å…¨ä½“ã®å¹³å‡é€Ÿåº¦
        avg_speed = self.processed_records / elapsed_total if elapsed_total > 0 else 0

        # ç¬é–“é€Ÿåº¦
        instant_speed = (self.processed_records - self.last_processed) / elapsed_since_last if elapsed_since_last > 0 else 0

        # æ®‹ã‚Šæ™‚é–“æ¨å®š
        remaining_records = self.total_records - self.processed_records
        eta_seconds = remaining_records / avg_speed if avg_speed > 0 else 0

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory_mb = 0
        if PSUTIL_AVAILABLE:
            try:
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
            except:
                pass

        self.last_report_time = current_time
        self.last_processed = self.processed_records

        return {
            'processed': self.processed_records,
            'total': self.total_records,
            'progress_pct': progress_pct,
            'avg_speed': avg_speed,
            'instant_speed': instant_speed,
            'eta_seconds': eta_seconds,
            'elapsed_total': elapsed_total,
            'memory_mb': memory_mb
        }

async def process_batch(generator: JobDataGenerator, conn: asyncpg.Connection,
                       start_id: int, batch_size: int, batch_num: int, total_batches: int,
                       progress_tracker: ProgressTracker, dry_run: bool = False) -> int:
    """ãƒãƒƒãƒã‚’å‡¦ç†ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""
    batch_start_time = time.time()

    try:
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        if PSUTIL_AVAILABLE:
            try:
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                if memory_mb > MEMORY_THRESHOLD_MB:
                    logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé–¾å€¤ã‚’è¶…é: {memory_mb:.1f}MB > {MEMORY_THRESHOLD_MB}MB")
            except:
                pass  # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ

        # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        generation_start = time.time()
        jobs = list(generator.generate_batch(start_id, batch_size))
        generation_time = time.time() - generation_start

        if not jobs:
            logger.warning(f"ãƒãƒƒãƒ {batch_num}: ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶")
            return 0

        # ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
        insertion_start = time.time()
        inserted_count = await bulk_insert_jobs(conn, jobs, dry_run)
        insertion_time = time.time() - insertion_start

        # é€²æ—æ›´æ–°
        progress_tracker.update(progress_tracker.processed_records + inserted_count)

        # è©³ç´°é€²æ—è¡¨ç¤º
        batch_elapsed = time.time() - batch_start_time
        batch_speed = inserted_count / batch_elapsed if batch_elapsed > 0 else 0

        logger.info(
            f"ğŸ“¦ ãƒãƒƒãƒ {batch_num:3d}/{total_batches} | "
            f"{inserted_count:,}ä»¶ | "
            f"{batch_elapsed:.2f}s | "
            f"{batch_speed:.0f} rec/s | "
            f"ç”Ÿæˆ:{generation_time:.2f}s æŠ•å…¥:{insertion_time:.2f}s"
        )

        # å®šæœŸçš„ãªè©³ç´°é€²æ—å ±å‘Š
        if batch_num % PROGRESS_REPORT_INTERVAL == 0 or batch_num == total_batches:
            progress_info = progress_tracker.get_progress_info()
            logger.info(
                f"â±ï¸ é€²æ—: {progress_info['processed']:,}/{progress_info['total']:,} "
                f"({progress_info['progress_pct']:.1f}%) | "
                f"å¹³å‡: {progress_info['avg_speed']:.0f} rec/s | "
                f"ç¬é–“: {progress_info['instant_speed']:.0f} rec/s | "
                f"æ®‹ã‚Šæ™‚é–“: {progress_info['eta_seconds']/60:.1f}åˆ† | "
                f"ãƒ¡ãƒ¢ãƒª: {progress_info['memory_mb']:.1f}MB"
            )

        return inserted_count

    except Exception as e:
        logger.error(f"âŒ ãƒãƒƒãƒ {batch_num} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"   ç¯„å›²: {start_id} - {start_id + batch_size - 1}")
        raise

# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

async def check_database_connection(database_url: str) -> bool:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®ç¢ºèª"""
    try:
        conn = await asyncpg.connect(database_url)
        result = await conn.fetchval("SELECT 1")
        await conn.close()
        assert result == 1
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªå®Œäº†")
        return True
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
        return False

async def verify_generated_data(conn: asyncpg.Connection, expected_count: int) -> Dict[str, Any]:
    """ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    try:
        # åŸºæœ¬çµ±è¨ˆ
        count = await conn.fetchval("SELECT COUNT(*) FROM jobs")
        fee_check = await conn.fetchval("SELECT COUNT(*) FROM jobs WHERE fee <= 500")
        min_fee = await conn.fetchval("SELECT MIN(fee) FROM jobs")
        max_fee = await conn.fetchval("SELECT MAX(fee) FROM jobs")
        avg_salary = await conn.fetchval("SELECT AVG((min_salary + max_salary) / 2.0) FROM jobs")

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        null_checks = {
            'company_name': await conn.fetchval("SELECT COUNT(*) FROM jobs WHERE company_name IS NULL"),
            'title': await conn.fetchval("SELECT COUNT(*) FROM jobs WHERE title IS NULL"),
            'pref_cd': await conn.fetchval("SELECT COUNT(*) FROM jobs WHERE pref_cd IS NULL"),
            'feature_codes': await conn.fetchval("SELECT COUNT(*) FROM jobs WHERE feature_codes IS NULL")
        }

        results = {
            'total_count': count,
            'expected_count': expected_count,
            'count_match': count == expected_count,
            'fee_validation': count - fee_check,  # fee > 500ã®ä»¶æ•°
            'fee_range': {'min': min_fee, 'max': max_fee},
            'avg_salary': float(avg_salary) if avg_salary else 0,
            'null_counts': null_checks,
            'data_quality_score': 100 - (sum(null_checks.values()) / count * 100) if count > 0 else 0
        }

        return results

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        raise

async def show_data_distribution(conn: asyncpg.Connection, limit: int = 5):
    """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®è¡¨ç¤º"""
    try:
        logger.info("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒç¢ºèª:")

        # åœ°åŸŸåˆ†å¸ƒ
        pref_dist = await conn.fetch("""
            SELECT p.name, COUNT(*) as cnt,
                   ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM jobs), 1) as pct
            FROM jobs j
            JOIN prefecture_master p ON j.pref_cd = p.code
            GROUP BY p.name
            ORDER BY cnt DESC
            LIMIT $1
        """, limit)

        logger.info(f"åœ°åŸŸTOP{limit}:")
        for row in pref_dist:
            logger.info(f"  {row['name']:10s}: {row['cnt']:7,}ä»¶ ({row['pct']:4.1f}%)")

        # è·ç¨®åˆ†å¸ƒ
        occ_dist = await conn.fetch("""
            SELECT o.name, COUNT(*) as cnt,
                   ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM jobs), 1) as pct
            FROM jobs j
            JOIN occupation_master o ON j.occupation_cd1 = o.code
            GROUP BY o.name
            ORDER BY cnt DESC
            LIMIT $1
        """, limit)

        logger.info(f"è·ç¨®TOP{limit}:")
        for row in occ_dist:
            logger.info(f"  {row['name']:15s}: {row['cnt']:7,}ä»¶ ({row['pct']:4.1f}%)")

        # çµ¦ä¸åˆ†å¸ƒ
        salary_dist = await conn.fetch("""
            SELECT
                CASE
                    WHEN min_salary < 1000 THEN 'ï½999å††'
                    WHEN min_salary < 1200 THEN '1000ï½1199å††'
                    WHEN min_salary < 1500 THEN '1200ï½1499å††'
                    WHEN min_salary < 2000 THEN '1500ï½1999å††'
                    ELSE '2000å††ï½'
                END as salary_range,
                COUNT(*) as cnt,
                ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM jobs), 1) as pct
            FROM jobs
            GROUP BY salary_range
            ORDER BY MIN(min_salary)
        """)

        logger.info("æ™‚çµ¦åˆ†å¸ƒ:")
        for row in salary_dist:
            logger.info(f"  {row['salary_range']:12s}: {row['cnt']:7,}ä»¶ ({row['pct']:4.1f}%)")

    except Exception as e:
        logger.warning(f"åˆ†å¸ƒç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")

async def main(total_jobs: int = TOTAL_JOBS, batch_size: int = BATCH_SIZE,
               dry_run: bool = False, clear_existing: bool = True,
               log_level: str = 'INFO', log_file: Optional[str] = None) -> Dict[str, Any]:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    # ãƒ­ã‚°è¨­å®š
    global logger
    logger = setup_logging(log_level, log_file)

    mode_text = "DRY RUN" if dry_run else "å®Ÿè¡Œ"
    logger.info(f"ğŸš€ æ±‚äººã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ ({mode_text}ãƒ¢ãƒ¼ãƒ‰)")
    logger.info(f"ğŸ“Š ç”Ÿæˆä»¶æ•°: {total_jobs:,}ä»¶")
    logger.info(f"ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size:,}ä»¶")
    logger.info(f"âš¡ ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {PARALLEL_WORKERS}")
    logger.info(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªé–¾å€¤: {MEMORY_THRESHOLD_MB}MB")

    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    if PSUTIL_AVAILABLE:
        try:
            logger.info(f"ğŸ’» CPUæ•°: {psutil.cpu_count()}, ãƒ¡ãƒ¢ãƒª: {psutil.virtual_memory().total / 1024**3:.1f}GB")
        except:
            logger.info("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
    else:
        logger.info("ğŸ’» psutilæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ï¼‰")

    total_start = time.time()
    results = {'success': False, 'total_inserted': 0, 'statistics': {}}

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
    if not await check_database_connection(DATABASE_URL):
        raise Exception("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = await asyncpg.connect(DATABASE_URL)

    try:
        # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        def progress_callback(phase: str, data: Dict[str, Any]):
            if phase == 'generating' and data.get('batch_progress', 0) % 500 == 0:
                logger.debug(f"ç”Ÿæˆé€²æ—: {data['current']:,}ä»¶")

        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        generator = JobDataGenerator(progress_callback=progress_callback)
        await generator.load_master_data(conn)

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
        if clear_existing and not dry_run:
            logger.info("ğŸ—‘ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ä¸­...")
            await conn.execute("TRUNCATE jobs CASCADE")
            logger.info("âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        elif dry_run:
            logger.info("ğŸ§ª DRY RUN: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")

        # é€²æ—è¿½è·¡åˆæœŸåŒ–
        progress_tracker = ProgressTracker(total_jobs)

        # ãƒãƒƒãƒå‡¦ç†
        total_batches = (total_jobs + batch_size - 1) // batch_size  # åˆ‡ã‚Šä¸Šã’
        total_inserted = 0

        logger.info(f"ğŸ“‹ {total_batches}ãƒãƒƒãƒã§å‡¦ç†é–‹å§‹...")

        for batch_num in range(1, total_batches + 1):
            start_id = (batch_num - 1) * batch_size + 1
            current_batch_size = min(batch_size, total_jobs - total_inserted)

            try:
                inserted = await process_batch(
                    generator, conn, start_id, current_batch_size,
                    batch_num, total_batches, progress_tracker, dry_run
                )
                total_inserted += inserted

            except Exception as e:
                logger.error(f"ãƒãƒƒãƒ {batch_num} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚ç¶šè¡Œã™ã‚‹ã‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèª
                logger.warning("å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™...")
                continue

        # å®Œäº†çµ±è¨ˆ
        total_elapsed = time.time() - total_start
        generation_stats = generator.get_generation_stats()

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
        verification_results = {}
        if not dry_run:
            logger.info("\nğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
            verification_results = await verify_generated_data(conn, total_jobs)
            await show_data_distribution(conn, limit=5)

        # çµæœã‚µãƒãƒªãƒ¼
        logger.info("\n" + "=" * 70)
        if dry_run:
            logger.info("âœ… DRY RUN ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            logger.info("ğŸ’¡ å®Ÿéš›ã®ç”Ÿæˆã‚’è¡Œã†å ´åˆã¯ --dry-run ãƒ•ãƒ©ã‚°ã‚’å¤–ã—ã¦ãã ã•ã„")
        else:
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†ï¼")

        logger.info(f"ğŸ“Š ç”Ÿæˆä»¶æ•°: {total_inserted:,}ä»¶")
        logger.info(f"â±ï¸ å‡¦ç†æ™‚é–“: {total_elapsed:.2f}ç§’ ({total_elapsed/60:.1f}åˆ†)")
        logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {total_inserted/total_elapsed:.0f} rec/s")
        logger.info(f"ğŸ’¾ æœ€å¤§ãƒ¡ãƒ¢ãƒª: {generation_stats['memory_usage_mb']:.1f}MB")

        if verification_results:
            logger.info(f"âœ”ï¸ DBç™»éŒ²æ•°: {verification_results['total_count']:,}ä»¶")
            logger.info(f"âœ”ï¸ fee > 500: {verification_results['fee_validation']:,}ä»¶")
            logger.info(f"âœ”ï¸ ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢: {verification_results['data_quality_score']:.1f}%")

        logger.info("=" * 70)

        # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
        results.update({
            'success': True,
            'total_inserted': total_inserted,
            'processing_time': total_elapsed,
            'average_speed': total_inserted/total_elapsed,
            'generation_stats': generation_stats,
            'verification_results': verification_results
        })

        return results

    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        logger.error(f"ğŸ’¡ è©³ç´°ãªãƒ­ã‚°ã¯ DEBUG ãƒ¬ãƒ™ãƒ«ã§ç¢ºèªã—ã¦ãã ã•ã„")
        results['error'] = str(e)
        raise

    finally:
        await conn.close()

def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ"""
    parser = argparse.ArgumentParser(
        description="æ±‚äººã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # é€šå¸¸å®Ÿè¡Œï¼ˆ10ä¸‡ä»¶ï¼‰
  python generate_sample_data.py

  # å°‘æ•°ã§ãƒ†ã‚¹ãƒˆ
  python generate_sample_data.py --total-jobs 1000

  # DRY RUNï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
  python generate_sample_data.py --dry-run

  # DEBUGãƒ­ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
  python generate_sample_data.py --log-level DEBUG --log-file generation.log

  # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
  python generate_sample_data.py --batch-size 2000
        """
    )

    parser.add_argument(
        '--total-jobs',
        type=int,
        default=TOTAL_JOBS,
        help=f'ç”Ÿæˆã™ã‚‹æ±‚äººæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {TOTAL_JOBS:,})'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=BATCH_SIZE,
        help=f'ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {BATCH_SIZE:,})'
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å®Ÿéš›ã®æŠ•å…¥ã‚’è¡Œã‚ãšã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿å®Ÿè¡Œ'
    )

    parser.add_argument(
        '--no-clear',
        action='store_true',
        help='æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ãªã„'
    )

    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFO)'
    )

    parser.add_argument(
        '--log-file',
        help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæŒ‡å®šæ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å‡ºåŠ›ï¼‰'
    )

    parser.add_argument(
        '--database-url',
        help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šURLï¼ˆç’°å¢ƒå¤‰æ•° DATABASE_URL ã‚ˆã‚Šå„ªå…ˆï¼‰'
    )

    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLä¸Šæ›¸ã
    if args.database_url:
        DATABASE_URL = args.database_url

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.total_jobs <= 0:
        print("ã‚¨ãƒ©ãƒ¼: --total-jobs ã¯æ­£ã®æ•´æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        sys.exit(1)

    if args.batch_size <= 0 or args.batch_size > args.total_jobs:
        print("ã‚¨ãƒ©ãƒ¼: --batch-size ã¯æ­£ã®æ•´æ•°ã‹ã¤ç·ä»¶æ•°ä»¥ä¸‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        sys.exit(1)

    # å®Ÿè¡Œ
    try:
        results = asyncio.run(main(
            total_jobs=args.total_jobs,
            batch_size=args.batch_size,
            dry_run=args.dry_run,
            clear_existing=not args.no_clear,
            log_level=args.log_level,
            log_file=args.log_file
        ))

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰
        if results['success']:
            sys.exit(0)
        else:
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)