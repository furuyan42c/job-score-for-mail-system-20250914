#!/usr/bin/env python3
"""
æ±‚äººã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

10ä¸‡ä»¶ã®æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆã—ã¦PostgreSQLã«æŠ•å…¥
å‡¦ç†æ™‚é–“ç›®æ¨™: 5åˆ†ä»¥å†…
"""

import asyncio
import random
import logging
import time
import json
from typing import List, Dict, Any, Generator
from datetime import datetime, timedelta
from decimal import Decimal
import os
import sys
from io import StringIO

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncpg
from faker import Faker
import numpy as np

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šURL
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:postgres@localhost:5432/job_matching"
).replace('+asyncpg', '')  # asyncpgã¯+ã‚’é™¤ã

# =============================================================================
# è¨­å®šå®šæ•°
# =============================================================================

# ç”Ÿæˆè¨­å®š
TOTAL_JOBS = 100000  # ç”Ÿæˆã™ã‚‹æ±‚äººæ•°
BATCH_SIZE = 5000    # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
PARALLEL_WORKERS = 4  # ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°

# ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒè¨­å®š
MIN_FEE = 501        # feeæœ€å°å€¤ï¼ˆ> 500ï¼‰
MAX_FEE = 50000      # feeæœ€å¤§å€¤

# åœ°åŸŸåˆ†å¸ƒï¼ˆäººå£æ¯”ç‡ãƒ™ãƒ¼ã‚¹ï¼‰
PREFECTURE_WEIGHTS = {
    '13': 0.15,  # æ±äº¬éƒ½ 15%
    '27': 0.08,  # å¤§é˜ªåºœ 8%
    '14': 0.07,  # ç¥å¥ˆå·çœŒ 7%
    '23': 0.06,  # æ„›çŸ¥çœŒ 6%
    '11': 0.05,  # åŸ¼ç‰çœŒ 5%
    '12': 0.05,  # åƒè‘‰çœŒ 5%
    '40': 0.04,  # ç¦å²¡çœŒ 4%
    '28': 0.04,  # å…µåº«çœŒ 4%
    '01': 0.03,  # åŒ—æµ·é“ 3%
    # ãã®ä»–ã¯å‡ç­‰åˆ†å¸ƒ
}

# è·ç¨®åˆ†å¸ƒï¼ˆãƒã‚¤ãƒˆå¸‚å ´ã®å®Ÿæ…‹ãƒ™ãƒ¼ã‚¹ï¼‰
OCCUPATION_WEIGHTS = {
    1: 0.25,   # é£²é£Ÿãƒ»ãƒ•ãƒ¼ãƒ‰ 25%
    2: 0.20,   # ã‚³ãƒ³ãƒ“ãƒ‹ãƒ»ã‚¹ãƒ¼ãƒ‘ãƒ¼ 20%
    3: 0.08,   # ã‚¢ãƒ‘ãƒ¬ãƒ«ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ 8%
    4: 0.12,   # è»½ä½œæ¥­ãƒ»ç‰©æµ 12%
    5: 0.08,   # å·¥å ´ãƒ»è£½é€  8%
    6: 0.07,   # äº‹å‹™ãƒ»ãƒ‡ãƒ¼ã‚¿å…¥åŠ› 7%
    7: 0.05,   # ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼ 5%
    8: 0.05,   # å–¶æ¥­ãƒ»è²©å£² 5%
    9: 0.03,   # ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒƒãƒ• 3%
    10: 0.03,  # æ•™è‚²ãƒ»å¡¾è¬›å¸« 3%
    11: 0.02,  # ä»‹è­·ãƒ»çœ‹è­·åŠ©æ‰‹ 2%
    12: 0.02,  # æ¸…æƒãƒ»ãƒ“ãƒ«ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ 2%
}

# é›‡ç”¨å½¢æ…‹åˆ†å¸ƒï¼ˆãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã®ã¿ï¼‰
EMPLOYMENT_TYPE_WEIGHTS = {
    1: 0.60,  # ã‚¢ãƒ«ãƒã‚¤ãƒˆ 60%
    3: 0.30,  # ãƒ‘ãƒ¼ãƒˆ 30%
    8: 0.10,  # æ—¥é›‡ã„ 10%
}

# ä¼æ¥­åãƒ‘ã‚¿ãƒ¼ãƒ³
COMPANY_PATTERNS = [
    "æ ªå¼ä¼šç¤¾{}", "{}", "{}å•†åº—", "{}ã‚°ãƒ«ãƒ¼ãƒ—",
    "{}ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹", "{}ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼", "{}ã‚µãƒ¼ãƒ“ã‚¹",
    "æœ‰é™ä¼šç¤¾{}", "{}ã‚·ã‚¹ãƒ†ãƒ ", "{}ãƒ•ãƒ¼ã‚º",
    "{}ã‚¹ãƒˆã‚¢", "{}ãƒãƒ¼ãƒˆ", "{}ã‚»ãƒ³ã‚¿ãƒ¼"
]

# æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
JOB_TITLE_TEMPLATES = {
    1: ["{}ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ›ãƒ¼ãƒ«ãƒ»ã‚­ãƒƒãƒãƒ³ã‚¹ã‚¿ãƒƒãƒ•", "é£²é£Ÿåº—ã‚¹ã‚¿ãƒƒãƒ•", "èª¿ç†è£œåŠ©"],
    2: ["ã‚³ãƒ³ãƒ“ãƒ‹ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ¬ã‚¸ã‚¹ã‚¿ãƒƒãƒ•", "å“å‡ºã—ã‚¹ã‚¿ãƒƒãƒ•", "{}åº—å“¡"],
    3: ["ã‚¢ãƒ‘ãƒ¬ãƒ«è²©å£²å“¡", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³è²©å£²", "ã‚·ãƒ§ãƒƒãƒ—ã‚¹ã‚¿ãƒƒãƒ•"],
    4: ["å€‰åº«ä½œæ¥­å“¡", "ä»•åˆ†ã‘ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ”ãƒƒã‚­ãƒ³ã‚°ä½œæ¥­", "æ¢±åŒ…ã‚¹ã‚¿ãƒƒãƒ•"],
    5: ["å·¥å ´ä½œæ¥­å“¡", "è£½é€ ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ©ã‚¤ãƒ³ä½œæ¥­", "çµ„ç«‹ä½œæ¥­å“¡"],
    6: ["ãƒ‡ãƒ¼ã‚¿å…¥åŠ›", "ä¸€èˆ¬äº‹å‹™", "äº‹å‹™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "PCã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼"],
    7: ["ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼", "é›»è©±å¯¾å¿œ", "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ", "ãƒ†ãƒ¬ã‚ªãƒš"],
    8: ["å–¶æ¥­ã‚¹ã‚¿ãƒƒãƒ•", "è²©å£²å“¡", "ã‚»ãƒ¼ãƒ«ã‚¹ã‚¹ã‚¿ãƒƒãƒ•"],
    9: ["ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒƒãƒ•", "è¨­å–¶ã‚¹ã‚¿ãƒƒãƒ•", "é‹å–¶ã‚¹ã‚¿ãƒƒãƒ•"],
    10: ["å¡¾è¬›å¸«", "å®¶åº­æ•™å¸«", "æ•™è‚²ã‚¹ã‚¿ãƒƒãƒ•"],
    11: ["ä»‹è­·ã‚¹ã‚¿ãƒƒãƒ•", "çœ‹è­·åŠ©æ‰‹", "ãƒ˜ãƒ«ãƒ‘ãƒ¼"],
    12: ["æ¸…æƒã‚¹ã‚¿ãƒƒãƒ•", "ãƒ“ãƒ«æ¸…æƒ", "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¹ã‚¿ãƒƒãƒ•"],
}

# =============================================================================
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–¢æ•°
# =============================================================================

class JobDataGenerator:
    """æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    def __init__(self):
        self.faker = Faker('ja_JP')
        self.master_data = {}
        self.company_names_cache = set()

    async def load_master_data(self, conn: asyncpg.Connection):
        """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # éƒ½é“åºœçœŒ
        rows = await conn.fetch("SELECT code, name FROM prefecture_master")
        self.master_data['prefectures'] = {row['code']: row['name'] for row in rows}

        # å¸‚åŒºç”ºæ‘
        rows = await conn.fetch("SELECT code, pref_cd, name FROM city_master")
        self.master_data['cities'] = [(row['code'], row['pref_cd']) for row in rows]

        # è·ç¨®
        rows = await conn.fetch("SELECT code FROM occupation_master WHERE is_active = TRUE")
        self.master_data['occupations'] = [row['code'] for row in rows]

        # é›‡ç”¨å½¢æ…‹
        rows = await conn.fetch("SELECT code FROM employment_type_master WHERE is_valid_for_matching = TRUE")
        self.master_data['employment_types'] = [row['code'] for row in rows]

        # ç‰¹å¾´
        rows = await conn.fetch("SELECT feature_code FROM feature_master WHERE is_active = TRUE")
        self.master_data['features'] = [row['feature_code'] for row in rows]

        logger.info(f"ğŸ“š ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {len(self.master_data['prefectures'])}éƒ½é“åºœçœŒ")

    def generate_company_name(self) -> str:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªä¼æ¥­åã‚’ç”Ÿæˆ"""
        attempts = 0
        while attempts < 100:
            base_name = self.faker.company()
            pattern = random.choice(COMPANY_PATTERNS)
            company_name = pattern.format(base_name.replace('æ ªå¼ä¼šç¤¾', '').replace('æœ‰é™ä¼šç¤¾', ''))

            if company_name not in self.company_names_cache:
                self.company_names_cache.add(company_name)
                return company_name
            attempts += 1

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä»˜åŠ 
        unique_name = f"{base_name}_{int(time.time() * 1000) % 100000}"
        self.company_names_cache.add(unique_name)
        return unique_name

    def generate_endcl_cd(self, index: int) -> str:
        """ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆï¼ˆCOMPANY_XXXXXå½¢å¼ï¼‰"""
        return f"COMPANY_{index:06d}"

    def select_prefecture(self) -> str:
        """é‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦éƒ½é“åºœçœŒã‚’é¸æŠ"""
        if random.random() < sum(PREFECTURE_WEIGHTS.values()):
            # é‡ã¿ä»˜ã‘é¸æŠ
            prefs = list(PREFECTURE_WEIGHTS.keys())
            weights = list(PREFECTURE_WEIGHTS.values())
            return np.random.choice(prefs, p=np.array(weights)/sum(weights))
        else:
            # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
            return random.choice(list(self.master_data['prefectures'].keys()))

    def select_city(self, pref_cd: str) -> str:
        """éƒ½é“åºœçœŒã«å¿œã˜ãŸå¸‚åŒºç”ºæ‘ã‚’é¸æŠ"""
        cities = [c[0] for c in self.master_data['cities'] if c[1] == pref_cd]
        if cities:
            return random.choice(cities)
        return f"{pref_cd}000"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def select_occupation(self) -> int:
        """é‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦è·ç¨®ã‚’é¸æŠ"""
        occs = list(OCCUPATION_WEIGHTS.keys())
        weights = list(OCCUPATION_WEIGHTS.values())
        return np.random.choice(occs, p=weights)

    def select_employment_type(self) -> int:
        """é‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦é›‡ç”¨å½¢æ…‹ã‚’é¸æŠ"""
        types = list(EMPLOYMENT_TYPE_WEIGHTS.keys())
        weights = list(EMPLOYMENT_TYPE_WEIGHTS.values())
        return np.random.choice(types, p=weights)

    def generate_salary(self, occupation: int) -> tuple:
        """è·ç¨®ã«å¿œã˜ãŸçµ¦ä¸ã‚’ç”Ÿæˆ"""
        # è·ç¨®åˆ¥ã®æ™‚çµ¦ãƒ¬ãƒ³ã‚¸
        salary_ranges = {
            1: (950, 1500),    # é£²é£Ÿ
            2: (900, 1300),    # ã‚³ãƒ³ãƒ“ãƒ‹
            3: (1000, 1600),   # ã‚¢ãƒ‘ãƒ¬ãƒ«
            4: (1000, 1800),   # è»½ä½œæ¥­
            5: (1100, 2000),   # å·¥å ´
            6: (1000, 1500),   # äº‹å‹™
            7: (1200, 1800),   # ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼
            8: (1100, 2000),   # å–¶æ¥­
            9: (1000, 1500),   # ã‚¤ãƒ™ãƒ³ãƒˆ
            10: (1500, 3000),  # å¡¾è¬›å¸«
            11: (1100, 1600),  # ä»‹è­·
            12: (950, 1400),   # æ¸…æƒ
        }

        min_range, max_range = salary_ranges.get(occupation, (1000, 1500))
        min_salary = random.randint(min_range, max_range - 200)
        max_salary = min_salary + random.randint(100, 500)

        return min_salary, max_salary

    def generate_features(self, salary_min: int) -> List[str]:
        """æ±‚äººã®ç‰¹å¾´ã‚’ç”Ÿæˆ"""
        features = []
        num_features = random.randint(3, 8)

        # é«˜æ™‚çµ¦ã®å ´åˆã¯å¿…ãšè¿½åŠ 
        if salary_min >= 1200:
            features.append('F03')  # é«˜åå…¥
            features.append('F04')  # æ™‚çµ¦1200å††ä»¥ä¸Š

        # ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´ã‚’è¿½åŠ 
        available_features = [f for f in self.master_data['features'] if f not in features]
        features.extend(random.sample(available_features, min(num_features - len(features), len(available_features))))

        return features[:num_features]

    def generate_job_title(self, occupation: int, company_name: str) -> str:
        """è·ç¨®ã«å¿œã˜ãŸæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        templates = JOB_TITLE_TEMPLATES.get(occupation, ["ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†"])
        template = random.choice(templates)

        if '{}' in template:
            return template.format(company_name.split('æ ªå¼ä¼šç¤¾')[-1].split('æœ‰é™ä¼šç¤¾')[-1][:10])
        return template

    def generate_catch_copy(self, features: List[str]) -> str:
        """ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆ"""
        copies = [
            "æœªçµŒé¨“OKï¼å……å®Ÿã®ç ”ä¿®ã§ã—ã£ã‹ã‚Šã‚µãƒãƒ¼ãƒˆ",
            "é«˜æ™‚çµ¦ï¼ãŒã£ã¤ã‚Šç¨¼ã’ã¾ã™",
            "é€±1æ—¥ã€œOKï¼è‡ªåˆ†ã®ãƒšãƒ¼ã‚¹ã§åƒã‘ã‚‹",
            "é§…ãƒã‚«ã§é€šå‹¤ä¾¿åˆ©ï¼",
            "ç¤¾å“¡ç™»ç”¨ã‚ã‚Šï¼ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒƒãƒ—å¯èƒ½",
            "æ—¥æ‰•ã„OKï¼ã™ãã«ãŠé‡‘ãŒå¿…è¦ãªæ–¹ã‚‚å®‰å¿ƒ",
            "ã‚·ãƒ•ãƒˆè‡ªç”±ï¼ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚‚å……å®Ÿ",
            "ã¾ã‹ãªã„ä»˜ãï¼ç¾å‘³ã—ã„é£Ÿäº‹ã‚‚æ¥½ã—ã‚ã‚‹",
            "å‹é”ã¨ä¸€ç·’ã«å¿œå‹ŸOKï¼",
            "çŸ­æœŸOKï¼1ãƒ¶æœˆã‹ã‚‰åƒã‘ã‚‹"
        ]

        # ç‰¹å¾´ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’é¸æŠ
        if 'F01' in features:
            return "æ—¥æ‰•ã„OKï¼ã™ãã«ãŠé‡‘ãŒå¿…è¦ãªæ–¹å¿…è¦‹ï¼"
        elif 'F03' in features:
            return "é«˜æ™‚çµ¦ï¼æœˆå25ä¸‡å††ä»¥ä¸Šå¯èƒ½ï¼"
        elif 'F10' in features:
            return "æœªçµŒé¨“å¤§æ­“è¿ï¼ä¸å¯§ãªç ”ä¿®ã§ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—ï¼"

        return random.choice(copies)

    def generate_job_description(self, occupation: int, company_name: str) -> str:
        """æ±‚äººèª¬æ˜æ–‡ã‚’ç”Ÿæˆ"""
        base_descriptions = {
            1: f"{company_name}ã§ä¸€ç·’ã«åƒãã¾ã›ã‚“ã‹ï¼Ÿæ¥å®¢ã‚„èª¿ç†è£œåŠ©ãªã©ã€ã‚„ã‚ŠãŒã„ã®ã‚ã‚‹ãŠä»•äº‹ã§ã™ã€‚",
            2: f"åœ°åŸŸã«æ„›ã•ã‚Œã‚‹{company_name}ã§ã®ãŠä»•äº‹ã€‚ãƒ¬ã‚¸ãƒ»å“å‡ºã—ãƒ»æ¸…æƒãªã©å¹…åºƒã„æ¥­å‹™ãŒã‚ã‚Šã¾ã™ã€‚",
            3: f"ãŠã—ã‚ƒã‚Œãª{company_name}ã§è²©å£²ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†ï¼ãŠå®¢æ§˜ã«ç´ æ•µãªå•†å“ã‚’ã”ææ¡ˆã™ã‚‹ãŠä»•äº‹ã§ã™ã€‚",
            4: f"{company_name}ã®ç‰©æµã‚»ãƒ³ã‚¿ãƒ¼ã§ã®è»½ä½œæ¥­ã€‚é‡ã„ã‚‚ã®ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚é»™ã€…ã¨ä½œæ¥­ã—ãŸã„æ–¹ã«æœ€é©ï¼",
            5: f"{company_name}ã®å·¥å ´ã§ã®ãƒ©ã‚¤ãƒ³ä½œæ¥­ã€‚ç°¡å˜ãªçµ„ç«‹ã‚„æ¤œæŸ»ã®ãŠä»•äº‹ã§ã™ã€‚",
            6: f"{company_name}ã§ã®äº‹å‹™ä½œæ¥­ã€‚PCã‚’ä½¿ã£ãŸç°¡å˜ãªãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãŒãƒ¡ã‚¤ãƒ³ã§ã™ã€‚",
            7: f"{company_name}ã®ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼ã§ãŠå®¢æ§˜å¯¾å¿œã€‚ç ”ä¿®å……å®Ÿã§æœªçµŒé¨“ã§ã‚‚å®‰å¿ƒï¼",
            8: f"{company_name}ã§å–¶æ¥­ãƒ»è²©å£²ã®ãŠä»•äº‹ã€‚äººã¨è©±ã™ã“ã¨ãŒå¥½ããªæ–¹å¤§æ­“è¿ï¼",
            9: f"å„ç¨®ã‚¤ãƒ™ãƒ³ãƒˆã§ã®{company_name}ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†ã€‚æ¥½ã—ãåƒã‘ã‚‹ç’°å¢ƒã§ã™ï¼",
            10: f"{company_name}ã§è¬›å¸«å‹Ÿé›†ã€‚ã‚ãªãŸã®çŸ¥è­˜ã‚’æ´»ã‹ã—ã¦ç”Ÿå¾’ã®æˆé•·ã‚’ã‚µãƒãƒ¼ãƒˆï¼",
            11: f"{company_name}ã§ã®ä»‹è­·ãƒ»çœ‹è­·è£œåŠ©ã€‚ã‚„ã‚ŠãŒã„ã®ã‚ã‚‹ç¦ç¥‰ã®ãŠä»•äº‹ã§ã™ã€‚",
            12: f"{company_name}ã§ã®æ¸…æƒæ¥­å‹™ã€‚ãã‚Œã„ãªç’°å¢ƒã¥ãã‚Šã®ãŠæ‰‹ä¼ã„ï¼",
        }

        base = base_descriptions.get(occupation, f"{company_name}ã§ã‚¹ã‚¿ãƒƒãƒ•å‹Ÿé›†ä¸­ï¼")
        additions = [
            "\n\nã€ä»•äº‹å†…å®¹ã€‘\nå…·ä½“çš„ãªæ¥­å‹™å†…å®¹ã¯é¢æ¥æ™‚ã«è©³ã—ãã”èª¬æ˜ã—ã¾ã™ã€‚",
            "\n\nã€ã“ã‚“ãªæ–¹æ­“è¿ã€‘\nãƒ»æ˜ã‚‹ãå…ƒæ°—ãªæ–¹\nãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¥½ããªæ–¹\nãƒ»è²¬ä»»æ„Ÿã®ã‚ã‚‹æ–¹",
            "\n\nã€å¾…é‡ã€‘\näº¤é€šè²»æ”¯çµ¦ã€åˆ¶æœè²¸ä¸ã€ç¤¾ä¼šä¿é™ºå®Œå‚™ï¼ˆæ¡ä»¶ã‚ã‚Šï¼‰",
            "\n\nã€å¿œå‹Ÿã€‘\nã¾ãšã¯ãŠæ°—è»½ã«ã”å¿œå‹Ÿãã ã•ã„ï¼å±¥æ­´æ›¸ä¸è¦ã®é¢æ¥ã‚‚å¯èƒ½ã§ã™ã€‚"
        ]

        return base + random.choice(additions)

    def generate_work_hours(self, employment_type: int) -> str:
        """å‹¤å‹™æ™‚é–“ã‚’ç”Ÿæˆ"""
        if employment_type == 1:  # ã‚¢ãƒ«ãƒã‚¤ãƒˆ
            patterns = [
                "9:00ï½18:00", "10:00ï½19:00", "11:00ï½20:00",
                "17:00ï½22:00", "18:00ï½23:00", "ã‚·ãƒ•ãƒˆåˆ¶"
            ]
        elif employment_type == 3:  # ãƒ‘ãƒ¼ãƒˆ
            patterns = [
                "9:00ï½14:00", "10:00ï½15:00", "9:00ï½13:00",
                "13:00ï½17:00", "14:00ï½18:00"
            ]
        else:  # æ—¥é›‡ã„
            patterns = [
                "8:00ï½17:00", "9:00ï½18:00", "æ—¥ã«ã‚ˆã£ã¦ç•°ãªã‚‹"
            ]

        return random.choice(patterns)

    def generate_single_job(self, job_id: int, company_index: int) -> Dict[str, Any]:
        """1ä»¶ã®æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        # åŸºæœ¬æƒ…å ±
        endcl_cd = self.generate_endcl_cd(company_index)
        company_name = self.generate_company_name()
        pref_cd = self.select_prefecture()
        city_cd = self.select_city(pref_cd)
        occupation = self.select_occupation()
        employment_type = self.select_employment_type()

        # çµ¦ä¸
        min_salary, max_salary = self.generate_salary(occupation)

        # ç‰¹å¾´
        features = self.generate_features(min_salary)

        # feeï¼ˆå¿œå‹Ÿä¿ƒé€²è²»ç”¨ï¼‰
        # é«˜æ™‚çµ¦ã‚„äººæ°—è·ç¨®ã¯é«˜ã‚ã®fee
        base_fee = random.randint(MIN_FEE, 5000)
        if min_salary >= 1500:
            base_fee += random.randint(1000, 3000)
        if occupation in [1, 2, 4]:  # äººæ°—è·ç¨®
            base_fee += random.randint(500, 2000)

        # æ²è¼‰æ—¥ï¼ˆéå»30æ—¥ã€œæœªæ¥7æ—¥ï¼‰
        posting_date = datetime.now() + timedelta(days=random.randint(-30, 7))

        # ãã®ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        job_data = {
            'job_id': job_id,
            'endcl_cd': endcl_cd,
            'company_name': company_name,
            'title': self.generate_job_title(occupation, company_name),
            'catch_copy': self.generate_catch_copy(features),
            'job_description': self.generate_job_description(occupation, company_name),
            'pref_cd': pref_cd,
            'city_cd': city_cd,
            'station_name': f"{self.faker.city()}é§…",
            'occupation_cd1': occupation,
            'occupation_cd2': None,
            'occupation_cd3': None,
            'employment_type_cd': employment_type,
            'min_salary': min_salary,
            'max_salary': max_salary,
            'salary_text': f"æ™‚çµ¦{min_salary}å††ã€œ{max_salary}å††",
            'work_hours': self.generate_work_hours(employment_type),
            'work_days_min': random.randint(1, 3),
            'work_days_max': random.randint(4, 7),
            'feature_codes': features,
            'fee': base_fee,
            'posting_date': posting_date.date(),
            'valid_until': (posting_date + timedelta(days=60)).date(),
            'is_active': True,
            'view_count': random.randint(0, 10000),
            'application_count': random.randint(0, 100),
            'created_at': datetime.now(),
            'updated_at': datetime.now()
        }

        return job_data

    def generate_batch(self, start_id: int, batch_size: int) -> Generator[Dict[str, Any], None, None]:
        """ãƒãƒƒãƒå˜ä½ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰"""
        company_index_start = start_id // 10  # 1ä¼æ¥­ã‚ãŸã‚Šå¹³å‡10æ±‚äºº

        for i in range(batch_size):
            job_id = start_id + i
            company_index = company_index_start + (i // 10)
            yield self.generate_single_job(job_id, company_index)

# =============================================================================
# ãƒ‡ãƒ¼ã‚¿æŠ•å…¥é–¢æ•°
# =============================================================================

async def bulk_insert_jobs(conn: asyncpg.Connection, jobs: List[Dict[str, Any]]):
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã§æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥"""
    # COPYç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    copy_data = []

    for job in jobs:
        # PostgreSQLé…åˆ—å½¢å¼ã«å¤‰æ›
        feature_codes_str = '{' + ','.join(job['feature_codes']) + '}'

        row = (
            job['job_id'],
            job['endcl_cd'],
            job['company_name'],
            job['title'],
            job['catch_copy'],
            job['job_description'],
            job['pref_cd'],
            job['city_cd'],
            job['station_name'],
            job['occupation_cd1'],
            job['occupation_cd2'],
            job['occupation_cd3'],
            job['employment_type_cd'],
            job['min_salary'],
            job['max_salary'],
            job['salary_text'],
            job['work_hours'],
            job['work_days_min'],
            job['work_days_max'],
            feature_codes_str,
            job['fee'],
            job['posting_date'],
            job['valid_until'],
            job['is_active'],
            job['view_count'],
            job['application_count'],
            job['created_at'],
            job['updated_at']
        )
        copy_data.append(row)

    # COPYæ–‡ã§ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    await conn.copy_records_to_table(
        'jobs',
        records=copy_data,
        columns=[
            'job_id', 'endcl_cd', 'company_name', 'title', 'catch_copy',
            'job_description', 'pref_cd', 'city_cd', 'station_name',
            'occupation_cd1', 'occupation_cd2', 'occupation_cd3',
            'employment_type_cd', 'min_salary', 'max_salary', 'salary_text',
            'work_hours', 'work_days_min', 'work_days_max', 'feature_codes',
            'fee', 'posting_date', 'valid_until', 'is_active',
            'view_count', 'application_count', 'created_at', 'updated_at'
        ]
    )

async def process_batch(generator: JobDataGenerator, conn: asyncpg.Connection,
                       start_id: int, batch_size: int, batch_num: int, total_batches: int):
    """ãƒãƒƒãƒã‚’å‡¦ç†"""
    start_time = time.time()

    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    jobs = list(generator.generate_batch(start_id, batch_size))

    # ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
    await bulk_insert_jobs(conn, jobs)

    # é€²æ—è¡¨ç¤º
    elapsed = time.time() - start_time
    speed = batch_size / elapsed

    logger.info(
        f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å®Œäº† | "
        f"{batch_size:,}ä»¶ | {elapsed:.2f}ç§’ | "
        f"{speed:.0f} rec/s"
    )

    return batch_size

# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    total_start = time.time()

    logger.info("ğŸš€ æ±‚äººã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™")
    logger.info(f"ğŸ“Š ç”Ÿæˆä»¶æ•°: {TOTAL_JOBS:,}ä»¶")
    logger.info(f"ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE:,}ä»¶")
    logger.info(f"âš¡ ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {PARALLEL_WORKERS}")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = await asyncpg.connect(DATABASE_URL)

    try:
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        generator = JobDataGenerator()
        await generator.load_master_data(conn)

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if os.getenv('CLEAR_EXISTING', 'true').lower() == 'true':
            await conn.execute("TRUNCATE jobs CASCADE")
            logger.info("ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

        # ãƒãƒƒãƒå‡¦ç†
        total_batches = TOTAL_JOBS // BATCH_SIZE
        total_inserted = 0

        for batch_num in range(1, total_batches + 1):
            start_id = (batch_num - 1) * BATCH_SIZE + 1

            inserted = await process_batch(
                generator, conn, start_id, BATCH_SIZE,
                batch_num, total_batches
            )
            total_inserted += inserted

            # æ®‹ã‚Šæ™‚é–“ã®æ¨å®š
            elapsed_total = time.time() - total_start
            avg_speed = total_inserted / elapsed_total
            remaining = TOTAL_JOBS - total_inserted
            eta = remaining / avg_speed if avg_speed > 0 else 0

            if batch_num % 5 == 0:  # 5ãƒãƒƒãƒã”ã¨ã«è©³ç´°è¡¨ç¤º
                logger.info(
                    f"â±ï¸  é€²æ—: {total_inserted:,}/{TOTAL_JOBS:,} | "
                    f"å¹³å‡é€Ÿåº¦: {avg_speed:.0f} rec/s | "
                    f"æ®‹ã‚Šæ™‚é–“: {eta/60:.1f}åˆ†"
                )

        # æ®‹ã‚Šã®ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆç«¯æ•°ï¼‰ã‚’å‡¦ç†
        remaining = TOTAL_JOBS - total_inserted
        if remaining > 0:
            start_id = total_inserted + 1
            await process_batch(
                generator, conn, start_id, remaining,
                total_batches + 1, total_batches + 1
            )
            total_inserted += remaining

        # å®Œäº†çµ±è¨ˆ
        total_elapsed = time.time() - total_start

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        count = await conn.fetchval("SELECT COUNT(*) FROM jobs")
        fee_check = await conn.fetchval("SELECT COUNT(*) FROM jobs WHERE fee <= 500")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†ï¼")
        logger.info(f"ğŸ“Š ç”Ÿæˆä»¶æ•°: {total_inserted:,}ä»¶")
        logger.info(f"â±ï¸  å‡¦ç†æ™‚é–“: {total_elapsed:.2f}ç§’ ({total_elapsed/60:.1f}åˆ†)")
        logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {total_inserted/total_elapsed:.0f} rec/s")
        logger.info(f"âœ”ï¸  DBç™»éŒ²æ•°: {count:,}ä»¶")
        logger.info(f"âœ”ï¸  fee > 500: {count - fee_check:,}ä»¶ (100%)")
        logger.info("=" * 60)

        # åˆ†å¸ƒç¢ºèª
        logger.info("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒç¢ºèª:")

        # åœ°åŸŸåˆ†å¸ƒ
        pref_dist = await conn.fetch("""
            SELECT p.name, COUNT(*) as cnt
            FROM jobs j
            JOIN prefecture_master p ON j.pref_cd = p.code
            GROUP BY p.name
            ORDER BY cnt DESC
            LIMIT 5
        """)
        logger.info("åœ°åŸŸTOP5:")
        for row in pref_dist:
            logger.info(f"  {row['name']}: {row['cnt']:,}ä»¶")

        # è·ç¨®åˆ†å¸ƒ
        occ_dist = await conn.fetch("""
            SELECT o.name, COUNT(*) as cnt
            FROM jobs j
            JOIN occupation_master o ON j.occupation_cd1 = o.code
            GROUP BY o.name
            ORDER BY cnt DESC
            LIMIT 5
        """)
        logger.info("è·ç¨®TOP5:")
        for row in occ_dist:
            logger.info(f"  {row['name']}: {row['cnt']:,}ä»¶")

    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    finally:
        await conn.close()

if __name__ == "__main__":
    # ç”Ÿæˆæ•°ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰å–å¾—
    if len(sys.argv) > 1:
        try:
            TOTAL_JOBS = int(sys.argv[1])
        except ValueError:
            logger.error("å¼•æ•°ã¯æ•´æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)

    # å®Ÿè¡Œ
    asyncio.run(main())