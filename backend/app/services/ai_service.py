"""
AI Service Integration for GPT-5 Nano
T032-GREEN: Mock implementation of GPT-5 nano integration for email content enhancement

This service provides AI-powered email content enhancement and personalization suggestions.
In production, this would integrate with OpenAI's GPT-5 nano model.
For now, it provides intelligent mock responses for development and testing.
"""

import asyncio
import hashlib
import json
import logging
import random
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

from app.services.gpt5_integration import (
    EmailContent,
    EmailLanguage,
    EmailSection,
    EmailSectionType,
    GenerationResult,
    GenerationStatus,
    JobMatch,
    UserProfile,
)

logger = logging.getLogger(__name__)

# ============================================================================
# ENUMS AND CONSTANTS
# ============================================================================


class AIModelType(Enum):
    """Available AI models"""

    GPT5_NANO = "gpt-5-nano"
    GPT4_TURBO = "gpt-4-turbo"
    CLAUDE_HAIKU = "claude-3-haiku"
    MOCK = "mock"


class ContentType(Enum):
    """Types of content that can be generated"""

    EMAIL_SUBJECT = "email_subject"
    EMAIL_GREETING = "email_greeting"
    SECTION_TITLE = "section_title"
    SECTION_DESCRIPTION = "section_description"
    JOB_DESCRIPTION = "job_description"
    PERSONALIZATION_NOTE = "personalization_note"
    CALL_TO_ACTION = "call_to_action"


class AIQualityLevel(Enum):
    """Quality levels for AI generation"""

    BASIC = "basic"
    STANDARD = "standard"
    PREMIUM = "premium"
    ULTRA = "ultra"


# ============================================================================
# DATA MODELS
# ============================================================================


@dataclass
class AIGenerationRequest:
    """Request for AI content generation"""

    content_type: ContentType
    user_profile: UserProfile
    job_matches: List[JobMatch] = None
    context_data: Dict[str, Any] = None
    language: EmailLanguage = EmailLanguage.JAPANESE
    quality_level: AIQualityLevel = AIQualityLevel.STANDARD
    max_length: int = 500
    tone: str = "professional"
    personalization_level: float = 0.7  # 0.0 to 1.0


@dataclass
class AIGenerationResponse:
    """Response from AI content generation"""

    content: str
    confidence_score: float
    generation_time_ms: int
    tokens_used: int
    model_used: AIModelType
    quality_metrics: Dict[str, float]
    suggestions: List[str] = None
    alternatives: List[str] = None


@dataclass
class AIPerformanceMetrics:
    """Performance metrics for AI service"""

    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    avg_response_time_ms: float = 0.0
    total_tokens_used: int = 0
    cache_hit_rate: float = 0.0
    quality_score_avg: float = 0.0


# ============================================================================
# MOCK AI TEMPLATES AND DATA
# ============================================================================


class MockAITemplates:
    """Mock AI templates for development and testing"""

    JAPANESE_SUBJECTS = {
        AIQualityLevel.BASIC: [
            "{name}æ§˜ã¸ã®æ±‚äººæƒ…å ±",
            "æœ¬æ—¥ã®æ±‚äºº{count}ä»¶",
            "ãŠã™ã™ã‚æ±‚äººã®ã”æ¡ˆå†…",
        ],
        AIQualityLevel.STANDARD: [
            "{name}æ§˜ã«å³é¸ã•ã‚ŒãŸ{count}ä»¶ã®æ±‚äººã‚’ãŠå±Šã‘",
            "ã‚ãªãŸã®ã‚¹ã‚­ãƒ«ã«ãƒãƒƒãƒã™ã‚‹{count}ä»¶ã®æ–°ç€æ±‚äºº",
            "{name}æ§˜ã¸ - {location}ã‚¨ãƒªã‚¢ã®æ³¨ç›®æ±‚äºº{count}ä»¶",
        ],
        AIQualityLevel.PREMIUM: [
            "{name}æ§˜å°‚ç”¨ - AIãŒé¸ã‚“ã ãƒãƒƒãƒåº¦{score}%ã®å³é¸æ±‚äºº{count}ä»¶",
            "ğŸ¯ {name}æ§˜ã«ãƒ”ãƒƒã‚¿ãƒªï¼{skill}çµŒé¨“ã‚’æ´»ã‹ã›ã‚‹æ±‚äºº{count}ä»¶",
            "âœ¨ ç‰¹åˆ¥ã‚»ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼š{name}æ§˜ã®ç†æƒ³ã«åˆã†{count}ä»¶ã®æ±‚äºº",
        ],
        AIQualityLevel.ULTRA: [
            "ğŸš€ {name}æ§˜ã®æ¬¡ã®ã‚­ãƒ£ãƒªã‚¢ã‚¹ãƒ†ãƒƒãƒ— - AIåˆ†æã«ã‚ˆã‚‹æœ€é©æ±‚äºº{count}ä»¶",
            "ğŸ’ {name}æ§˜ã ã‘ã®ç‰¹åˆ¥ãªæ©Ÿä¼šï¼š{skill}Ã—{location}ã®è¶…å³é¸æ±‚äºº{count}ä»¶",
            "ğŸŠ æœ—å ±ï¼{name}æ§˜ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã«{score}%ãƒãƒƒãƒã™ã‚‹å¤¢ã®æ±‚äºº{count}ä»¶",
        ],
    }

    GREETINGS = {
        EmailLanguage.JAPANESE: {
            "morning": [
                "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€{name}æ§˜",
                "æœã®ãŠå¿™ã—ã„æ™‚é–“ã«ãŠç–²ã‚Œã•ã¾ã§ã™ã€{name}æ§˜",
                "æ–°ã—ã„ä¸€æ—¥ã®å§‹ã¾ã‚Šã«ã€{name}æ§˜",
            ],
            "afternoon": [
                "ã“ã‚“ã«ã¡ã¯ã€{name}æ§˜",
                "åˆå¾Œã®ãŠæ™‚é–“ã«ãŠç–²ã‚Œã•ã¾ã§ã™ã€{name}æ§˜",
                "ãŠå¿™ã—ã„ä¸­å¤±ç¤¼ã„ãŸã—ã¾ã™ã€{name}æ§˜",
            ],
            "evening": [
                "ãŠç–²ã‚Œã•ã¾ã§ã™ã€{name}æ§˜",
                "ä¸€æ—¥ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€{name}æ§˜",
                "å¤•æ–¹ã®ãŠæ™‚é–“ã«å¤±ç¤¼ã„ãŸã—ã¾ã™ã€{name}æ§˜",
            ],
        }
    }

    SECTION_DESCRIPTIONS = {
        "editorial_picks": [
            "ç§ãŸã¡ã®å°‚é–€ãƒãƒ¼ãƒ ãŒ{name}æ§˜ã®ãŸã‚ã«å³é¸ã—ãŸç‰¹åˆ¥ãªæ±‚äººã§ã™ã€‚",
            "ç·¨é›†éƒ¨ãŒè‡ªä¿¡ã‚’æŒã£ã¦ãŠã™ã™ã‚ã™ã‚‹ã€{name}æ§˜ã«ã´ã£ãŸã‚Šã®æ¡ˆä»¶ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚",
            "{name}æ§˜ã®ã‚¹ã‚­ãƒ«ã€Œ{skills}ã€ã‚’æœ€å¤§é™æ´»ã‹ã›ã‚‹æ±‚äººã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚",
        ],
        "top_recommendations": [
            "AIã®é«˜åº¦ãªåˆ†æã«ã‚ˆã‚Šã€{name}æ§˜ã«æœ€ã‚‚ãƒãƒƒãƒã™ã‚‹æ±‚äººTOP5ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚",
            "ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦{score}%ä»¥ä¸Šã®ã€{name}æ§˜ã¸ã®å¼·åŠ›æ¨å¥¨æ±‚äººã§ã™ã€‚",
            "{name}æ§˜ã®çµŒé¨“ã¨å¸Œæœ›ã‚’ç·åˆçš„ã«åˆ†æã—ãŸçµæœã‚’ã”è¦§ãã ã•ã„ã€‚",
        ],
        "trending_jobs": [
            "ä»Šã€å¤šãã®æ±‚è·è€…ã‹ã‚‰æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã‚‹äººæ°—æ€¥ä¸Šæ˜‡ã®æ±‚äººã§ã™ã€‚",
            "æ¥­ç•Œã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å…ˆå–ã‚Šã—ãŸã€å°†æ¥æ€§è±Šã‹ãªæ±‚äººã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚",
            "ç«¶äº‰ç‡ã¯é«˜ã‚ã§ã™ãŒã€{name}æ§˜ãªã‚‰ååˆ†ã«ãƒãƒ£ãƒ³ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚",
        ],
    }

    CALL_TO_ACTIONS = {
        AIQualityLevel.STANDARD: ["è©³ç´°ã‚’ç¢ºèªã™ã‚‹", "ä»Šã™ãå¿œå‹Ÿã™ã‚‹", "æ°—ã«ãªã‚‹æ±‚äººã‚’ä¿å­˜"],
        AIQualityLevel.PREMIUM: [
            "ğŸ¯ ã“ã®æ©Ÿä¼šã‚’é€ƒã•ãšè©³ç´°ã‚’ãƒã‚§ãƒƒã‚¯",
            "ğŸ’« {name}æ§˜ã®æœªæ¥ã¸ã®ç¬¬ä¸€æ­©ã‚’è¸ã¿å‡ºã™",
            "â­ ç†æƒ³ã®ã‚­ãƒ£ãƒªã‚¢ã«å‘ã‘ã¦ä»Šã™ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
        ],
    }


# ============================================================================
# AI SERVICE CLASS
# ============================================================================


class AIService:
    """
    AI Service for content enhancement and personalization

    This service provides intelligent content generation for email campaigns,
    job descriptions, and personalization suggestions. Currently implemented
    as a sophisticated mock service for development and testing.
    """

    def __init__(
        self,
        model_type: AIModelType = AIModelType.MOCK,
        api_key: Optional[str] = None,
        quality_level: AIQualityLevel = AIQualityLevel.STANDARD,
        enable_caching: bool = True,
        cache_ttl: int = 3600,
    ):
        """Initialize the AI Service"""
        self.model_type = model_type
        self.api_key = api_key
        self.quality_level = quality_level
        self.enable_caching = enable_caching
        self.cache_ttl = cache_ttl
        self.templates = MockAITemplates()
        self.metrics = AIPerformanceMetrics()
        self._cache: Dict[str, Tuple[AIGenerationResponse, datetime]] = {}

        logger.info(f"AIService initialized with model: {model_type.value}")

    async def generate_content(self, request: AIGenerationRequest) -> AIGenerationResponse:
        """
        Generate AI-powered content based on the request

        Args:
            request: Content generation request

        Returns:
            Generated content response
        """
        start_time = time.time()

        try:
            # Check cache first
            if self.enable_caching:
                cache_key = self._generate_cache_key(request)
                cached_response = self._get_from_cache(cache_key)
                if cached_response:
                    self.metrics.cache_hit_rate = (
                        self.metrics.cache_hit_rate * self.metrics.total_requests + 1
                    ) / (self.metrics.total_requests + 1)
                    return cached_response

            # Generate content based on model type
            if self.model_type == AIModelType.MOCK:
                response = await self._generate_mock_content(request)
            else:
                response = await self._generate_real_ai_content(request)

            # Calculate response time
            response.generation_time_ms = int((time.time() - start_time) * 1000)

            # Cache the response
            if self.enable_caching:
                self._add_to_cache(cache_key, response)

            # Update metrics
            self._update_metrics(response, success=True)

            logger.info(
                f"Content generated successfully: {request.content_type.value} "
                f"for user {request.user_profile.user_id}"
            )

            return response

        except Exception as e:
            logger.error(f"Content generation failed: {str(e)}", exc_info=True)
            self._update_metrics(None, success=False)

            # Return fallback response
            return AIGenerationResponse(
                content="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                confidence_score=0.0,
                generation_time_ms=int((time.time() - start_time) * 1000),
                tokens_used=0,
                model_used=self.model_type,
                quality_metrics={"error": 1.0},
                suggestions=["æ¨™æº–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã”åˆ©ç”¨ãã ã•ã„"],
            )

    async def _generate_mock_content(self, request: AIGenerationRequest) -> AIGenerationResponse:
        """Generate mock AI content for development/testing"""

        # Simulate processing time
        await asyncio.sleep(random.uniform(0.1, 0.5))

        content = ""
        confidence_score = random.uniform(0.7, 0.95)
        tokens_used = random.randint(50, 200)

        # Generate content based on type
        if request.content_type == ContentType.EMAIL_SUBJECT:
            content = self._generate_mock_subject(request)
        elif request.content_type == ContentType.EMAIL_GREETING:
            content = self._generate_mock_greeting(request)
        elif request.content_type == ContentType.SECTION_DESCRIPTION:
            content = self._generate_mock_section_description(request)
        elif request.content_type == ContentType.JOB_DESCRIPTION:
            content = self._generate_mock_job_description(request)
        elif request.content_type == ContentType.CALL_TO_ACTION:
            content = self._generate_mock_cta(request)
        else:
            content = "Mock AI ç”Ÿæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„"

        # Generate quality metrics
        quality_metrics = {
            "readability": random.uniform(0.8, 0.95),
            "personalization": request.personalization_level,
            "engagement": random.uniform(0.7, 0.9),
            "relevance": random.uniform(0.8, 0.95),
        }

        # Generate suggestions
        suggestions = self._generate_mock_suggestions(request)
        alternatives = self._generate_mock_alternatives(content, request)

        return AIGenerationResponse(
            content=content,
            confidence_score=confidence_score,
            generation_time_ms=0,  # Will be set by caller
            tokens_used=tokens_used,
            model_used=self.model_type,
            quality_metrics=quality_metrics,
            suggestions=suggestions,
            alternatives=alternatives,
        )

    def _generate_mock_subject(self, request: AIGenerationRequest) -> str:
        """Generate mock email subject"""
        templates = self.templates.JAPANESE_SUBJECTS.get(
            self.quality_level, self.templates.JAPANESE_SUBJECTS[AIQualityLevel.STANDARD]
        )

        template = random.choice(templates)

        # Extract variables from user profile and job matches
        variables = {
            "name": request.user_profile.name or "ãŠå®¢æ§˜",
            "count": len(request.job_matches) if request.job_matches else 5,
            "location": request.user_profile.location or "æ±äº¬éƒ½",
            "score": random.randint(85, 95),
            "skill": ", ".join(request.user_profile.job_preferences.get("skills", ["IT"])[:2]),
        }

        try:
            return template.format(**variables)
        except KeyError:
            return template

    def _generate_mock_greeting(self, request: AIGenerationRequest) -> str:
        """Generate mock email greeting"""
        current_hour = datetime.now().hour

        if current_hour < 12:
            time_period = "morning"
        elif current_hour < 17:
            time_period = "afternoon"
        else:
            time_period = "evening"

        greetings = self.templates.GREETINGS[request.language][time_period]
        greeting_template = random.choice(greetings)

        return greeting_template.format(name=request.user_profile.name or "ãŠå®¢æ§˜")

    def _generate_mock_section_description(self, request: AIGenerationRequest) -> str:
        """Generate mock section description"""
        section_key = request.context_data.get("section_type", "editorial_picks")
        descriptions = self.templates.SECTION_DESCRIPTIONS.get(
            section_key, ["å³é¸ã•ã‚ŒãŸæ±‚äººã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚"]
        )

        description_template = random.choice(descriptions)

        variables = {
            "name": request.user_profile.name or "ãŠå®¢æ§˜",
            "skills": ", ".join(request.user_profile.job_preferences.get("skills", ["IT"])[:2]),
            "score": random.randint(85, 95),
        }

        try:
            return description_template.format(**variables)
        except KeyError:
            return description_template

    def _generate_mock_job_description(self, request: AIGenerationRequest) -> str:
        """Generate mock enhanced job description"""
        if not request.job_matches:
            return "é­…åŠ›çš„ãªæ±‚äººã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚"

        job = request.job_matches[0]

        enhancements = [
            f"âœ¨ {job.title}ã®é­…åŠ›ï¼š{job.company}ã§ã®ã‚­ãƒ£ãƒªã‚¢æˆé•·ãŒæœŸå¾…ã§ãã¾ã™",
            f"ğŸ’¼ {job.company}ã§ã¯ã€ã‚ãªãŸã®{job.title}ã‚¹ã‚­ãƒ«ã‚’æœ€å¤§é™æ´»ç”¨ã§ãã‚‹ç’°å¢ƒãŒæ•´ã£ã¦ã„ã¾ã™",
            f"ğŸš€ {job.location}ã§æ–°ã—ã„ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼{job.title}ã¨ã—ã¦æ´»èºã—ã¾ã›ã‚“ã‹",
            f"â­ {job.company}ã®{job.title}ãƒã‚¸ã‚·ãƒ§ãƒ³ - ç†æƒ³çš„ãªè·å ´ç’°å¢ƒã‚’ãŠç´„æŸ",
        ]

        return random.choice(enhancements)

    def _generate_mock_cta(self, request: AIGenerationRequest) -> str:
        """Generate mock call-to-action"""
        ctas = self.templates.CALL_TO_ACTIONS.get(
            self.quality_level, self.templates.CALL_TO_ACTIONS[AIQualityLevel.STANDARD]
        )

        cta_template = random.choice(ctas)

        try:
            return cta_template.format(name=request.user_profile.name or "ãŠå®¢æ§˜")
        except KeyError:
            return cta_template

    def _generate_mock_suggestions(self, request: AIGenerationRequest) -> List[str]:
        """Generate mock improvement suggestions"""
        suggestions = [
            "ã‚ˆã‚Šå…·ä½“çš„ãªã‚¹ã‚­ãƒ«æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã¨ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒå‘ä¸Šã—ã¾ã™",
            "æœ€è¿‘ã®å¿œå‹Ÿå±¥æ­´ã‚’åˆ†æã—ã¦ã€èˆˆå‘³ã®å¤‰åŒ–ã‚’åæ˜ ã•ã›ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™",
            "åœ°åŸŸåˆ¥ã®æ±‚äººå‹•å‘ã‚’è€ƒæ…®ã—ãŸå†…å®¹èª¿æ•´ãŒåŠ¹æœçš„ã§ã™",
            "æ™‚æœŸçš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åæ˜ ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
        ]

        return random.sample(suggestions, random.randint(1, 3))

    def _generate_mock_alternatives(self, content: str, request: AIGenerationRequest) -> List[str]:
        """Generate mock content alternatives"""
        # Simple rule-based alternatives
        alternatives = []

        if "å³é¸" in content:
            alternatives.append(content.replace("å³é¸", "ç‰¹åˆ¥ã«é¸å®š"))
        if "ã”ç´¹ä»‹" in content:
            alternatives.append(content.replace("ã”ç´¹ä»‹", "ãŠå±Šã‘"))
        if "ãŠã™ã™ã‚" in content:
            alternatives.append(content.replace("ãŠã™ã™ã‚", "æ¨å¥¨"))

        return alternatives[:2]  # Return max 2 alternatives

    async def _generate_real_ai_content(self, request: AIGenerationRequest) -> AIGenerationResponse:
        """Generate content using real AI model (placeholder for future implementation)"""

        # This would implement actual OpenAI API calls
        # For now, fall back to mock generation
        logger.warning("Real AI model not yet implemented, falling back to mock")
        return await self._generate_mock_content(request)

    def _generate_cache_key(self, request: AIGenerationRequest) -> str:
        """Generate cache key for request"""
        request_str = json.dumps(
            {
                "content_type": request.content_type.value,
                "user_id": request.user_profile.user_id,
                "language": request.language.value,
                "quality_level": self.quality_level.value,
                "job_count": len(request.job_matches) if request.job_matches else 0,
                "context_hash": hashlib.md5(
                    json.dumps(request.context_data or {}, sort_keys=True).encode()
                ).hexdigest()[:8],
            },
            sort_keys=True,
        )

        return hashlib.sha256(request_str.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[AIGenerationResponse]:
        """Get response from cache if valid"""
        if cache_key in self._cache:
            response, timestamp = self._cache[cache_key]
            if datetime.now() - timestamp < timedelta(seconds=self.cache_ttl):
                return response
            else:
                del self._cache[cache_key]
        return None

    def _add_to_cache(self, cache_key: str, response: AIGenerationResponse):
        """Add response to cache"""
        self._cache[cache_key] = (response, datetime.now())

        # Clean old cache entries if cache gets too large
        if len(self._cache) > 1000:
            cutoff_time = datetime.now() - timedelta(seconds=self.cache_ttl)
            self._cache = {k: v for k, v in self._cache.items() if v[1] > cutoff_time}

    def _update_metrics(self, response: Optional[AIGenerationResponse], success: bool):
        """Update performance metrics"""
        self.metrics.total_requests += 1

        if success:
            self.metrics.successful_requests += 1
            if response:
                self.metrics.total_tokens_used += response.tokens_used

                # Update average quality score
                avg_quality = sum(response.quality_metrics.values()) / len(response.quality_metrics)
                self.metrics.quality_score_avg = (
                    self.metrics.quality_score_avg * (self.metrics.successful_requests - 1)
                    + avg_quality
                ) / self.metrics.successful_requests
        else:
            self.metrics.failed_requests += 1

    def get_metrics(self) -> AIPerformanceMetrics:
        """Get current performance metrics"""
        return self.metrics

    def clear_cache(self):
        """Clear the content cache"""
        self._cache.clear()
        logger.info("AI service cache cleared")


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================


async def create_ai_service(
    model_type: AIModelType = AIModelType.MOCK,
    quality_level: AIQualityLevel = AIQualityLevel.STANDARD,
    **kwargs,
) -> AIService:
    """Create and configure an AI service instance"""
    return AIService(model_type=model_type, quality_level=quality_level, **kwargs)


async def batch_generate_content(
    ai_service: AIService, requests: List[AIGenerationRequest]
) -> List[AIGenerationResponse]:
    """Generate content for multiple requests concurrently"""
    tasks = [ai_service.generate_content(request) for request in requests]
    return await asyncio.gather(*tasks, return_exceptions=True)


def enhance_email_content_with_ai(
    email_content: EmailContent, ai_service: AIService, user_profile: UserProfile
) -> EmailContent:
    """Enhance existing email content with AI suggestions (sync wrapper)"""
    # This would be used to post-process generated email content
    # For now, return as-is
    return email_content


# ============================================================================
# TESTING UTILITIES
# ============================================================================


async def test_ai_service():
    """Test the AI service with sample data"""

    # Create test service
    ai_service = await create_ai_service(
        model_type=AIModelType.MOCK, quality_level=AIQualityLevel.PREMIUM
    )

    # Create test user profile
    user_profile = UserProfile(
        user_id=1,
        name="ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼",
        email="test@example.com",
        preferred_language=EmailLanguage.JAPANESE,
        location="æ±äº¬éƒ½æ¸‹è°·åŒº",
        job_preferences={
            "skills": ["Python", "æ©Ÿæ¢°å­¦ç¿’", "ãƒ‡ãƒ¼ã‚¿åˆ†æ"],
            "industries": ["IT", "é‡‘è"],
        },
    )

    # Create test job matches
    job_matches = [
        JobMatch(
            job_id=1,
            title="ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ",
            company="ãƒ†ãƒƒã‚¯æ ªå¼ä¼šç¤¾",
            location="æ±äº¬éƒ½",
            description="æ©Ÿæ¢°å­¦ç¿’ã‚’æ´»ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿åˆ†ææ¥­å‹™",
        ),
        JobMatch(
            job_id=2,
            title="Pythonã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
            company="AIä¼æ¥­",
            location="æ±äº¬éƒ½æ–°å®¿åŒº",
            description="Pythonã‚’ä½¿ã£ãŸã‚·ã‚¹ãƒ†ãƒ é–‹ç™º",
        ),
    ]

    # Test different content types
    test_requests = [
        AIGenerationRequest(
            content_type=ContentType.EMAIL_SUBJECT,
            user_profile=user_profile,
            job_matches=job_matches,
        ),
        AIGenerationRequest(content_type=ContentType.EMAIL_GREETING, user_profile=user_profile),
        AIGenerationRequest(
            content_type=ContentType.SECTION_DESCRIPTION,
            user_profile=user_profile,
            job_matches=job_matches,
            context_data={"section_type": "editorial_picks"},
        ),
        AIGenerationRequest(
            content_type=ContentType.JOB_DESCRIPTION,
            user_profile=user_profile,
            job_matches=job_matches,
        ),
    ]

    # Generate content
    responses = await batch_generate_content(ai_service, test_requests)

    # Display results
    for i, response in enumerate(responses):
        if isinstance(response, Exception):
            print(f"Request {i+1} failed: {response}")
        else:
            print(f"\n=== Request {i+1}: {test_requests[i].content_type.value} ===")
            print(f"Content: {response.content}")
            print(f"Confidence: {response.confidence_score:.2f}")
            print(f"Quality: {response.quality_metrics}")
            if response.suggestions:
                print(f"Suggestions: {response.suggestions}")

    # Show metrics
    metrics = ai_service.get_metrics()
    print(f"\n=== Service Metrics ===")
    print(f"Total requests: {metrics.total_requests}")
    print(f"Success rate: {metrics.successful_requests / metrics.total_requests * 100:.1f}%")
    print(f"Average quality: {metrics.quality_score_avg:.2f}")


if __name__ == "__main__":
    import asyncio

    asyncio.run(test_ai_service())
